wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.10.30
    code_path: code/curriculum_code/task_difficulty_experiment_mujoco.py
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.5
    t:
      1:
      - 1
      - 5
      2:
      - 1
      - 5
      3:
      - 1
      4: 3.8.5
      5: 0.10.30
      8:
      - 3
      - 5
num_tasks:
  desc: null
  value: 100
steps_per_task:
  desc: null
  value: 500
student:
  desc: null
  value: <stable_baselines3.ppo.ppo.PPO object at 0x000002554EDB0E50>
student_params:
  desc: null
  value:
    _current_progress_remaining: 1
    _episode_num: 0
    _last_dones: null
    _last_obs: null
    _last_original_obs: null
    _n_updates: 0
    _total_timesteps: 0
    _vec_normalize_env: null
    action_noise: null
    action_space: Box(-2.0, 2.0, (1,), float32)
    batch_size: 64
    clip_range: stable_baselines3.common.utils.constant_fn.<locals>.func
    clip_range_vf: null
    device: cuda
    ent_coef: 0.0
    env: <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002554EEADE20>
    ep_info_buffer: null
    ep_success_buffer: null
    eval_env: null
    gae_lambda: 0.95
    gamma: 0.99
    learning_rate: 0.0003
    lr_schedule: stable_baselines3.common.utils.constant_fn.<locals>.func
    max_grad_norm: 0.5
    n_envs: 1
    n_epochs: 10
    n_steps: 125
    num_timesteps: 0
    observation_space: Box(-8.0, 8.0, (5,), float32)
    policy: "ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten):\
      \ Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n\
      \    (shared_net): Sequential()\n    (policy_net): Sequential(\n      (0): Linear(in_features=5,\
      \ out_features=8, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=8,\
      \ out_features=8, bias=True)\n      (3): Tanh()\n    )\n    (value_net): Sequential(\n\
      \      (0): Linear(in_features=5, out_features=8, bias=True)\n      (1): Tanh()\n\
      \      (2): Linear(in_features=8, out_features=8, bias=True)\n      (3): Tanh()\n\
      \    )\n  )\n  (action_net): Linear(in_features=8, out_features=1, bias=True)\n\
      \  (value_net): Linear(in_features=8, out_features=1, bias=True)\n)"
    policy_class: stable_baselines3.common.policies.ActorCriticPolicy
    policy_kwargs:
      net_arch:
      - pi:
        - 8
        - 8
        vf:
        - 8
        - 8
    rollout_buffer: <stable_baselines3.common.buffers.RolloutBuffer object at 0x000002554EE90D60>
    sde_sample_freq: -1
    seed: null
    start_time: null
    target_kl: null
    tensorboard_log: null
    use_sde: false
    verbose: 0
    vf_coef: 0.5
task:
  desc: null
  value: MBPendulum-v2
teacher:
  desc: null
  value: <curriculum.teachers.predefined_tasks_teacher.PredefinedTasksTeacher object
    at 0x0000025549E492B0>
teacher_params:
  desc: null
  value:
    env_wrapper: <environment.parametric_mujoco.parametric_pendulum_locomotion.MBPendulumAngleContinuousWrapper
      object at 0x0000025549E49340>
    eval_data: []
    history: <curriculum.history.History object at 0x0000025549E49250>
    idx: 0
    seed: null
    tasks_to_cycle:
    - goal_angle: 3.1101767270538954
    - goal_angle: 3.1098593944626236
    - goal_angle: 3.10953558569602
    - goal_angle: 3.1092051004600014
    - goal_angle: 3.1088677301148997
    - goal_angle: 3.1085232572362163
    - goal_angle: 3.1081714551473487
    - goal_angle: 3.107812087422161
    - goal_angle: 3.1074449073551214
    - goal_angle: 3.1070696573964987
    - goal_angle: 3.1066860685499065
    - goal_angle: 3.1062938597292336
    - goal_angle: 3.1058927370717275
    - goal_angle: 3.1054823932037037
    - goal_angle: 3.105062506455028
    - goal_angle: 3.1046327400181486
    - goal_angle: 3.104192741047058
    - goal_angle: 3.103742139691121
    - goal_angle: 3.10328054805821
    - goal_angle: 3.1028075591010302
    - goal_angle: 3.102322745419921
    - goal_angle: 3.1018256579747328
    - goal_angle: 3.1013158246976165
    - goal_angle: 3.100792748997718
    - goal_angle: 3.100255908147822
    - goal_angle: 3.099704751541929
    - goal_angle: 3.0991386988115526
    - goal_angle: 3.098557137787193
    - goal_angle: 3.097959422289935
    - goal_angle: 3.097344869736416
    - goal_angle: 3.0967127585385104
    - goal_angle: 3.0960623252768977
    - goal_angle: 3.0953927616252375
    - goal_angle: 3.0947032109989006
    - goal_angle: 3.093992764899039
    - goal_angle: 3.093260458919181
    - goal_angle: 3.0925052683774528
    - goal_angle: 3.0917261035328125
    - goal_angle: 3.090921804338345
    - goal_angle: 3.090091134678485
    - goal_angle: 3.089232776029963
    - goal_angle: 3.0883453204781017
    - goal_angle: 3.0874272630106585
    - goal_angle: 3.0864769930004985
    - goal_angle: 3.0854927847756897
    - goal_angle: 3.084472787160888
    - goal_angle: 3.083415011856649
    - goal_angle: 3.082317320503193
    - goal_angle: 3.0811774102515277
    - goal_angle: 3.079992797637052
    - goal_angle: 3.078760800517997
    - goal_angle: 3.077478517802246
    - goal_angle: 3.076142806640006
    - goal_angle: 3.074750256704904
    - goal_angle: 3.07329716112045
    - goal_angle: 3.07177948351002
    - goal_angle: 3.0701928205536615
    - goal_angle: 3.0685323593202627
    - goal_angle: 3.0667928285043216
    - goal_angle: 3.0649684425266273
    - goal_angle: 3.0630528372500483
    - goal_angle: 3.0610389958054394
    - goal_angle: 3.0589191627058514
    - goal_angle: 3.0566847440333125
    - goal_angle: 3.0543261909900767
    - goal_angle: 3.0518328634872276
    - goal_angle: 3.0491928696606814
    - goal_angle: 3.046392876208284
    - goal_angle: 3.043417883165112
    - goal_angle: 3.0402509550868966
    - goal_angle: 3.036872898470133
    - goal_angle: 3.0332618724315243
    - goal_angle: 3.029392915961586
    - goal_angle: 3.0252373701235045
    - goal_angle: 3.0207621669132627
    - goal_angle: 3.015928947446201
    - goal_angle: 3.0106929596902186
    - goal_angle: 3.0050016686511065
    - goal_angle: 2.99879298751753
    - goal_angle: 2.9919930034188504
    - goal_angle: 2.9845130209103035
    - goal_angle: 2.9762456718219092
    - goal_angle: 2.96705972839036
    - goal_angle: 2.95679308573157
    - goal_angle: 2.945243112740431
    - goal_angle: 2.9321531433504737
    - goal_angle: 2.9171931783333793
    - goal_angle: 2.8999316802367323
    - goal_angle: 2.8797932657906435
    - goal_angle: 2.855993321445266
    - goal_angle: 2.827433388230814
    - goal_angle: 2.792526803190927
    - goal_angle: 2.748893571891069
    - goal_angle: 2.6927937030769655
    - goal_angle: 2.6179938779914944
    - goal_angle: 2.5132741228718345
    - goal_angle: 2.356194490192345
    - goal_angle: 2.0943951023931957
    - goal_angle: 1.5707963267948966
