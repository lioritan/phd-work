wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.10.30
    code_path: code/curriculum_code/task_difficulty_experiment_mujoco.py
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.5
    t:
      1:
      - 1
      - 5
      2:
      - 1
      - 5
      3:
      - 1
      4: 3.8.5
      5: 0.10.30
      8:
      - 3
      - 5
num_tasks:
  desc: null
  value: 100
steps_per_task:
  desc: null
  value: 500
student:
  desc: null
  value: <stable_baselines3.ppo.ppo.PPO object at 0x0000015A3B450AF0>
student_params:
  desc: null
  value:
    _current_progress_remaining: 1
    _episode_num: 0
    _last_dones: null
    _last_obs: null
    _last_original_obs: null
    _n_updates: 0
    _total_timesteps: 0
    _vec_normalize_env: null
    action_noise: null
    action_space: Box(-2.0, 2.0, (1,), float32)
    batch_size: 64
    clip_range: stable_baselines3.common.utils.constant_fn.<locals>.func
    clip_range_vf: null
    device: cuda
    ent_coef: 0.0
    env: <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000015A3B562070>
    ep_info_buffer: null
    ep_success_buffer: null
    eval_env: null
    gae_lambda: 0.95
    gamma: 0.99
    learning_rate: 0.0003
    lr_schedule: stable_baselines3.common.utils.constant_fn.<locals>.func
    max_grad_norm: 0.5
    n_envs: 1
    n_epochs: 10
    n_steps: 125
    num_timesteps: 0
    observation_space: Box(-8.0, 8.0, (5,), float32)
    policy: "ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten):\
      \ Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n\
      \    (shared_net): Sequential()\n    (policy_net): Sequential(\n      (0): Linear(in_features=5,\
      \ out_features=8, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=8,\
      \ out_features=8, bias=True)\n      (3): Tanh()\n    )\n    (value_net): Sequential(\n\
      \      (0): Linear(in_features=5, out_features=8, bias=True)\n      (1): Tanh()\n\
      \      (2): Linear(in_features=8, out_features=8, bias=True)\n      (3): Tanh()\n\
      \    )\n  )\n  (action_net): Linear(in_features=8, out_features=1, bias=True)\n\
      \  (value_net): Linear(in_features=8, out_features=1, bias=True)\n)"
    policy_class: stable_baselines3.common.policies.ActorCriticPolicy
    policy_kwargs:
      net_arch:
      - pi:
        - 8
        - 8
        vf:
        - 8
        - 8
    rollout_buffer: <stable_baselines3.common.buffers.RolloutBuffer object at 0x0000015A3B531FD0>
    sde_sample_freq: -1
    seed: null
    start_time: null
    target_kl: null
    tensorboard_log: null
    use_sde: false
    verbose: 0
    vf_coef: 0.5
task:
  desc: null
  value: MBPendulum-v2
teacher:
  desc: null
  value: <curriculum.teachers.predefined_tasks_teacher.PredefinedTasksTeacher object
    at 0x0000015A364D92B0>
teacher_params:
  desc: null
  value:
    env_wrapper: <environment.parametric_mujoco.parametric_pendulum_locomotion.MBPendulumAngleContinuousWrapper
      object at 0x0000015A364D9340>
    eval_data: []
    history: <curriculum.history.History object at 0x0000015A364D9250>
    idx: 0
    seed: null
    tasks_to_cycle:
    - goal_angle: 3.1101767270538954
    - goal_angle: 3.078760800517997
    - goal_angle: 3.0473448739820994
    - goal_angle: 3.015928947446201
    - goal_angle: 2.9845130209103035
    - goal_angle: 2.9530970943744053
    - goal_angle: 2.9216811678385075
    - goal_angle: 2.8902652413026098
    - goal_angle: 2.858849314766712
    - goal_angle: 2.827433388230814
    - goal_angle: 2.796017461694916
    - goal_angle: 2.764601535159018
    - goal_angle: 2.73318560862312
    - goal_angle: 2.701769682087222
    - goal_angle: 2.670353755551324
    - goal_angle: 2.638937829015426
    - goal_angle: 2.607521902479528
    - goal_angle: 2.5761059759436304
    - goal_angle: 2.5446900494077327
    - goal_angle: 2.5132741228718345
    - goal_angle: 2.4818581963359367
    - goal_angle: 2.4504422698000385
    - goal_angle: 2.419026343264141
    - goal_angle: 2.3876104167282426
    - goal_angle: 2.356194490192345
    - goal_angle: 2.324778563656447
    - goal_angle: 2.293362637120549
    - goal_angle: 2.261946710584651
    - goal_angle: 2.230530784048753
    - goal_angle: 2.199114857512855
    - goal_angle: 2.167698930976957
    - goal_angle: 2.1362830044410592
    - goal_angle: 2.104867077905161
    - goal_angle: 2.0734511513692633
    - goal_angle: 2.0420352248333655
    - goal_angle: 2.0106192982974678
    - goal_angle: 1.9792033717615696
    - goal_angle: 1.9477874452256718
    - goal_angle: 1.9163715186897738
    - goal_angle: 1.8849555921538759
    - goal_angle: 1.8535396656179781
    - goal_angle: 1.8221237390820801
    - goal_angle: 1.7907078125461822
    - goal_angle: 1.7592918860102844
    - goal_angle: 1.7278759594743864
    - goal_angle: 1.6964600329384885
    - goal_angle: 1.6650441064025905
    - goal_angle: 1.6336281798666925
    - goal_angle: 1.6022122533307945
    - goal_angle: 1.5707963267948966
    - goal_angle: 1.5393804002589986
    - goal_angle: 1.5079644737231006
    - goal_angle: 1.4765485471872026
    - goal_angle: 1.4451326206513047
    - goal_angle: 1.4137166941154067
    - goal_angle: 1.3823007675795087
    - goal_angle: 1.3508848410436112
    - goal_angle: 1.3194689145077132
    - goal_angle: 1.2880529879718152
    - goal_angle: 1.2566370614359172
    - goal_angle: 1.2252211349000193
    - goal_angle: 1.1938052083641213
    - goal_angle: 1.1623892818282235
    - goal_angle: 1.1309733552923256
    - goal_angle: 1.0995574287564276
    - goal_angle: 1.0681415022205296
    - goal_angle: 1.0367255756846316
    - goal_angle: 1.0053096491487337
    - goal_angle: 0.973893722612836
    - goal_angle: 0.942477796076938
    - goal_angle: 0.9110618695410401
    - goal_angle: 0.8796459430051422
    - goal_angle: 0.8482300164692442
    - goal_angle: 0.8168140899333463
    - goal_angle: 0.7853981633974483
    - goal_angle: 0.7539822368615503
    - goal_angle: 0.7225663103256523
    - goal_angle: 0.6911503837897544
    - goal_angle: 0.6597344572538565
    - goal_angle: 0.6283185307179585
    - goal_angle: 0.5969026041820605
    - goal_angle: 0.5654866776461629
    - goal_angle: 0.5340707511102649
    - goal_angle: 0.5026548245743669
    - goal_angle: 0.471238898038469
    - goal_angle: 0.4398229715025711
    - goal_angle: 0.4084070449666731
    - goal_angle: 0.37699111843077515
    - goal_angle: 0.3455751918948772
    - goal_angle: 0.31415926535897926
    - goal_angle: 0.2827433388230813
    - goal_angle: 0.2513274122871833
    - goal_angle: 0.21991148575128536
    - goal_angle: 0.18849555921538774
    - goal_angle: 0.1570796326794898
    - goal_angle: 0.12566370614359185
    - goal_angle: 0.09424777960769387
    - goal_angle: 0.06283185307179592
    - goal_angle: 0.03141592653589796
    - goal_angle: 0.0
