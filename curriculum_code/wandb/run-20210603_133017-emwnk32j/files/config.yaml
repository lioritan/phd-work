wandb_version: 1

_wandb:
  desc: null
  value:
    cli_version: 0.10.30
    code_path: code/curriculum_code/task_difficulty_experiment_mujoco.py
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.5
    t:
      1:
      - 1
      - 5
      2:
      - 1
      - 5
      3:
      - 1
      4: 3.8.5
      5: 0.10.30
      8:
      - 3
      - 5
num_tasks:
  desc: null
  value: 100
steps_per_task:
  desc: null
  value: 500
student:
  desc: null
  value: <stable_baselines3.a2c.a2c.A2C object at 0x00000160CC605280>
student_params:
  desc: null
  value:
    _current_progress_remaining: 1
    _episode_num: 0
    _last_dones: null
    _last_obs: null
    _last_original_obs: null
    _n_updates: 0
    _total_timesteps: 0
    _vec_normalize_env: null
    action_noise: null
    action_space: Box(-2.0, 2.0, (1,), float32)
    device: cuda
    ent_coef: 0.0
    env: <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000160CC605130>
    ep_info_buffer: null
    ep_success_buffer: null
    eval_env: null
    gae_lambda: 1.0
    gamma: 0.99
    learning_rate: 0.0007
    lr_schedule: stable_baselines3.common.utils.constant_fn.<locals>.func
    max_grad_norm: 0.5
    n_envs: 1
    n_steps: 5
    normalize_advantage: false
    num_timesteps: 0
    observation_space: Box(-8.0, 8.0, (3,), float32)
    policy: "ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten):\
      \ Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n\
      \    (shared_net): Sequential(\n      (0): Linear(in_features=3, out_features=8,\
      \ bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=8, out_features=8,\
      \ bias=True)\n      (3): Tanh()\n    )\n    (policy_net): Sequential()\n   \
      \ (value_net): Sequential()\n  )\n  (action_net): Linear(in_features=8, out_features=1,\
      \ bias=True)\n  (value_net): Linear(in_features=8, out_features=1, bias=True)\n\
      )"
    policy_class: stable_baselines3.common.policies.ActorCriticPolicy
    policy_kwargs:
      net_arch:
      - 8
      - 8
      optimizer_class: torch.optim.rmsprop.RMSprop
      optimizer_kwargs:
        alpha: 0.99
        eps: 1.0e-05
        weight_decay: 0
    rollout_buffer: <stable_baselines3.common.buffers.RolloutBuffer object at 0x00000160CC605EE0>
    sde_sample_freq: -1
    seed: null
    start_time: null
    tensorboard_log: null
    use_sde: false
    verbose: 0
    vf_coef: 0.5
task:
  desc: null
  value: MBPendulum-v2
teacher:
  desc: null
  value: <curriculum.teachers.predefined_tasks_teacher.PredefinedTasksTeacher object
    at 0x00000160C75A8160>
teacher_params:
  desc: null
  value:
    env_wrapper: <environment.parametric_mujoco.parametric_pendulum_locomotion.MBPendulumAngleContinuousWrapper
      object at 0x00000160C75A8280>
    eval_data: []
    history: <curriculum.history.History object at 0x00000160C75A8100>
    idx: 0
    seed: null
    tasks_to_cycle:
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
    - goal_angle: 0
