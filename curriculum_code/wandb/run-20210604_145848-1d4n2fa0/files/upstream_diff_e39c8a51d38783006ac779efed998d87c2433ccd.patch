diff --git a/curriculum_code/environment/parametric_mujoco/parametric_pendulum_locomotion.py b/curriculum_code/environment/parametric_mujoco/parametric_pendulum_locomotion.py
index 7759f9d..274d3a4 100644
--- a/curriculum_code/environment/parametric_mujoco/parametric_pendulum_locomotion.py
+++ b/curriculum_code/environment/parametric_mujoco/parametric_pendulum_locomotion.py
@@ -2,6 +2,7 @@ from typing import Any, Dict
 
 import torch
 import numpy as np
+from gym import spaces
 from gym.envs.classic_control import PendulumEnv
 from gym.envs.classic_control.pendulum import angle_normalize
 
@@ -19,13 +20,13 @@ class PendRewardModel(StateActionReward):
 
     def state_reward(self, state, next_state=None):
         angle_cost = -0.5 * (self.cos_angle - state[..., 0]) ** 2 - 0.5 * (self.sin_angle - state[..., 1]) ** 2
-        return 100*angle_cost - 0.1 * state[..., 2] ** 2
+        return angle_cost - 0.1 * state[..., 2] ** 2
 
 
 class AngledPendulumEnv(PendulumEnv):
     def __init__(self, angle=0.0):
         super(AngledPendulumEnv, self).__init__()
-        self.angle = angle_normalize(angle)
+        self.angle = angle #angle_normalize(angle)
         self.m = 0.1
 
     def step(self, u):
@@ -38,8 +39,8 @@ class AngledPendulumEnv(PendulumEnv):
 
         u = np.clip(u, -self.max_torque, self.max_torque)[0]
         self.last_u = u  # for rendering
-        angle_cost = (self.angle - angle_normalize(th)) ** 2
-        costs = 100*angle_cost + .1 * thdot ** 2 + .001 * (u ** 2)
+        angle_cost = angle_normalize(self.angle - th) ** 2
+        costs = angle_cost + .1 * thdot ** 2 + .001 * (u ** 2)
 
         newthdot = thdot + (-3 * g / (2 * l) * np.sin(th + np.pi) + 3. / (m * l ** 2) * u) * dt
         newth = th + newthdot * dt
@@ -52,6 +53,12 @@ class AngledPendulumEnv(PendulumEnv):
         """Get reward model."""
         return PendRewardModel(self.angle)
 
+    def reset(self):
+        high = np.array([np.pi, 0])
+        self.state = high#self.np_random.uniform(low=-high, high=high)
+        self.last_u = None
+        return self._get_obs()
+
 
 class MBPendulumAngleContinuousWrapper(EnvironmentWrapper):
     def __init__(self):
diff --git a/curriculum_code/task_difficulty_experiment_mujoco.py b/curriculum_code/task_difficulty_experiment_mujoco.py
index 60f0080..acbaa7e 100644
--- a/curriculum_code/task_difficulty_experiment_mujoco.py
+++ b/curriculum_code/task_difficulty_experiment_mujoco.py
@@ -5,7 +5,7 @@ import os
 import pickle
 
 import gym
-from stable_baselines3 import PPO
+from stable_baselines3 import PPO, A2C, TD3, SAC
 
 import wandb
 import numpy as np
@@ -15,8 +15,8 @@ from tqdm import tqdm
 from curriculum.eval.task_difficulty_estimate import estimate_task_difficulties
 from curriculum.teachers.predefined_tasks_teacher import PredefinedTasksTeacher
 from curriculum.teachers.random_teacher import RandomTeacher
-from environment.parametric_mujoco.parametric_ant import AntWrapper
-from environment.parametric_mujoco.parametric_half_cheetah import HalfCheetahWrapper
+#from environment.parametric_mujoco.parametric_ant import AntWrapper
+#from environment.parametric_mujoco.parametric_half_cheetah import HalfCheetahWrapper
 from environment.parametric_mujoco.parametric_pendulum_locomotion import MBPendulumAngleContinuousWrapper
 from student_algorithms.gp_model_ensemble.dpgpmm_algorithm import DPGPMMAlgorithm
 from student_algorithms.gp_model_ensemble.dpgpmm_policy import DPGPMMPolicy
@@ -27,12 +27,12 @@ from student_algorithms.nn_model_ensemble.nnmm_policy import NNMMPolicy
 def measure_difficulty(steps_per_task, tasks, wrapper, easy_task, student_alg="PPO"):
     random_teacher = RandomTeacher(None, wrapper)
 
-    random_teacher = PredefinedTasksTeacher({"tasks": [
-        {"goal_angle": np.pi}
-    ]*100}, wrapper)
+    # random_teacher = PredefinedTasksTeacher({"tasks":
+    #     [{"goal_angle": 0}]*100
+    # }, wrapper)
     #random_teacher = PredefinedTasksTeacher({"tasks": [easy_task]}, wrapper)
 
-    wandb.init(project=f'mb_me_{wrapper.name}', entity='liorf', save_code=True)
+    wandb.init(project=f'sched_{wrapper.name}', entity='liorf', save_code=True)
     config = wandb.config
     config.task = wrapper.name
     config.teacher = str(random_teacher)
@@ -69,6 +69,20 @@ def measure_difficulty(steps_per_task, tasks, wrapper, easy_task, student_alg="P
                       policy_kwargs={"net_arch": [dict(pi=[8, 8], vf=[8, 8])]},
                       n_steps=steps_per_task // 4)
 
+        student = SAC(policy='MlpPolicy', env=ref_env,
+                      verbose=0,
+                      policy_kwargs={"net_arch": [dict(pi=[8, 8], vf=[8, 8])]},
+                      )
+
+        # student = TD3(policy="MlpPolicy", env=ref_env,
+        #               verbose=0,
+        #               policy_kwargs={"net_arch": [8, 8]},
+        #               )
+
+        # student = A2C(policy='MlpPolicy', env=ref_env,
+        #               policy_kwargs={"net_arch": [8, 8]},
+        #                     verbose=0)
+
     config.student = str(student)
     config.student_params = student.__dict__
 
@@ -106,12 +120,17 @@ def measure_difficulty(steps_per_task, tasks, wrapper, easy_task, student_alg="P
     with open(f"./results/{date_string}/difficulty/{wrapper.name}/hist.pkl", "wb") as fptr:
         pickle.dump(random_teacher.history.history, fptr)
 
+    difficulty_estimates, task_params = estimate_task_difficulties(student, wrapper, 60, 3, steps_per_task)
+    print(zip(difficulty_estimates.mean(axis=1).tolist(), task_params))
+
     # eval and record video
     wandb.gym.monitor()  # Any env used with gym wrapper monitor will now be recorded
     evaluate(steps_per_task, wrapper.create_env(easy_task),
              student, f"./results/{date_string}/difficulty/{wrapper.name}/")
 
 
+
+
 def evaluate(action_limit, base_env, student, base_dir):
     eval_env = gym.wrappers.Monitor(base_env, directory=base_dir)
     student.set_env(eval_env)
