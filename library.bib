Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Li,
abstract = {We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures.},
archivePrefix = {arXiv},
arxivId = {2103.07607},
author = {Li, Yunfei and Wu, Yilin and Xu, Huazhe and Wang, Xiaolong and Wu, Yi},
eprint = {2103.07607},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2021 - Solving Compositional Reinforcement Learning Problems via Task Reduction.pdf:pdf},
month = {mar},
title = {{Solving Compositional Reinforcement Learning Problems via Task Reduction}},
url = {https://sites.google.com/view/sir-compositional. http://arxiv.org/abs/2103.07607},
year = {2021}
}
@techreport{Gupta2018,
abstract = {Meta-learning is a powerful tool that builds on multi-task learning to learn how to quickly adapt a model to new tasks. In the context of reinforcement learning, meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by meta-learning prior tasks. The performance of meta-learning algorithms critically depends on the tasks available for meta-training: in the same way that supervised learning algorithms generalize best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We describe a general recipe for unsupervised meta-reinforcement learning, and describe an effective instantiation of this approach based on a recently proposed unsupervised exploration technique and model-agnostic meta-learning. We also discuss practical and conceptual considerations for developing unsupervised meta-learning methods. Our experimental results demonstrate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design, significantly exceeds the performance of learning from scratch, and even matches performance of meta-learning methods that use hand-specified task distributions.},
annote = {2020 Berkley paper
Trying to minimize worst case adaptation regret
Create tasks for meta-RL by defining goal states/trajectories
Latent “task” variable, try to find goals/trajectories with high mutual information to task variable},
archivePrefix = {arXiv},
arxivId = {1806.04640},
author = {Gupta, Abhishek and Finn, Chelsea and Eysenbach, Benjamin and Levine, Sergey},
booktitle = {arXiv},
eprint = {1806.04640},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta et al. - 2018 - Unsupervised meta-learning for reinforcement learning.pdf:pdf},
keywords = {diversity:discriminator,diversity:entropy,env-type:randomized,env:mujoco},
mendeley-tags = {diversity:discriminator,diversity:entropy,env-type:randomized,env:mujoco},
title = {{Unsupervised meta-learning for reinforcement learning}},
year = {2018}
}
@article{OpenAI2021,
abstract = {We train a single, goal-conditioned policy that can solve many robotic manipulation tasks, including tasks with previously unseen goals and objects. We rely on asymmetric self-play for goal discovery, where two agents, Alice and Bob, play a game. Alice is asked to propose challenging goals and Bob aims to solve them. We show that this method can discover highly diverse and complex goals without any human priors. Bob can be trained with only sparse rewards, because the interaction between Alice and Bob results in a natural curriculum and Bob can learn from Alice's trajectory when relabeled as a goal-conditioned demonstration. Finally, our method scales, resulting in a single policy that can generalize to many unseen tasks such as setting a table, stacking blocks, and solving simple puzzles. Videos of a learned policy is available at https://robotics-self-play.github.io.},
archivePrefix = {arXiv},
arxivId = {2101.04882},
author = {OpenAI, OpenAI and Plappert, Matthias and Sampedro, Raul and Xu, Tao and Akkaya, Ilge and Kosaraju, Vineet and Welinder, Peter and D'Sa, Ruben and Petron, Arthur and Pinto, Henrique Ponde de Oliveira and Paino, Alex and Noh, Hyeonwoo and Weng, Lilian and Yuan, Qiming and Chu, Casey and Zaremba, Wojciech},
eprint = {2101.04882},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/OpenAI et al. - 2021 - Asymmetric self-play for automatic goal discovery in robotic manipulation.pdf:pdf},
month = {jan},
number = {c},
pages = {1--12},
title = {{Asymmetric self-play for automatic goal discovery in robotic manipulation}},
url = {http://arxiv.org/abs/2101.04882},
year = {2021}
}
@techreport{Wang2020,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
eprint = {2010.13166},
file = {::},
keywords = {Example Reweighting,Index Terms Curriculum Learning,Machine Learning,Self-Paced Learning,Training Strategy},
title = {{A Comprehensive Survey on Curriculum Learning}},
url = {http://arxiv.org/abs/2010.13166},
year = {2020}
}
@techreport{Kaddour2020,
abstract = {Data-efficient learning algorithms are essential in many practical applications where data collection is expensive, e.g., in robotics due to the wear and tear. To address this problem, meta-learning algorithms use prior experience about tasks to learn new, related tasks efficiently. Typically, a set of training tasks is assumed given or randomly chosen. However, this setting does not take into account the sequential nature that naturally arises when training a model from scratch in real-life: how do we collect a set of training tasks in a data-efficient manner? In this work, we introduce task selection based on prior experience into a meta-learning algorithm by conceptualizing the learner and the active meta-learning setting using a probabilistic latent variable model. We provide empirical evidence that our approach improves data-efficiency when compared to strong baselines on simulated robotic experiments.},
annote = {October 2020 paper, university college London
Learn task dataset embedding(trajectories to latent)
Pick new task from embedded space that maximizes “utility”
Task “utility” is defined by an information metric, like posterior task probability for all training tasks (so a good task distribution is one that is very different from the training)},
archivePrefix = {arXiv},
arxivId = {2007.08949},
author = {Kaddour, Jean and S{\ae}mundsson, Steind{\'{o}}r and Deisenroth, Marc Peter},
booktitle = {arXiv},
eprint = {2007.08949},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaddour, S{\ae}mundsson, Deisenroth - 2020 - Probabilistic Active Meta-Learning.pdf:pdf},
keywords = {diversity:latent+entropy,env-type:parametrized,env:toy},
mendeley-tags = {diversity:latent+entropy,env-type:parametrized,env:toy},
title = {{Probabilistic Active Meta-Learning}},
year = {2020}
}
@misc{Laskin2020,
abstract = {Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
annote = {Generalization by adding random noise to observations/state},
archivePrefix = {arXiv},
arxivId = {2004.14990},
author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
booktitle = {arXiv},
eprint = {2004.14990},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laskin et al. - 2020 - Reinforcement learning with augmented data.pdf:pdf},
keywords = {env:procgen,generalization},
mendeley-tags = {env:procgen,generalization},
title = {{Reinforcement learning with augmented data}},
url = {https://www.github.com/MishaLaskin/rad.},
year = {2020}
}
@article{Won2019,
abstract = {Recently, deep reinforcement learning (DRL) has attracted great attention in designing controllers for physics-based characters. Despite the recent success of DRL, the learned controller is viable for a single character. Changes in body size and proportions require learning controllers from scratch. In this paper, we present a new method of learning parametric controllers for body shape variation. A single parametric controller enables us to simulate and control various characters having different heights, weights, and body proportions. The users are allowed to create new characters through body shape parameters, and they can control the characters immediately. Our characters can also change their body shapes on the fly during simulation. The key to the success of our approach includes the adaptive sampling of body shapes that tackles the challenges in learning parametric controllers, which relies on the marginal value function that measures control capabilities of body shapes.We demonstrate parametric controllers for various physically simulated characters such as bipeds, quadrupeds, and underwater animals.},
author = {Won, Jungdam and Lee, Jehee},
doi = {10.1145/3355089.3356499},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Won, Lee - 2019 - Learning body shape variation in physics-based characters.pdf:pdf},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Character Animation,Deep Learning,Locomotion Control,Neural Network,Physics-based Simulation and Control,Reinforcement Learning},
number = {6},
title = {{Learning body shape variation in physics-based characters}},
url = {https://doi.org/10.1145/3355089.3356499.},
volume = {38},
year = {2019}
}
@techreport{Dennis2020,
abstract = {A wide range of reinforcement learning (RL) problems-including robustness, transfer learning, unsupervised RL, and emergent complexity-require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate struc-tured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.},
annote = {2020 December Berkley paper
Try to create a set of tasks/environments unsupervised
Gets input of environments with underspecified parameters (where to place obstacles in a maze) and learn legal parameter combinations that create environments that can be solved
Create a maze: random creates easy maze, adversarial creates unsolvable maze, let's try to make a hard but solvable maze
2 agents that try to learn the policy, “good” and “evil”
How to make sure it's hard and solvable: Reward is difference between “good” and “evil”, so unsolvable = 0 reward, environment generator tries to change env to improve “evil” agent policy without improving “good” agent policy too much
As “good” agent improves, env need to be harder, encouraged to find an easy env that “good” cannot solve (if task reward is also time-based)},
archivePrefix = {arXiv},
arxivId = {2012.02096v1},
author = {Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {2012.02096v1},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dennis et al. - 2020 - Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design.pdf:pdf},
keywords = {adversarial,env-type:parametrized,env:gridworld,goal reachability},
mendeley-tags = {adversarial,env-type:parametrized,env:gridworld,goal reachability},
number = {NeurIPS},
title = {{Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design}},
year = {2020}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL{\$}{\^{}}2{\$}, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL{\$}{\^{}}2{\$} experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL{\$}{\^{}}2{\$} is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL{\$}{\^{}}2{\$} on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1611.02779},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan et al. - 2016 - RL{\$}2{\$} Fast Reinforcement Learning via Slow Reinforcement Learning.pdf:pdf},
keywords = {alg:RL2,env:3dmaze},
mendeley-tags = {alg:RL2,env:3dmaze},
month = {nov},
title = {{RL{\$}{\^{}}2{\$}: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02779},
year = {2016}
}
@article{Narvekar2019,
abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
archivePrefix = {arXiv},
arxivId = {1812.00285},
author = {Narvekar, Sanmit and Stone, Peter},
eprint = {1812.00285},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar, Stone - 2019 - Learning curriculum policies for reinforcement learning(2).pdf:pdf},
isbn = {9781510892002},
issn = {15582914},
journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
keywords = {Curriculum learning,Reinforcement learning,Transfer learning,custom mdp,env-type:custom,env:gridworld,fixed curriculum},
mendeley-tags = {custom mdp,env-type:custom,env:gridworld,fixed curriculum},
month = {dec},
pages = {25--33},
title = {{Learning Curriculum Policies for Reinforcement Learning}},
url = {www.ifaamas.org http://arxiv.org/abs/1812.00285},
volume = {1},
year = {2018}
}
@techreport{Narvekar2020,
abstract = {Curriculum learning for reinforcement learning (RL) is an active area of research that seeks to speed up training of RL agents on a target task by first training them through a series of progressively more challenging source tasks. Each task in this sequence builds upon skills learned in previous tasks to gradually develop the repertoire needed to solve the final task. Over the past few years, many automated methods to develop cur-ricula have been developed. However, they all have one key limitation: the curriculum must be regenerated from scratch for each new agent or task encountered. In many cases, this generation process can be very expensive. However, there is structure that can be exploited between tasks and agents, such that knowledge gained developing a curriculum for one task can be reused to speed up creating a curriculum for a new task. In this paper, we present a method to generalize a curriculum learned for one set of tasks to a novel set of unseen tasks.},
annote = {Try to use a learned curriculum for one task to build a curriculum for another task (curriculum policy)
Grid worlds with manual task set designed such that solving each task gives a skill for the target task
Show that solving sub-tasks creates learning (even if the exact configuration of the test is not in the train, but all the needed skills are)},
author = {Narvekar, Sanmit and Stone, Peter},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar, Stone - 2020 - Generalizing Curricula for Reinforcement Learning.pdf:pdf},
keywords = {alg:dqn,custom mdp,env-type:set,env:gridworld},
mendeley-tags = {alg:dqn,custom mdp,env-type:set,env:gridworld},
title = {{Generalizing Curricula for Reinforcement Learning}},
year = {2020}
}
@techreport{Pan2019,
abstract = {Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary. Supplementary materials are available at https://sites.google.com/view/rararl.},
archivePrefix = {arXiv},
arxivId = {1904.00511},
author = {Pan, Xinlei and Seita, Daniel and Gao, Yang and Canny, John},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2019.8794293},
eprint = {1904.00511},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan et al. - 2019 - Risk averse robust adversarial reinforcement learning.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
keywords = {adversarial},
mendeley-tags = {adversarial},
pages = {8522--8528},
title = {{Risk averse robust adversarial reinforcement learning}},
volume = {2019-May},
year = {2019}
}
@article{Turchetta2020,
abstract = {In safety-critical applications autonomous agents may need to learn in an environment where mistakes can be very costly. In such settings, the agent needs to behave safely not only after but also while learning. To achieve this, existing safe reinforcement learning methods make an agent rely on priors that let it avoid dangerous situations during exploration with high probability, but both the probabilistic guarantees and the smoothness assumptions inherent in the priors are not viable in many scenarios of interest such as autonomous driving. This paper presents an alternative approach inspired by human teaching, where an agent learns under the supervision of an automatic instructor that saves the agent from violating constraints during learning. In this model, we introduce the monitor that neither needs to know how to do well at the task the agent is learning nor needs to know how the environment works. Instead, it has a library of reset controllers that it activates when the agent starts behaving dangerously, preventing it from doing damage. Crucially, the choices of which reset controller to apply in which situation affect the speed of agent learning. Based on observing agents' progress, the teacher itself learns a policy for choosing the reset controllers, a curriculum, to optimize the agent's final policy reward. Our experiments use this framework in two environments to induce curricula for safe and efficient learning.},
annote = {Assume “teacher” that gets a set of unwanted states reset-distributions, if agent gets to unwanted state, teacher can move him to a safe state with the reset
Shows some safety guarantees under this new teacher constraint, if teacher has the unsafe states in its set and you can't reach an unsafe state from an unwanted (but safe) state, the agent will not violate constraints ever

Curriculum = the teacher's chosen intervention set (the agent sees this by the provided MDP)},
archivePrefix = {arXiv},
arxivId = {2006.12136},
author = {Turchetta, Matteo and Kolobov, Andrey and Shah, Shital and Krause, Andreas and Agarwal, Alekh},
eprint = {2006.12136},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turchetta et al. - 2020 - Safe Reinforcement Learning via Curriculum Induction.pdf:pdf},
journal = {arXiv},
keywords = {Safety objective,env-type:custom,env:toy,fixed curriculum},
mendeley-tags = {Safety objective,env-type:custom,env:toy,fixed curriculum},
month = {jun},
title = {{Safe Reinforcement Learning via Curriculum Induction}},
url = {http://arxiv.org/abs/2006.12136},
year = {2020}
}
@article{Florensa2018,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
archivePrefix = {arXiv},
arxivId = {1705.06366},
author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
eprint = {1705.06366},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Held et al. - 2018 - Automatic Goal Generation for Reinforcement Learning Agents.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
month = {may},
pages = {2458--2471},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {https://sites.google.com/view/ http://arxiv.org/abs/1705.06366},
volume = {4},
year = {2017}
}
@techreport{Mehta2020,
abstract = {Gradient-based meta-learners such as Model-Agnostic Meta-Learning (MAML) have shown strong few-shot performance in supervised and reinforcement learning settings. However, specifically in the case of meta-reinforcement learning (meta-RL), we can show that gradient-based meta-learners are sensitive to task distributions. With the wrong curriculum, agents suffer the effects of meta-overfitting, shallow adaptation, and adaptation instability. In this work, we begin by highlighting intriguing failure cases of gradient-based meta-RL and show that task distributions can wildly affect algorithmic outputs, stability, and performance. To address this problem, we leverage insights from recent literature on domain randomization and propose meta Active Domain Randomization (meta-ADR), which learns a curriculum of tasks for gradient-based meta-RL in a similar as ADR does for sim2real transfer. We show that this approach induces more stable policies on a variety of simulated locomotion and navigation tasks. We assess in- and out-of-distribution generalization and find that the learned task distributions, even in an unstructured task space, greatly improve the adaptation performance of MAML. Finally, we motivate the need for better benchmarking in meta-RL that prioritizes generalization over single-task adaption performance.},
annote = {Use previous work (active domain randomization) to pick tasks
Use MAML/PEARL/other gradient method to adapt to new task},
archivePrefix = {arXiv},
arxivId = {2002.07956},
author = {Mehta, Bhairav and Deleu, Tristan and Raparthy, Sharath Chandra and Pal, Chris J. and Paull, Liam},
booktitle = {arXiv},
eprint = {2002.07956},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehta et al. - 2020 - Curriculum in gradient-based meta-reinforcement learning.pdf:pdf},
keywords = {diversity:discriminator,env-type:parametrized,env:mujoco},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:mujoco},
title = {{Curriculum in gradient-based meta-reinforcement learning}},
year = {2020}
}
@techreport{Racaniere2019,
abstract = {Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula-the breakdown of tasks into simpler, static challenges with dense rewards-to build up to complex behaviors. While curricula are also useful for artificial agents, handcrafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.},
annote = {ICLR 2020, deepmind
Same env with different params, env defined by free parameters (where to place things) - pretty big variations
Teacher-student (called here setter-solver) system, need to define/learn goal validity (can an expert solver solve this task), goal feasibility (can the current student solve this task), goal coverage (exploration of different goals)
Teacher (setter) is a generator that gets desired feasibility (difficulty), and a “judge” is discriminator for feasibility (supervised prediction of env solvable probability) -{\textgreater} GAN
Student (solver) gets sparse reward - 1 if got to goal, 0 if failed (enough time passed)
Goal Validity measured with loss function for probability of student getting to a goal close to specified goal (state and action space assumed continuous)
Goal coverage measured with loss by entropy (info gain) of new task for generator (so if generated task is similar to others in same difficulty, loss is high)
Total teacher (setter) loss is combination of all three losses (with equal weight)
Add additional discrimantion loss for student to discriminate teacher-generated goals from real goals (to avoid problems with easy goals, needed because env changes are big)
Ablation shows validity and coverage are most important},
archivePrefix = {arXiv},
arxivId = {1909.12892},
author = {Racani{\`{e}}re, S{\'{e}}bastien and Lampinen, Andrew K. and Santoro, Adam and Reichert, David P. and Firoiu, Vlad and Lillicrap, Timothy P.},
booktitle = {arXiv},
eprint = {1909.12892},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Racani{\`{e}}re et al. - 2019 - Automated curricula through setter-solver interactions.pdf:pdf},
keywords = {diversity:entropy,env-type:parametrized,env:custom,goal reachability},
mendeley-tags = {diversity:entropy,env-type:parametrized,env:custom,goal reachability},
title = {{Automated curricula through setter-solver interactions}},
year = {2019}
}
@misc{Portelas2019,
abstract = {We consider the problem of how a teacher algorithm can enable an unknown Deep Reinforcement Learning (DRL) student to become good at a skill over a wide range of diverse environments. To do so, we study how a teacher algorithm can learn to generate a learning curriculum, whereby it sequentially samples parameters controlling a stochastic procedural generation of environments. Because it does not initially know the capacities of its student, a key challenge for the teacher is to discover which environments are easy, difficult or unlearnable, and in what order to propose them to maximize the efficiency of learning over the learnable ones. To achieve this, this problem is transformed into a surrogate continuous bandit problem where the teacher samples environments in order to maximize absolute learning progress of its student. We present a new algorithm modeling absolute learning progress with Gaussian mixture models (ALP-GMM). We also adapt existing algorithms and provide a complete study in the context of DRL. Using parameterized variants of the BipedalWalker environment, we study their efficiency to personalize a learning curriculum for different learners (embodiments), their robustness to the ratio of learnable/unlearnable environments, and their scalability to non-linear and high-dimensional parameter spaces. Videos and code are available at https://github.com/flowersteam/teachDeepRL.},
annote = {ALP-GMM},
archivePrefix = {arXiv},
arxivId = {1910.07224},
author = {Portelas, R{\'{e}}my and Colas, C{\'{e}}dric and Hofmann, Katja and Oudeyer, Pierre Yves},
booktitle = {arXiv},
eprint = {1910.07224},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Portelas et al. - 2019 - Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments(2).pdf:pdf},
keywords = {Curiosity,Curriculum Learning,Deep Reinforcement Learning,Learning Progress,Parameterized Procedural Environments,Teacher-Student Learning,env-type:parametrized,env:pybullet},
mendeley-tags = {env-type:parametrized,env:pybullet},
month = {oct},
publisher = {arXiv},
title = {{Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments}},
url = {http://arxiv.org/abs/1910.07224},
year = {2019}
}
@misc{Klink2020,
abstract = {Generalization and reuse of agent behaviour across a variety of learning tasks promises to carry the next wave of breakthroughs in Reinforcement Learning (RL). The field of Curriculum Learning proposes strategies that aim to support a learn-ing agent by exposing it to a tailored series of tasks throughout learning, e.g. by progressively increasing their complexity. In this paper, we con-sider recently established results in Curriculum Learning for episodic RL, proposing an extension that is easily integrated with well-known RL al-gorithms and providing a theoretical formulation from an RL-as-Inference perspective. We evalu-ate the proposed scheme with different Deep RL algorithms on representative tasks, demonstrat-ing that it is capable of significantly improving learning performance.},
annote = {Use KL divergence between steps and known test distribution to pick tasks that have good start conditions (high V) and are "close" to test distribution.
Curriculum induced by restricting change of task distributions as well as the task distribution itself.},
archivePrefix = {arXiv},
arxivId = {2004.11812},
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
booktitle = {arXiv},
eprint = {2004.11812},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klink et al. - Unknown - Self-Paced Deep Reinforcement Learning.pdf:pdf},
keywords = {env-type:parametrized,env:gripper,env:mujoco,reward weights},
mendeley-tags = {env-type:parametrized,env:gripper,env:mujoco,reward weights},
title = {{Self-paced deep reinforcement learning}},
year = {2020}
}
@inproceedings{Rakelly2019,
abstract = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100x as well as in asymptotic performance on several meta-RL benchmarks.},
archivePrefix = {arXiv},
arxivId = {1903.08254},
author = {Rakelly, Kate and Zhou, Aurick and Quiilen, Deirdre and Finn, Chelsea and Levine, Sergey},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1903.08254},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rakelly et al. - 2019 - Efficient off-policy meta-reinforcement learning via probabilistic context variables(2).pdf:pdf},
isbn = {9781510886988},
keywords = {alg:pearl,env:mujoco},
mendeley-tags = {alg:pearl,env:mujoco},
month = {mar},
pages = {9291--9301},
publisher = {International Machine Learning Society (IMLS)},
title = {{Efficient off-policy meta-reinforcement learning via probabilistic context variables}},
url = {http://arxiv.org/abs/1903.08254},
volume = {2019-June},
year = {2019}
}
@article{Jabri2019,
abstract = {In principle, meta-reinforcement learning algorithms leverage experience across many tasks to learn fast reinforcement learning (RL) strategies that transfer to similar tasks. However, current meta-RL approaches rely on manually-defined distributions of training tasks, and hand-crafting these task distributions can be challenging and time-consuming. Can "useful" pre-training tasks be discovered in an unsupervised manner? We develop an unsupervised algorithm for inducing an adaptive meta-training task distribution, i.e. an automatic curriculum, by modeling unsupervised interaction in a visual environment. The task distribution is scaffolded by a parametric density model of the meta-learner's trajectory distribution. We formulate unsupervised meta-RL as information maximization between a latent task variable and the meta-learner's data distribution, and describe a practical instantiation which alternates between integration of recent experience into the task distribution and meta-learning of the updated tasks. Repeating this procedure leads to iterative reorganization such that the curriculum adapts as the meta-learner's data distribution shifts. In particular, we show how discriminative clustering for visual representation can support trajectory-level task acquisition and exploration in domains with pixel observations, avoiding pitfalls of alternatives. In experiments on vision-based navigation and manipulation domains, we show that the algorithm allows for unsupervised meta-learning that transfers to downstream tasks specified by hand-crafted reward functions and serves as pre-training for more efficient supervised meta-learning of test task distributions.},
annote = {2019 Berkley paper
Behavior model - learned GMM (gaussian mixture model) from state to reward, unsupervised, used to create tasks, updated from trajectories (E-step)
Visual data, discriminative clustering (take advantage of other research in vision unsupervised data)
Hopefully behavior model learns the environment well, which may help (can't do better without reward functions)
Curriculum = since GMM is updated from trajectories, new tasks should try to find under-explored regions},
archivePrefix = {arXiv},
arxivId = {1912.04226},
author = {Jabri, Allan and Hsu, Kyle and Eysenbach, Benjamin and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea},
eprint = {1912.04226},
journal = {arXiv},
keywords = {diversity:entropy,env-type:set,env:3dmaze,task embedding},
mendeley-tags = {diversity:entropy,env-type:set,env:3dmaze,task embedding},
month = {dec},
title = {{Unsupervised Curricula for Visual Meta-Reinforcement Learning}},
url = {http://arxiv.org/abs/1912.04226},
year = {2019}
}
@inproceedings{Narvekar2016,
abstract = {Transfer learning in reinforcement learning has been an active area of research over the past decade. In transfer learning, training on a source task is leveraged to speed up or otherwise improve learning on a target task. This paper presents the more ambitious problem of curriculum learning in reinforcement learning, in which the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We take the position that each stage of such a curriculum should be tailored to the current ability of the agent in order to promote learning new behaviors. Thus, as a first step towards creating a curriculum, the trainer must be able to create novel, agent-specific source tasks. We explore how such a space of useful tasks can be created using a parameterized model of the domain and observed trajectories on the target task. We experimentally show that these methods can be used to form components of a curriculum and that such a curriculum can be used successfully for transfer learning in 2 challenging multiagent reinforcement learning domains.},
annote = {Pick tasks according to agent skill.
Apply task/action simplification (limit env param space or remove actions that are specified as mistakes, also pick start states where trajectories give good return.
Approaches that require env-specific tailoring},
author = {Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar et al. - Unknown - Source Task Creation for Curriculum Learning.pdf:pdf},
isbn = {9781450342391},
issn = {15582914},
keywords = {Curriculum learning,Reinforcement learning,Transfer learning,env-type:parametrized,env:custom},
mendeley-tags = {env-type:parametrized,env:custom},
pages = {566--574},
title = {{Source task creation for curriculum learning}},
url = {www.ifaamas.org},
year = {2016}
}
@techreport{Anonymous2021,
abstract = {Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the $\backslash$emph{\{}implicit curricula{\}} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of $\backslash$emph{\{}explicit curricula{\}}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.},
archivePrefix = {arXiv},
arxivId = {2012.03107},
author = {Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
booktitle = {ICLR submission},
eprint = {2012.03107},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Dyer, Neyshabur - 2020 - When Do Curricula Work(2).pdf:pdf},
number = {April},
pages = {80--89},
title = {{When Do Curricula Work?}},
url = {http://arxiv.org/abs/2012.03107},
year = {2020}
}
@article{Jain2017,
abstract = {In this work, we propose several online methods to build a $\backslash$emph{\{}learning curriculum{\}} from a given set of target-task-specific training tasks in order to speed up reinforcement learning (RL). These methods can decrease the total training time needed by an RL agent compared to training on the target task from scratch. Unlike traditional transfer learning, we consider creating a sequence from several training tasks in order to provide the most benefit in terms of reducing the total time to train. Our methods utilize the learning trajectory of the agent on the curriculum tasks seen so far to decide which tasks to train on next. An attractive feature of our methods is that they are weakly coupled to the choice of the RL algorithm as well as the transfer learning method. Further, when there is domain information available, our methods can incorporate such knowledge to further speed up the learning. We experimentally show that these methods can be used to obtain suitable learning curricula that speed up the overall training time on two different domains.},
annote = {Pick next task according to metric (run policy and pick task with highest return)
If task has pre-defined features, can also learn this (regression).
Very basic, toy tasks, weak baseline},
archivePrefix = {arXiv},
arxivId = {1703.07853},
author = {Jain, Vikas and Tulabandhula, Theja},
eprint = {1703.07853},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jain, Tulabandhula - 2017 - Faster Reinforcement Learning Using Active Simulators.pdf:pdf},
journal = {arXiv},
keywords = {env-type:set,env:gridworld,reward weights},
mendeley-tags = {env-type:set,env:gridworld,reward weights},
month = {mar},
title = {{Faster Reinforcement Learning Using Active Simulators}},
url = {http://arxiv.org/abs/1703.07853},
year = {2017}
}
@misc{Zhou2019,
abstract = {Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.},
archivePrefix = {arXiv},
arxivId = {1906.03352},
author = {Zhou, Allan and Jang, Eric and Kappler, Daniel and Herzog, Alex and Khansari, Mohi and Wohlhart, Paul and Bai, Yunfei and Kalakrishnan, Mrinal and Levine, Sergey and Finn, Chelsea},
booktitle = {arXiv},
eprint = {1906.03352},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2019 - Watch, try, learn Meta-learning from demonstrations and rewards.pdf:pdf},
keywords = {IRL,env:gripper},
mendeley-tags = {IRL,env:gripper},
title = {{Watch, try, learn: Meta-learning from demonstrations and rewards}},
url = {https://sites.google.com/view/watch-try-learn-project},
year = {2019}
}
@article{Zhao2020,
abstract = {Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can $\backslash$emph{\{}also{\}} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only {\$}8{\$} hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images.},
archivePrefix = {arXiv},
arxivId = {2010.13957},
author = {Zhao, Tony Z. and Nagabandi, Anusha and Rakelly, Kate and Finn, Chelsea and Levine, Sergey},
eprint = {2010.13957},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2020 - MELD Meta-Reinforcement Learning from Images via Latent State Models.pdf:pdf},
keywords = {env:mujoco,env:robotics,env:sim2real,meta-learning,reinforcement learning,robotic manipulation},
mendeley-tags = {env:mujoco,env:robotics,env:sim2real},
month = {oct},
title = {{MELD: Meta-Reinforcement Learning from Images via Latent State Models}},
url = {http://arxiv.org/abs/2010.13957},
year = {2020}
}
@techreport{Raparthy2020,
abstract = {Goal-directed Reinforcement Learning (RL) traditionally considers an agent interacting with an environment, prescribing a real-valued reward to an agent proportional to the completion of some goal. Goal-directed RL has seen large gains in sample efficiency, due to the ease of reusing or generating new experience by proposing goals. In this work, we build on the framework of self-play, allowing an agent to interact with itself in order to make progress on some unknown task. We use Active Domain Randomization and self-play to create a novel, coupled environment-goal curriculum, where agents learn through progressively more difficult tasks and environment variations. Our method, Self-Supervised Active Domain Randomization (SS-ADR), generates a growing curriculum, encouraging the agent to try tasks that are just outside of its current capabilities, while building a domain-randomization curriculum that enables state-of-the-art results on various sim2real transfer tasks. Our results show that a curriculum of co-evolving the environment difficulty along with the difficulty of goals set in each environment, provides practical benefits in the goal-directed tasks tested.},
annote = {Teacher tries to choose a task that's just a bit too hard - easy for teacher to solve (ref env) but hard for agent (on random env)
Teacher is the old agent, does a bunch of actions in reference env until stopping, gives that goal to the agent. Agent has random env (torque/weight/{\ldots}) so it's hopefully harder},
archivePrefix = {arXiv},
arxivId = {2002.07911},
author = {Raparthy, Sharath Chandra and Mehta, Bhairav and Golemo, Florian and Paull, Liam},
booktitle = {arXiv},
eprint = {2002.07911},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raparthy et al. - 2020 - Generating automatic curricula via self-supervised active domain randomization.pdf:pdf},
keywords = {diversity:discriminator,env-type:parametrized,env:custom,env:sim2real},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:custom,env:sim2real},
title = {{Generating automatic curricula via self-supervised active domain randomization}},
year = {2020}
}
@inproceedings{Narvekar2017,
abstract = {Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain, and automatically sequencing such tasks into a curriculum. Finally, we also present ideas for future work.},
author = {Narvekar, Sanmit},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/757},
file = {::},
isbn = {9780999241103},
issn = {10450823},
pages = {5195--5196},
title = {{Curriculum learning in reinforcement learning}},
year = {2017}
}
@misc{Zhang2020,
abstract = {Continually solving new, unsolved tasks is the key to learning diverse behaviors. Through reinforcement learning (RL), we have made massive strides towards solving tasks that have a single goal. However, in the multi-task domain, where an agent needs to reach multiple goals, the choice of training goals can largely affect sample efficiency. When biological agents learn, there is often an organized and meaningful order to which learning happens. Inspired by this, we propose setting up an automatic curriculum for goals that the agent needs to solve. Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a significantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy. This simple technique samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement. We evaluate our method across 13 multi-goal robotic tasks and 5 navigation tasks, and demonstrate performance gains over current state-of-the-art methods.},
annote = {Pick goals by looking at value function estimation.
Specifically, estimated confidence of V is indicative of difficulty (hard/easy=high confidence).
To estimate confidence, look at disagreement of ensamble.},
archivePrefix = {arXiv},
arxivId = {2006.09641},
author = {Zhang, Yunzhi and Abbeel, Pieter and Pinto, Lerrel},
booktitle = {arXiv},
eprint = {2006.09641},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Abbeel, Pinto - Unknown - Automatic Curriculum Learning through Value Disagreement.pdf:pdf},
keywords = {env-type:parametrized,env:gripper,env:mujoco,medium difficulty},
mendeley-tags = {env-type:parametrized,env:gripper,env:mujoco,medium difficulty},
title = {{Automatic curriculum learning through value disagreement}},
url = {https://github.com/zzyunzhi/vds.},
year = {2020}
}
@techreport{Justesen2018,
abstract = {Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.},
annote = {Increase difficulty each time agent achieves goal, decrease every time it doesn't (env difficulty defined by manual specification of requirements)},
archivePrefix = {arXiv},
arxivId = {1806.10729},
author = {Justesen, Niels and Torrado, Ruben Rodriguez and Bontrager, Philip and Khalifa, Ahmed and Togelius, Julian and Risi, Sebastian},
booktitle = {arXiv},
eprint = {1806.10729},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Justesen et al. - 2018 - Illuminating generalization in deep reinforcement learning through procedural level generation.pdf:pdf},
isbn = {1806.10729v5},
keywords = {env-type:parametrized,env:custom,reward weights},
mendeley-tags = {env-type:parametrized,env:custom,reward weights},
title = {{Illuminating generalization in deep reinforcement learning through procedural level generation}},
url = {https://contest.openai.com/},
year = {2018}
}
@article{Ha2018,
abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
annote = {Motivtion: learn dynamics model for visual tasks

Algorithm: VAE with RNN and a linear controller

Method: RNN models the transition dynamics. The VAE is only needed for visual feature encoding

Novelty: first successful model learning with RNN

Theory: none},
archivePrefix = {arXiv},
arxivId = {1803.10122},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
doi = {10.5281/zenodo.1207631},
eprint = {1803.10122},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha, Schmidhuber - 2018 - World Models.pdf:pdf},
month = {mar},
title = {{World Models}},
url = {https://worldmodels.github.io http://arxiv.org/abs/1803.10122 http://dx.doi.org/10.5281/zenodo.1207631},
year = {2018}
}
@techreport{Srinivasan2019,
abstract = {In this paper, we present a technique that improves the process of training an agent (using RL) for instruction following. We develop a training curriculum that uses a nominal number of expert demonstrations and trains the agent in a manner that draws parallels from one of the ways in which humans learn to perform complex tasks, i.e by starting from the goal and working backwards. We test our method on the BabyAI platform and show an improvement in sample efficiency for some of its tasks compared to a PPO (proximal policy optimization) baseline.},
annote = {2019 paper by Bengio's group
Similar approach to reverse curriculum generation but for discrete state/action
Use expert demonstrations to find what close states are (since random walk can get us back to the goal)},
archivePrefix = {arXiv},
arxivId = {1912.00444},
author = {Srinivasan, Anirudh and Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Bengio, Yoshua},
booktitle = {arXiv},
eprint = {1912.00444},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivasan et al. - 2019 - Automated curriculum generation for policy gradients from demonstrations.pdf:pdf},
keywords = {alg:ppo,env-type:custom,env:toy,fixed curriculum,medium difficulty},
mendeley-tags = {alg:ppo,env-type:custom,env:toy,fixed curriculum,medium difficulty},
title = {{Automated curriculum generation for policy gradients from demonstrations}},
year = {2019}
}
@techreport{Alver2020,
abstract = {Due to the realization that deep reinforcement learning algorithms trained on high-dimensional tasks can strongly overfit to their training environments , there have been several studies that investigated the generalization performance of these algorithms. However, there has been no similar study that evaluated the generalization performance of algorithms that were specifically designed for generalization, i.e. meta-reinforcement learning algorithms. In this paper, we assess the generalization performance of these algorithms by leveraging high-dimensional, procedurally generated environments. We find that these algorithms can display strong overfitting when they are evaluated on challenging tasks. We also observe that scalability to high-dimensional tasks with sparse rewards remains a significant problem among many of the current meta-reinforcement learning algorithms. With these results, we highlight the need for developing meta-reinforcement learning algorithms that can both generalize and scale.},
archivePrefix = {arXiv},
arxivId = {2006.07262v3},
author = {Alver, Safa and Precup, Doina},
eprint = {2006.07262v3},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alver, Precup - 2020 - A Brief Look at Generalization in Visual Meta-Reinforcement Learning.pdf:pdf},
keywords = {alg:RL2,env:procgen,generalization},
mendeley-tags = {alg:RL2,env:procgen,generalization},
title = {{A Brief Look at Generalization in Visual Meta-Reinforcement Learning}},
year = {2020}
}
@techreport{Florensa2017,
abstract = {Many relevant tasks require an agent to reach a certain state or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in “reverse,” gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.},
annote = {Given goal states, use random motion in MDP to find more states
Use new starts as new curriculum tasks, do a policy training step
Label, filter, sort tasks by difficulty (want mid-difficulty tasks - reward in given range) for new policy},
archivePrefix = {arXiv},
arxivId = {1707.05300},
author = {Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael R. and Abbeel, Pieter},
booktitle = {arXiv},
eprint = {1707.05300},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Florensa et al. - 2017 - Reverse curriculum generation for reinforcement learning.pdf:pdf},
keywords = {Automatic Curriculum Generation,Reinforcement Learning,Robotic Manipulation,alg:trpo,env-type:randomized,env:custom,fixed curriculum,medium difficulty},
mendeley-tags = {alg:trpo,env-type:randomized,env:custom,fixed curriculum,medium difficulty},
title = {{Reverse curriculum generation for reinforcement learning}},
year = {2017}
}
@inproceedings{Finn2017,
abstract = {We propose an algorithm for mcta-lcaming that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.03400},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Finn, Abbeel, Levine - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks(2).pdf:pdf},
isbn = {9781510855144},
keywords = {alg:maml,env:mujoco},
mendeley-tags = {alg:maml,env:mujoco},
month = {mar},
pages = {1856--1868},
publisher = {International Machine Learning Society (IMLS)},
title = {{Model-agnostic meta-learning for fast adaptation of deep networks}},
url = {http://arxiv.org/abs/1703.03400},
volume = {3},
year = {2017}
}
@misc{Reny2019,
abstract = {Goal-oriented reinforcement learning has recently been a practical framework for robotic manipulation tasks, in which an agent is required to reach a certain goal defined by a function on the state space. However, the sparsity of such reward definition makes traditional reinforcement learning algorithms very inefficient. Hindsight Experience Replay (HER), a recent advance, has greatly improved sample efficiency and practical applicability for such problems. It exploits previous replays by constructing imaginary goals in a simple heuristic way, acting like an implicit curriculum to alleviate the challenge of sparse reward signal. In this paper, we introduce Hindsight Goal Generation (HGG), a novel algorithmic framework that generates valuable hindsight goals which are easy for an agent to achieve in the short term and are also potential for guiding the agent to reach the actual goal in the long term. We have extensively evaluated our goal generation algorithm on a number of robotic manipulation tasks and demonstrated substantially improvement over the original HER in terms of sample efficiency.},
annote = {HER with goals close to the real goal.
Force goal diversity by ensuring that for each trajectory in the buffer, at most one state is directly connected to the start state.},
archivePrefix = {arXiv},
arxivId = {1906.04279},
author = {Reny, Zhizhou and Dongy, Kefan and Zhou, Yuan and Liu, Qiang and Peng, Jian},
booktitle = {arXiv},
eprint = {1906.04279},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Exploration via Hindsight Goal Generation.pdf:pdf},
keywords = {diversity:other,env-type:randomized,env:gripper,experience replay,medium difficulty},
mendeley-tags = {diversity:other,env-type:randomized,env:gripper,experience replay,medium difficulty},
title = {{Exploration via hindsight goal generation}},
year = {2019}
}
@techreport{Fang2020,
abstract = {We introduce Adaptive Procedural Task Generation (APT-Gen), an approach for progressively generating a sequence of tasks as curricula to facilitate reinforcement learning in hard-exploration problems. At the heart of our approach, a task generator learns to create tasks via a black-box procedural generation module by adaptively sampling from the parameterized task space. To enable curriculum learning in the absence of a direct indicator of learning progress, the task generator is trained by balancing the agent's expected return in the generated tasks and their similarities to the target task. Through adversarial training, the similarity between the generated tasks and the target task is adaptively estimated by a task discriminator defined on the agent's behaviors. In this way, our approach can efficiently generate tasks of rich variations for target tasks of unknown parameterization or not covered by the predefined task space. Experiments demonstrate the effectiveness of our approach through quantitative and qualitative analysis in various scenarios.},
archivePrefix = {arXiv},
arxivId = {2007.00350},
author = {Fang, Kuan and Zhu, Yuke and Savarese, Silvio and Fei-Fei, Li},
booktitle = {arXiv},
eprint = {2007.00350},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang et al. - 2020 - Adaptive Procedural Task Generation for Hard-Exploration Problems.pdf:pdf},
keywords = {diversity:discriminator,env-type:parametrized,env:gridworld},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:gridworld},
title = {{Adaptive Procedural Task Generation for Hard-Exploration Problems}},
year = {2020}
}
@article{Lakshminarayanan2016,
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
annote = {Paper using adversarial ensambles to approximate bayesian NN for ML},
archivePrefix = {arXiv},
arxivId = {1612.01474},
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
eprint = {1612.01474},
file = {::},
month = {dec},
title = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
url = {http://arxiv.org/abs/1612.01474},
year = {2016}
}
@article{Matiisen2020,
abstract = {We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task, and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e., where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with long short-term memory (LSTM) and navigation in Minecraft. Our automatically ordered curriculum of submazes enabled to solve a Minecraft maze that could not be solved at all when training directly on that maze, and the learning was an order of magnitude faster than a uniform sampling of those submazes.},
annote = {2017 openAi paper, Cited by most interesting papers 
A teacher chooses tasks from a known set to give to a student -{\textgreater} POMDP since the student state is unknown and we only see the score
Need to define: task difficulty, task mastery threshold, have a method that re-introduces old tasks (neural network forgetting)
Focus on algorithms that give tasks the student will improve quickly (high learning curve for reward) and are probabilistic
Fast improvement -{\textgreater} slow down when approaching optimal -{\textgreater} new task now shows faster improvement
Ideally start and end when all tasks are uniform
Use absolute value of curve to handle forgetting},
archivePrefix = {arXiv},
arxivId = {1707.00183},
author = {Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
doi = {10.1109/TNNLS.2019.2934906},
eprint = {1707.00183},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matiisen et al. - 2020 - Teacher-Student Curriculum Learning.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Active learning,curriculum learning,deep reinforcement learning,env-type:randomized,env:minecraft,learning progress,reward weights},
mendeley-tags = {env-type:randomized,env:minecraft,reward weights},
number = {9},
pages = {3732--3740},
pmid = {31502993},
publisher = {IEEE},
title = {{Teacher-Student Curriculum Learning}},
volume = {31},
year = {2020}
}
@article{Sukhbaatar2017,
abstract = {We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.},
archivePrefix = {arXiv},
arxivId = {1703.05407},
author = {Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
eprint = {1703.05407},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sukhbaatar et al. - 2017 - Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play.pdf:pdf},
month = {mar},
number = {i},
pages = {1--16},
title = {{Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play}},
url = {http://arxiv.org/abs/1703.05407},
year = {2017}
}
@misc{Xu2020a,
abstract = {Continuously learning to solve unseen tasks with limited experience has been extensively pursued in meta-learning and continual learning, but with restricted assumptions such as accessible task distributions, independently and identically distributed tasks, and clear task delineations. However, real-world physical tasks frequently violate these assumptions, resulting in performance degradation. This paper proposes a continual online model-based reinforcement learning approach that does not require pre-training to solve task-agnostic problems with unknown task boundaries. We maintain a mixture of experts to handle nonstationarity, and represent each different type of dynamics with a Gaussian Process to efficiently leverage collected data and expressively model uncertainty. We propose a transition prior to account for the temporal dependencies in streaming data and update the mixture online via sequential variational inference. Our approach reliably handles the task distribution shift by generating new models for never-before-seen dynamics and reusing old models for previously seen dynamics. In experiments, our approach outperforms alternative methods in non-stationary tasks, including classic control with changing dynamics and decision making in different driving scenarios. Codes available at: https://github.com/mxu34/mbrl-gpmm.},
archivePrefix = {arXiv},
arxivId = {2006.11441},
author = {Xu, Mengdi and Ding, Wenhao and Zhu, Jiacheng and Liu, Zuxin and Chen, Baiming and Zhao, Ding},
booktitle = {arXiv},
eprint = {2006.11441},
file = {::},
issn = {23318422},
title = {{Task-agnostic online reinforcement learning with an infinite mixture of Gaussian processes}},
url = {https://github.com/mxu34/mbrl-gpmm.},
year = {2020}
}
@techreport{Gutierrez2020,
abstract = {In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of tasks to prepare for and learn faster in new, unseen, but related tasks. The training tasks are usually hand-crafted to be representative of the expected distribution of test tasks and hence all used in training. We show that given a set of training tasks, learning can be both faster and more effective (leading to better performance in the test tasks), if the training tasks are appropriately selected. We propose a task selection algorithm, Information-Theoretic Task Selection (ITTS), based on information theory, which optimizes the set of tasks used for training in meta-RL, irrespectively of how they are generated. The algorithm establishes which training tasks are both sufficiently relevant for the test tasks, and different enough from one another. We reproduce different meta-RL experiments from the literature and show that ITTS improves the final performance in all of them.},
annote = {2020 NeuroIPS
MetaRL, pre-defined training tasks, test task from same distribution
Method to choose the next task (curriculum) and also terminate
Assumes access to optimal policy for each task
Assumes a validation set of different tasks in addition to training tasks, pick training task set of different (KL divergence of optimal policies is big) and relevant (for any validation task, do X steps with policy, learn Y episodes to adapt to validation task, assert entropy of the adapted policy is lower = info gain from adapting = policy closer to deterministic after adapting)
It's much better than MAML/RL{\^{}}2 without task selection, but this is super inefficient},
archivePrefix = {arXiv},
arxivId = {2011.01054},
author = {Gutierrez, Ricardo Luna and Leonetti, Matteo},
eprint = {2011.01054},
keywords = {alg:RL2,alg:maml,diversity:entropy,env-type:set,env:toy},
mendeley-tags = {alg:RL2,alg:maml,diversity:entropy,env-type:set,env:toy},
title = {{Information-theoretic Task Selection for Meta-Reinforcement Learning}},
url = {http://arxiv.org/abs/2011.01054},
year = {2020}
}
@techreport{Wang2019,
abstract = {While the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. We test POET in a 2-D bipedal-walking obstacle-course domain in which POET can modify the types of challenges and their difficulty. At the same time, a neural network controlling a biped walker is optimized for each environment. The results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.},
archivePrefix = {arXiv},
arxivId = {1901.01753},
author = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.},
booktitle = {arXiv},
eprint = {1901.01753},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - Paired Open-Ended Trailblazer (POET) Endlessly generating increasingly complex and diverse learning environments an.pdf:pdf},
keywords = {alg:ES,env-type:parametrized,env:pybullet},
mendeley-tags = {alg:ES,env-type:parametrized,env:pybullet},
title = {{Paired Open-Ended Trailblazer (POET): Endlessly generating increasingly complex and diverse learning environments and their solutions}},
year = {2019}
}
@techreport{Al-Shedivat2017,
abstract = {The ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.},
annote = {MAML with changing task distribution due to self-play.
Use something like IS to handle mismatch},
archivePrefix = {arXiv},
arxivId = {1710.03641},
author = {Al-Shedivat, Maruan and Bansal, Trapit and Burda, Yura and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter},
booktitle = {arXiv},
eprint = {1710.03641},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Al-Shedivat et al. - 2017 - Continuous adaptation via meta-learning in nonstationary and competitive environments.pdf:pdf},
keywords = {adversarial,env-type:custom,env:custom},
mendeley-tags = {adversarial,env-type:custom,env:custom},
title = {{Continuous adaptation via meta-learning in nonstationary and competitive environments}},
url = {https://goo.gl/tboqaN.},
year = {2017}
}
@techreport{Mehta2019,
abstract = {Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. Our experiments across various physics-based simulated and real-robot tasks show that this enhancement leads to more robust, consistent policies.},
annote = {creates tasks by learning a “Stein Variational Policy Gradient” (SVPG) to learn “particles” (env parameters) with high entropy. 
Reward for env = how well a “discriminator” network can say that trajectory came from new env vs some reference env -{\textgreater} a policy=env is good for teaching if it's very different from the reference environment (similar notion to disagreement?)},
archivePrefix = {arXiv},
arxivId = {1904.04762},
author = {Mehta, Bhairav and Diaz, Manfred and Golemo, Florian and Pal, Christopher J. and Paull, Liams},
booktitle = {arXiv},
eprint = {1904.04762},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehta et al. - 2019 - Active domain randomization.pdf:pdf},
keywords = {Domain randomization,Reinforcement learning,Sim2real,diversity:discriminator,env-type:parametrized,env:sim2real,env:toy},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:sim2real,env:toy},
title = {{Active domain randomization}},
year = {2019}
}
@article{Curi2020,
abstract = {Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these two uncertainties for learning the model, they ignore it when optimizing the policy, which leads to greedy and insufficient exploration. At the same time, there are no practical solvers for optimistic exploration algorithms. In this paper, we propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL reparameterizes the set of plausible models and hallucinates control directly on the epistemic uncertainty. By augmenting the input space with the hallucinated inputs, H-UCRL can be solved using standard greedy planners. Furthermore, we analyze H-UCRL and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration significantly speeds-up learning when there are penalties on actions, a setting that is notoriously difficult for existing model-based reinforcement learning algorithms.},
annote = {Motivtion: (1) Previous MBRL work in tabular settings, or are computationally intractable (TS, UCRL), or do not explore effectively. (2) MB approaches can distinguish between aleatoric and epistemic uncertainty. 

Algorithm: essentially UCRL where confidence is derived from a probabilistic DNN, and a computationally tractable algorithm is suggested for solving the expanded MDP in UCRL.  

Method: reduce exploration to greedy exploitation over a re-parametrized cost. 

Novelty: first optimistic exploration algorithm for DNNs.  

Theory: regret bounds provided.},
archivePrefix = {arXiv},
arxivId = {2006.08684},
author = {Curi, Sebastian and Berkenkamp, Felix and Krause, Andreas},
eprint = {2006.08684},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Curi, Berkenkamp, Krause - Unknown - Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
month = {jun},
title = {{Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning}},
url = {http://arxiv.org/abs/2006.08684},
year = {2020}
}
@article{Narvekar2020a,
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
archivePrefix = {arXiv},
arxivId = {2003.04960},
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
eprint = {2003.04960},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar et al. - 2020 - Curriculum Learning for Reinforcement Learning Domains A Framework and Survey.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {curriculum learning,reinforcement learning,transfer learning},
month = {mar},
pages = {1--50},
title = {{Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey}},
url = {http://arxiv.org/abs/2003.04960},
volume = {21},
year = {2020}
}
@article{Chua2018,
abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
annote = {Motivtion: MBRL methods have poor asymptotic performance, using an ensemble (that approximates bayesian NN) can help

Algorithm: MPC with CEM sampling, the dynamics model is learned with an ensemble 

Method: 

Novelty: significant empirical improvement over model-free methods and simple model-based methods, mostly shown for complex tasks, medium timestep count

Theory: none},
archivePrefix = {arXiv},
arxivId = {1805.12114},
author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
eprint = {1805.12114},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {may},
pages = {4754--4765},
title = {{Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}},
url = {https://github.com/kchua/handful-of-trials http://arxiv.org/abs/1805.12114},
volume = {2018-Decem},
year = {2018}
}
@article{Wang2020a,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
eprint = {2010.13166},
file = {::},
title = {{A Comprehensive Survey on Curriculum Learning}},
url = {http://arxiv.org/abs/2010.13166},
year = {2020}
}
@techreport{Jiang2020,
abstract = {Simulated environments with procedurally generated content have become popular benchmarks for testing systematic generalization of reinforcement learning agents. Every level in such an environment is algorithmically created, thereby exhibiting a unique configuration of underlying factors of variation, such as layout, positions of entities, asset appearances, or even the rules governing environment transitions. Fixed sets of training levels can be determined to aid comparison and reproducibility, and test levels can be held out to evaluate the generalization and robustness of agents. While prior work samples training levels in a direct way (e.g.{\~{}}uniformly) for the agent to learn from, we investigate the hypothesis that different levels provide different learning progress for an agent at specific times during training. We introduce Prioritized Level Replay, a general framework for estimating the future learning potential of a level given the current state of the agent's policy. We find that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning potential in generating entire episodes that an agent would experience when replaying it. We report significantly improved sample-efficiency and generalization on the majority of Procgen Benchmark environments as well as two challenging MiniGrid environments. Lastly, we present a qualitative analysis showing that Prioritized Level Replay induces an implicit curriculum, taking the agent gradually from easier to harder levels.},
annote = {Off-policy HER-like sampling fix.
Assign env replay probability based on the TD-error of the agent
Add some optimistic “how long ago we last replayed this level” bonus},
archivePrefix = {arXiv},
arxivId = {2010.03934},
author = {Jiang, Minqi and Grefenstette, Ed and Rockt{\"{a}}schel, Tim},
eprint = {2010.03934},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Grefenstette, Rockt{\"{a}}schel - 2020 - Prioritized Level Replay.pdf:pdf},
issn = {23318422},
keywords = {env-type:parametrized,env:procgen,reward weights},
mendeley-tags = {env-type:parametrized,env:procgen,reward weights},
title = {{Prioritized Level Replay}},
url = {http://arxiv.org/abs/2010.03934},
year = {2020}
}
@inproceedings{Wohlke2020,
abstract = {Sparse reward problems present a challenge for reinforcement learning (RL) agents. Previous work has shown that choosing start states according to a curriculum can significantly improve the learning performance. We observe that many existing curriculum generation algorithms rely on two key components: Performance measure estimation and a start selection policy. Therefore, we propose a unifying framework for performance-based start state curricula in RL, which allows to analyze and compare the performance influence of the two key components. Furthermore, a new start state selection policy using spatial performance measure gradients is introduced. We conduct extensive empirical evaluations to compare performance-based start state curricula and investigate the influence of performance measure model choice and estimation. Benchmarking on difficult robotic navigation tasks and a high-dimensional robotic manipulation task, we demonstrate state-of-the-art performance of our novel spatial gradient curriculum.},
annote = {Pick start state according to value function estimate.
Can reset to new start state (from predefined set) whenever.
Single fixed, sparse-reward task.
Measured on probability to reach goal given random start state.},
author = {W{\"{o}}hlke, Jan and Schmitt, Felix and van Hoof, Herke},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/W{\"{o}}hlke, Schmitt, Van Hoof - 2020 - A Performance-Based Start State Curriculum Framework for Reinforcement Learning.pdf:pdf},
isbn = {9781450375184},
issn = {15582914},
keywords = {Learning agent capabilities,Machine learning for robotics,Reinforcement learning,env-type:set,env:gridworld,env:gripper,env:mujoco,goal reachability,reward weights},
mendeley-tags = {env-type:set,env:gridworld,env:gripper,env:mujoco,goal reachability,reward weights},
pages = {1503--1511},
title = {{A performance-based start state curriculum framework for reinforcement learning}},
url = {www.ifaamas.org},
volume = {2020-May},
year = {2020}
}
@techreport{Portelas2020a,
abstract = {Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.},
archivePrefix = {arXiv},
arxivId = {2003.04664},
author = {Portelas, R{\'{e}}my and Colas, C{\'{e}}dric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre Yves},
booktitle = {arXiv},
doi = {10.24963/ijcai.2020/671},
eprint = {2003.04664},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Portelas et al. - 2020 - Automatic curriculum learning for deep RL A short survey.pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
title = {{Automatic curriculum learning for deep RL: A short survey}},
year = {2020}
}
@misc{Raghu2019,
abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains – is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of a MAML-trained network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
archivePrefix = {arXiv},
arxivId = {1909.09157},
author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
booktitle = {arXiv},
eprint = {1909.09157},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raghu et al. - 2019 - Rapid learning or feature reuse towards understanding the effectiveness of MAML.pdf:pdf},
keywords = {alg:maml,env:mujoco,generalization},
mendeley-tags = {alg:maml,env:mujoco,generalization},
title = {{Rapid learning or feature reuse? towards understanding the effectiveness of MAML}},
year = {2019}
}
@misc{Zintgraf2019,
abstract = {Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We also evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher return during training than existing methods.},
archivePrefix = {arXiv},
arxivId = {1910.08348},
author = {Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
booktitle = {arXiv},
eprint = {1910.08348},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zintgraf et al. - 2019 - variBAD A very good method for bayes-adaptive deep RL via meta-learning.pdf:pdf},
keywords = {env:gridworld,env:mujoco},
mendeley-tags = {env:gridworld,env:mujoco},
title = {{variBAD: A very good method for bayes-adaptive deep RL via meta-learning}},
year = {2019}
}
@article{Raileanu2020,
abstract = {Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.},
annote = {Motivtion: intrinsic rewards based on state visit count don't work for procedurally-generated environments, and rewards based on surprise diminish as training continues. A method based on actions with high impact

Algorithm: Learn a dynmaics model with both forward (next state prediction) and inverse (action prediction), and use it to estimate differences between states

Method: high diference in state representation as impact, state visit count to prevent looping

Novelty: an interesting combination of both major intinsic rewards for exploration, learning a state representation via model learning

Theory: none},
archivePrefix = {arXiv},
arxivId = {2002.12292},
author = {Raileanu, Roberta and Rockt{\"{a}}schel, Tim},
eprint = {2002.12292},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raileanu, Rockt{\"{a}}schel - 2020 - RIDE Rewarding Impact-Driven Exploration for Procedurally-Generated Environments.pdf:pdf},
issn = {23318422},
month = {feb},
title = {{RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments}},
url = {http://arxiv.org/abs/2002.12292},
year = {2020}
}
@article{Xu2020,
abstract = {Deep reinforcement learning includes a broad family of algorithms that parameterise an internal representation, such as a value function or policy, by a deep neural network. Each algorithm optimises its parameters with respect to an objective, such as Q-learning or policy gradient, that defines its semantics. In this work, we propose an algorithm based on meta-gradient descent that discovers its own objective, flexibly parameterised by a deep neural network, solely from interactive experience with its environment. Over time, this allows the agent to learn how to learn increasingly effectively. Furthermore, because the objective is discovered online, it can adapt to changes over time. We demonstrate that the algorithm discovers how to address several important issues in RL, such as bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm adapts over time to learn with greater efficiency, eventually outperforming the median score of a strong actor-critic baseline.},
archivePrefix = {arXiv},
arxivId = {2007.08433},
author = {Xu, Zhongwen and van Hasselt, Hado and Hessel, Matteo and Oh, Junhyuk and Singh, Satinder and Silver, David},
eprint = {2007.08433},
journal = {arXiv},
keywords = {env:atari},
mendeley-tags = {env:atari},
month = {jul},
title = {{Meta-Gradient Reinforcement Learning with an Objective Discovered Online}},
url = {http://arxiv.org/abs/2007.08433},
year = {2020}
}
@techreport{Feng2020,
abstract = {Despite significant progress in general AI planning, certain domains remain out of reach of current AI planning systems. Sokoban is a PSPACE-complete planning task and represents one of the hardest domains for current AI planners. Even domain-specific specialized search methods fail quickly due to the exponential search complexity on hard instances. Our approach based on deep reinforcement learning augmented with a curriculum-driven method is the first one to solve hard instances within one day of training while other modern solvers cannot solve these instances within any reasonable time limit. In contrast to prior efforts, which use carefully handcrafted pruning techniques, our approach automatically uncovers domain structure. Our results reveal that deep RL provides a promising framework for solving previously unsolved AI planning problems, provided a proper training curriculum can be devised.},
annote = {Curriculum is for each new task (=board)
A fixed stopping point to ensure it terminates
Curriculum is domain-specific (number of boxes) and increased over time (difficulty increase)
Use MCTS to solve tasks, and do alphaZero style update},
archivePrefix = {arXiv},
arxivId = {2006.02689},
author = {Feng, Dieqiao and Gomes, Carla P. and Selman, Bart},
booktitle = {arXiv},
doi = {10.24963/ijcai.2020/304},
eprint = {2006.02689},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng, Gomes, Selman - 2020 - Solving Hard AI Planning Instances Using Curriculum-Driven Deep Reinforcement Learning.pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
keywords = {env-type:parametrized,env:custom,fixed curriculum},
mendeley-tags = {env-type:parametrized,env:custom,fixed curriculum},
title = {{Solving Hard AI Planning Instances Using Curriculum-Driven Deep Reinforcement Learning}},
year = {2020}
}
@techreport{Li2019,
abstract = {We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple datasets collected from different tasks, we train a multi-task policy to perform well in unseen tasks sampled from the same distribution. The task identities of the unseen tasks are not provided. To perform well, the policy must infer the task identity from collected transitions by modelling its dependency on states, actions and rewards. Because the different datasets may have state-action distributions with large divergence, the task inference module can learn to ignore the rewards and spuriously correlate {\$}\backslashtextit{\{}only{\}}{\$} state-action pairs to the task identity, leading to poor test time performance. To robustify task inference, we propose a novel application of the triplet loss. To mine hard negative examples, we relabel the transitions from the training tasks by approximating their reward functions. When we allow further training on the unseen tasks, using the trained policy as an initialization leads to significantly faster convergence compared to randomly initialized policies (up to {\$}80\backslash{\%}{\$} improvement and across 5 different Mujoco task distributions). We name our method {\$}\backslashtextbf{\{}MBML{\}}{\$} ({\$}\backslashtextbf{\{}M{\}}\backslashtext{\{}ulti-task{\}}{\$} {\$}\backslashtextbf{\{}B{\}}\backslashtext{\{}atch{\}}{\$} RL with {\$}\backslashtextbf{\{}M{\}}\backslashtext{\{}etric{\}}{\$} {\$}\backslashtextbf{\{}L{\}}\backslashtext{\{}earning{\}}{\$}).},
archivePrefix = {arXiv},
arxivId = {1909.11373},
author = {Li, Jiachen and Vuong, Quan and Liu, Shuang and Liu, Minghua and Ciosek, Kamil and Ross, Keith and Christensen, Henrik Iskov and Su, Hao},
eprint = {1909.11373},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2019 - Multi-task Batch Reinforcement Learning with Metric Learning.pdf:pdf},
keywords = {alg:mbml,alg:pearl,env:d4rl,env:mujoco,task policy combination},
mendeley-tags = {alg:mbml,alg:pearl,env:d4rl,env:mujoco,task policy combination},
title = {{Multi-task Batch Reinforcement Learning with Metric Learning}},
url = {http://arxiv.org/abs/1909.11373},
year = {2019}
}
@techreport{Portelas2020,
abstract = {A major challenge in the Deep RL (DRL) community is to train agents able to generalize their control policy over situations never seen in training. Training on diverse tasks has been identified as a key ingredient for good generalization, which pushed researchers towards using rich procedural task generation systems controlled through complex continuous parameter spaces. In such complex task spaces, it is essential to rely on some form of Automatic Curriculum Learning (ACL) to adapt the task sampling distribution to a given learning agent, instead of randomly sampling tasks, as many could end up being either trivial or unfeasible. Since it is hard to get prior knowledge on such task spaces, many ACL algorithms explore the task space to detect progress niches over time, a costly tabula-rasa process that needs to be performed for each new learning agents, although they might have similarities in their capabilities profiles. To address this limitation, we introduce the concept of Meta-ACL, and formalize it in the context of black-box RL learners, i.e. algorithms seeking to generalize curriculum generation to an (unknown) distribution of learners. In this work, we present AGAIN, a first instantiation of Meta-ACL, and showcase its benefits for curriculum generation over classical ACL in multiple simulated environments including procedurally generated parkour environments with learners of varying morphologies. Videos and code are available at https://sites.google.com/view/meta-acl .},
annote = {Only seems to work for multiple students.

Add “knowledge component” KC per student, the student's return on uniformly picked tasks (pre-tests)
Pre-train student, pre-test it, find k most similar students by KC distance, get the trajectory of the best one (in terms of score on KC after training), use that trajectory to pick task distribution with a bandit},
archivePrefix = {arXiv},
arxivId = {2011.08463},
author = {Portelas, R{\'{e}}my and Romac, Cl{\'{e}}ment and Hofmann, Katja and Oudeyer, Pierre-Yves},
eprint = {2011.08463},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Portelas et al. - 2020 - Meta Automatic Curriculum Learning.pdf:pdf},
issn = {23318422},
keywords = {Automatic Curriculum Learning,Deep Reinforcement Learning,env-type:parametrized,env:pybullet,reward weights},
mendeley-tags = {env-type:parametrized,env:pybullet,reward weights},
title = {{Meta Automatic Curriculum Learning}},
url = {http://arxiv.org/abs/2011.08463},
year = {2020}
}
@article{Milano2021,
abstract = {We demonstrate how an evolutionary algorithm can be extended with a curriculum learning process that selects automatically the environmental conditions in which the evolving agents are evaluated. The environmental conditions are selected so to adjust the level of difficulty to the ability level of the current evolving agents and so to challenge the weaknesses of the evolving agents. The method does not require domain knowledge and does not introduce additional hyperparameters. The results collected on two benchmark problems, that require to solve a task in significantly varying environmental conditions, demonstrate that the method proposed outperforms conventional algorithms and generates solutions that are robust to variations},
archivePrefix = {arXiv},
arxivId = {2102.08849},
author = {Milano, Nicola and Nolfi, Stefano},
eprint = {2102.08849},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Milano, Nolfi - 2021 - Automated Curriculum Learning for Embodied Agents A Neuroevolutionary Approach.pdf:pdf},
month = {feb},
title = {{Automated Curriculum Learning for Embodied Agents: A Neuroevolutionary Approach}},
url = {http://arxiv.org/abs/2102.08849},
year = {2021}
}
@techreport{Akkaya2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems.},
annote = {Fix 1 param, sample the rest, measure performance, increase or decrease param according to improvement},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
booktitle = {arXiv},
eprint = {1910.07113},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Akkaya et al. - 2019 - Solving Rubik's cube with a robot hand.pdf:pdf},
keywords = {diversity:entropy,env-type:parametrized,env:sim2real,fixed curriculum},
mendeley-tags = {diversity:entropy,env-type:parametrized,env:sim2real,fixed curriculum},
title = {{Solving Rubik's cube with a robot hand}},
url = {https://openai.com/bibtex/openai2019rubiks.bib},
year = {2019}
}
@misc{Cobbe2018,
abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
booktitle = {arXiv},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cobbe et al. - 2018 - Quantifying Generalization in Reinforcement Learning.pdf:pdf},
issn = {2640-3498},
keywords = {alg:impala,env:procgen,generalization},
mendeley-tags = {alg:impala,env:procgen,generalization},
month = {may},
pages = {1282--1289},
publisher = {PMLR},
title = {{Quantifying Generalization in Reinforcement Learning}},
url = {http://proceedings.mlr.press/v97/cobbe19a.html},
year = {2018}
}
@article{Co-Reyes2021,
abstract = {We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.},
archivePrefix = {arXiv},
arxivId = {2101.03958},
author = {Co-Reyes, John D and Miao, Yingjie and Peng, Daiyi and Real, Esteban and Levine, Sergey and Le, Quoc V and Lee, Honglak and Faust, Aleksandra},
eprint = {2101.03958},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Co-Reyes et al. - Unknown - EVOLVING REINFORCEMENT LEARNING ALGORITHMS.pdf:pdf},
title = {{Evolving Reinforcement Learning Algorithms}},
url = {http://arxiv.org/abs/2101.03958},
year = {2021}
}
