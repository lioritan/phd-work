Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Co-Reyes2021,
abstract = {We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free RL agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like DQN, enabling interpretable modifications which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference (TD) algorithm. Bootstrapped from DQN, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed RL algorithms that address overestimation in value-based methods.},
archivePrefix = {arXiv},
arxivId = {2101.03958},
author = {Co-Reyes, John D and Miao, Yingjie and Peng, Daiyi and Real, Esteban and Levine, Sergey and Le, Quoc V and Lee, Honglak and Faust, Aleksandra},
eprint = {2101.03958},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Co-Reyes et al. - Unknown - EVOLVING REINFORCEMENT LEARNING ALGORITHMS.pdf:pdf},
title = {{Evolving Reinforcement Learning Algorithms}},
url = {http://arxiv.org/abs/2101.03958},
year = {2021}
}
@article{Zhao2020,
abstract = {Meta-reinforcement learning algorithms can enable autonomous agents, such as robots, to quickly acquire new behaviors by leveraging prior experience in a set of related training tasks. However, the onerous data requirements of meta-training compounded with the challenge of learning from sensory inputs such as images have made meta-RL challenging to apply to real robotic systems. Latent state models, which learn compact state representations from a sequence of observations, can accelerate representation learning from visual inputs. In this paper, we leverage the perspective of meta-learning as task inference to show that latent state models can $\backslash$emph{\{}also{\}} perform meta-learning given an appropriately defined observation space. Building on this insight, we develop meta-RL with latent dynamics (MELD), an algorithm for meta-RL from images that performs inference in a latent state model to quickly acquire new skills given observations and rewards. MELD outperforms prior meta-RL methods on several simulated image-based robotic control problems, and enables a real WidowX robotic arm to insert an Ethernet cable into new locations given a sparse task completion signal after only {\$}8{\$} hours of real world meta-training. To our knowledge, MELD is the first meta-RL algorithm trained in a real-world robotic control setting from images.},
archivePrefix = {arXiv},
arxivId = {2010.13957},
author = {Zhao, Tony Z. and Nagabandi, Anusha and Rakelly, Kate and Finn, Chelsea and Levine, Sergey},
eprint = {2010.13957},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2020 - MELD Meta-Reinforcement Learning from Images via Latent State Models.pdf:pdf},
keywords = {env:mujoco,env:robotics,env:sim2real,meta-learning,reinforcement learning,robotic manipulation},
mendeley-tags = {env:mujoco,env:robotics,env:sim2real},
month = {oct},
title = {{MELD: Meta-Reinforcement Learning from Images via Latent State Models}},
url = {http://arxiv.org/abs/2010.13957},
year = {2020}
}
@article{Florensa2018,
abstract = {Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing. We use a generator network to propose tasks for the agent to try to achieve, specified as goal states. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.},
archivePrefix = {arXiv},
arxivId = {1705.06366},
author = {Florensa, Carlos and Held, David and Geng, Xinyang and Abbeel, Pieter},
eprint = {1705.06366},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Held et al. - 2018 - Automatic Goal Generation for Reinforcement Learning Agents.pdf:pdf},
isbn = {9781510867963},
journal = {35th International Conference on Machine Learning, ICML 2018},
month = {may},
pages = {2458--2471},
title = {{Automatic Goal Generation for Reinforcement Learning Agents}},
url = {https://sites.google.com/view/ http://arxiv.org/abs/1705.06366},
volume = {4},
year = {2017}
}
@article{Won2019,
abstract = {Recently, deep reinforcement learning (DRL) has attracted great attention in designing controllers for physics-based characters. Despite the recent success of DRL, the learned controller is viable for a single character. Changes in body size and proportions require learning controllers from scratch. In this paper, we present a new method of learning parametric controllers for body shape variation. A single parametric controller enables us to simulate and control various characters having different heights, weights, and body proportions. The users are allowed to create new characters through body shape parameters, and they can control the characters immediately. Our characters can also change their body shapes on the fly during simulation. The key to the success of our approach includes the adaptive sampling of body shapes that tackles the challenges in learning parametric controllers, which relies on the marginal value function that measures control capabilities of body shapes.We demonstrate parametric controllers for various physically simulated characters such as bipeds, quadrupeds, and underwater animals.},
author = {Won, Jungdam and Lee, Jehee},
doi = {10.1145/3355089.3356499},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Won, Lee - 2019 - Learning body shape variation in physics-based characters.pdf:pdf},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Character Animation,Deep Learning,Locomotion Control,Neural Network,Physics-based Simulation and Control,Reinforcement Learning},
number = {6},
title = {{Learning body shape variation in physics-based characters}},
url = {https://doi.org/10.1145/3355089.3356499.},
volume = {38},
year = {2019}
}
@article{Ding2021,
abstract = {Despite recent advances in its theoretical understanding, there still remains a significant gap in the ability of existing PAC-Bayesian theories on meta-learning to explain performance improvements in the few-shot learning setting, where the number of training examples in the target tasks is severely limited. This gap originates from an assumption in the existing theories which supposes that the number of training examples in the observed tasks and the number of training examples in the target tasks follow the same distribution, an assumption that rarely holds in practice. By relaxing this assumption, we develop two PAC-Bayesian bounds tailored for the few-shot learning setting and show that two existing meta-learning algorithms (MAML and Reptile) can be derived from our bounds, thereby bridging the gap between practice and PAC-Bayesian theories. Furthermore, we derive a new computationally-efficient PACMAML algorithm, and show it outperforms existing meta-learning algorithms on several few-shot benchmark datasets.},
archivePrefix = {arXiv},
arxivId = {2105.14099},
author = {Ding, Nan and Chen, Xi and Levinboim, Tomer and Goodman, Sebastian and Soricut, Radu},
eprint = {2105.14099},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2021 - Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot Meta-Learning.pdf:pdf},
month = {may},
title = {{Bridging the Gap Between Practice and PAC-Bayes Theory in Few-Shot Meta-Learning}},
url = {http://arxiv.org/abs/2105.14099},
year = {2021}
}
@inproceedings{Tian2020,
abstract = {The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code: http://github.com/WangYueFt/rfs/.},
archivePrefix = {arXiv},
arxivId = {2003.11539},
author = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-58568-6_16},
eprint = {2003.11539},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian et al. - 2020 - Rethinking Few-Shot Image Classification A Good Embedding is All You Need.pdf:pdf},
isbn = {9783030585679},
issn = {16113349},
pages = {266--282},
title = {{Rethinking Few-Shot Image Classification: A Good Embedding is All You Need?}},
url = {http://github.com/WangYueFt/},
volume = {12359 LNCS},
year = {2020}
}
@article{Narvekar2019,
abstract = {Curriculum learning in reinforcement learning is a training methodology that seeks to speed up learning of a difficult target task, by first training on a series of simpler tasks and transferring the knowledge acquired to the target task. Automatically choosing a sequence of such tasks (i.e. a curriculum) is an open problem that has been the subject of much recent work in this area. In this paper, we build upon a recent method for curriculum design, which formulates the curriculum sequencing problem as a Markov Decision Process. We extend this model to handle multiple transfer learning algorithms, and show for the first time that a curriculum policy over this MDP can be learned from experience. We explore various representations that make this possible, and evaluate our approach by learning curriculum policies for multiple agents in two different domains. The results show that our method produces curricula that can train agents to perform on a target task as fast or faster than existing methods.},
archivePrefix = {arXiv},
arxivId = {1812.00285},
author = {Narvekar, Sanmit and Stone, Peter},
eprint = {1812.00285},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar, Stone - 2019 - Learning curriculum policies for reinforcement learning(2).pdf:pdf},
isbn = {9781510892002},
issn = {15582914},
journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
keywords = {Curriculum learning,Reinforcement learning,Transfer learning,custom mdp,env-type:custom,env:gridworld,fixed curriculum},
mendeley-tags = {custom mdp,env-type:custom,env:gridworld,fixed curriculum},
month = {dec},
pages = {25--33},
title = {{Learning Curriculum Policies for Reinforcement Learning}},
url = {www.ifaamas.org http://arxiv.org/abs/1812.00285},
volume = {1},
year = {2018}
}
@article{Li,
abstract = {We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task reduction and self-imitation. Task reduction tackles a hard-to-solve task by actively reducing it to an easier task whose solution is known by the RL agent. Once the original hard task is successfully solved by task reduction, the agent naturally obtains a self-generated solution trajectory to imitate. By continuously collecting and imitating such demonstrations, the agent is able to progressively expand the solved subspace in the entire task space. Experiment results show that SIR can significantly accelerate and improve learning on a variety of challenging sparse-reward continuous-control problems with compositional structures.},
archivePrefix = {arXiv},
arxivId = {2103.07607},
author = {Li, Yunfei and Wu, Yilin and Xu, Huazhe and Wang, Xiaolong and Wu, Yi},
eprint = {2103.07607},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2021 - Solving Compositional Reinforcement Learning Problems via Task Reduction.pdf:pdf},
month = {mar},
title = {{Solving Compositional Reinforcement Learning Problems via Task Reduction}},
url = {https://sites.google.com/view/sir-compositional. http://arxiv.org/abs/2103.07607},
year = {2021}
}
@inproceedings{Finn2017,
abstract = {We propose an algorithm for mcta-lcaming that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.03400},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Finn, Abbeel, Levine - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks(2).pdf:pdf},
isbn = {9781510855144},
keywords = {alg:maml,env:mujoco},
mendeley-tags = {alg:maml,env:mujoco},
month = {mar},
pages = {1856--1868},
publisher = {International Machine Learning Society (IMLS)},
title = {{Model-agnostic meta-learning for fast adaptation of deep networks}},
url = {http://arxiv.org/abs/1703.03400},
volume = {3},
year = {2017}
}
@techreport{Portelas2020a,
abstract = {Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.},
archivePrefix = {arXiv},
arxivId = {2003.04664},
author = {Portelas, R{\'{e}}my and Colas, C{\'{e}}dric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre Yves},
booktitle = {arXiv},
doi = {10.24963/ijcai.2020/671},
eprint = {2003.04664},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Portelas et al. - 2020 - Automatic curriculum learning for deep RL A short survey.pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
title = {{Automatic curriculum learning for deep RL: A short survey}},
year = {2020}
}
@inproceedings{Hacohen2019,
abstract = {Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive "teacher" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function.},
archivePrefix = {arXiv},
arxivId = {1904.03626},
author = {Hacohen, Guy and Weinshall, Daphna},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1904.03626},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hacohen, Weinshall - 2019 - On the power of curriculum learning in training deep networks.pdf:pdf},
isbn = {9781510886988},
pages = {4483--4496},
title = {{On the power of curriculum learning in training deep networks}},
volume = {2019-June},
year = {2019}
}
@techreport{Akkaya2019,
abstract = {We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key components: a novel algorithm, which we call automatic domain randomization (ADR) and a robot platform built for machine learning. ADR automatically generates a distribution over randomized environments of ever-increasing difficulty. Control policies and vision state estimators trained with ADR exhibit vastly improved sim2real transfer. For control policies, memory-augmented models trained on an ADR-generated distribution of environments show clear signs of emergent meta-learning at test time. The combination of ADR with our custom robot platform allows us to solve a Rubik's cube with a humanoid robot hand, which involves both control and state estimation problems.},
annote = {Fix 1 param, sample the rest, measure performance, increase or decrease param according to improvement},
archivePrefix = {arXiv},
arxivId = {1910.07113},
author = {Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and Schneider, Jonas and Tezak, Nikolas and Tworek, Jerry and Welinder, Peter and Weng, Lilian and Yuan, Qiming and Zaremba, Wojciech and Zhang, Lei},
booktitle = {arXiv},
eprint = {1910.07113},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Akkaya et al. - 2019 - Solving Rubik's cube with a robot hand.pdf:pdf},
keywords = {diversity:entropy,env-type:parametrized,env:sim2real,fixed curriculum},
mendeley-tags = {diversity:entropy,env-type:parametrized,env:sim2real,fixed curriculum},
title = {{Solving Rubik's cube with a robot hand}},
url = {https://openai.com/bibtex/openai2019rubiks.bib},
year = {2019}
}
@techreport{Soviany,
abstract = {Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective tax-onomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work. Index Terms-machine learning, curriculum learning, learning from easy to hard, neural networks, deep learning.},
archivePrefix = {arXiv},
arxivId = {2101.10382v1},
author = {Soviany, Petru and Ionescu, Tudor and Rota, Paolo and Sebe, Nicu},
eprint = {2101.10382v1},
file = {::},
title = {{Curriculum Learning: A Survey}}
}
@article{Zhou2021a,
abstract = {We study a novel curriculum learning scheme where in each round, samples are selected to achieve the greatest progress and fastest learning speed towards the ground-truth on all available samples. Inspired by an analysis of optimization dynamics under gradient flow for both regression and classification, the problem reduces to selecting training samples by a score computed from samples' residual and linear temporal dynamics. It encourages the model to focus on the samples at the learning frontier, i.e., those with large loss but fast learning speed. The scores in discrete time can be estimated via already-available byproducts of training, and thus require negligible extra compute. We discuss the properties and potential advantages of the proposed dynamics optimization via current deep learning theory and empirical studies. By integrating it with cyclical training of neural networks, we introduce "dynamics-optimized curriculum learning (DoCL)", which selects the training set at each step by a weighted sampling based on the scores. On nine different datasets, DoCL significantly outperforms random mini-batch SGD and recent curriculum learning methods both in terms of efficiency and final performance.},
author = {Zhou, Tianyi and Bilmes, Jeff A},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou, Bilmes - 2021 - Curriculum Learning by Optimizing Learning Dynamics.pdf:pdf},
title = {{Curriculum Learning by Optimizing Learning Dynamics}},
volume = {130},
year = {2021}
}
@article{Jabri2019,
abstract = {In principle, meta-reinforcement learning algorithms leverage experience across many tasks to learn fast reinforcement learning (RL) strategies that transfer to similar tasks. However, current meta-RL approaches rely on manually-defined distributions of training tasks, and hand-crafting these task distributions can be challenging and time-consuming. Can "useful" pre-training tasks be discovered in an unsupervised manner? We develop an unsupervised algorithm for inducing an adaptive meta-training task distribution, i.e. an automatic curriculum, by modeling unsupervised interaction in a visual environment. The task distribution is scaffolded by a parametric density model of the meta-learner's trajectory distribution. We formulate unsupervised meta-RL as information maximization between a latent task variable and the meta-learner's data distribution, and describe a practical instantiation which alternates between integration of recent experience into the task distribution and meta-learning of the updated tasks. Repeating this procedure leads to iterative reorganization such that the curriculum adapts as the meta-learner's data distribution shifts. In particular, we show how discriminative clustering for visual representation can support trajectory-level task acquisition and exploration in domains with pixel observations, avoiding pitfalls of alternatives. In experiments on vision-based navigation and manipulation domains, we show that the algorithm allows for unsupervised meta-learning that transfers to downstream tasks specified by hand-crafted reward functions and serves as pre-training for more efficient supervised meta-learning of test task distributions.},
annote = {2019 Berkley paper
Behavior model - learned GMM (gaussian mixture model) from state to reward, unsupervised, used to create tasks, updated from trajectories (E-step)
Visual data, discriminative clustering (take advantage of other research in vision unsupervised data)
Hopefully behavior model learns the environment well, which may help (can't do better without reward functions)
Curriculum = since GMM is updated from trajectories, new tasks should try to find under-explored regions},
archivePrefix = {arXiv},
arxivId = {1912.04226},
author = {Jabri, Allan and Hsu, Kyle and Eysenbach, Benjamin and Gupta, Abhishek and Levine, Sergey and Finn, Chelsea},
eprint = {1912.04226},
journal = {arXiv},
keywords = {diversity:entropy,env-type:set,env:3dmaze,task embedding},
mendeley-tags = {diversity:entropy,env-type:set,env:3dmaze,task embedding},
month = {dec},
title = {{Unsupervised Curricula for Visual Meta-Reinforcement Learning}},
url = {http://arxiv.org/abs/1912.04226},
year = {2019}
}
@techreport{Alver2020,
abstract = {Due to the realization that deep reinforcement learning algorithms trained on high-dimensional tasks can strongly overfit to their training environments , there have been several studies that investigated the generalization performance of these algorithms. However, there has been no similar study that evaluated the generalization performance of algorithms that were specifically designed for generalization, i.e. meta-reinforcement learning algorithms. In this paper, we assess the generalization performance of these algorithms by leveraging high-dimensional, procedurally generated environments. We find that these algorithms can display strong overfitting when they are evaluated on challenging tasks. We also observe that scalability to high-dimensional tasks with sparse rewards remains a significant problem among many of the current meta-reinforcement learning algorithms. With these results, we highlight the need for developing meta-reinforcement learning algorithms that can both generalize and scale.},
archivePrefix = {arXiv},
arxivId = {2006.07262v3},
author = {Alver, Safa and Precup, Doina},
eprint = {2006.07262v3},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alver, Precup - 2020 - A Brief Look at Generalization in Visual Meta-Reinforcement Learning.pdf:pdf},
keywords = {alg:RL2,env:procgen,generalization},
mendeley-tags = {alg:RL2,env:procgen,generalization},
title = {{A Brief Look at Generalization in Visual Meta-Reinforcement Learning}},
year = {2020}
}
@article{Wang2021,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer+Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
doi = {10.1109/TPAMI.2021.3069908},
eprint = {2010.13166},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Chen, Zhu - 2021 - A Survey on Curriculum Learning.pdf:pdf},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computational modeling,Convergence,Curriculum Learning,Data models,Example Reweighting,Machine Learning,Machine learning,Machine learning algorithms,Self-Paced Learning,Task analysis,Training,Training Strategy},
number = {1},
title = {{A Survey on Curriculum Learning}},
year = {2021}
}
@techreport{Dennis2020,
abstract = {A wide range of reinforcement learning (RL) problems-including robustness, transfer learning, unsupervised RL, and emergent complexity-require specifying a distribution of tasks or environments in which a policy will be trained. However, creating a useful distribution of environments is error prone, and takes a significant amount of developer time and effort. We propose Unsupervised Environment Design (UED) as an alternative paradigm, where developers provide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difficulty of the environment to the agent's learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate struc-tured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, defined as the difference between the protagonist and antagonist agent's return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our experiments demonstrate that PAIRED produces a natural curriculum of increasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments.},
annote = {2020 December Berkley paper
Try to create a set of tasks/environments unsupervised
Gets input of environments with underspecified parameters (where to place obstacles in a maze) and learn legal parameter combinations that create environments that can be solved
Create a maze: random creates easy maze, adversarial creates unsolvable maze, let's try to make a hard but solvable maze
2 agents that try to learn the policy, “good” and “evil”
How to make sure it's hard and solvable: Reward is difference between “good” and “evil”, so unsolvable = 0 reward, environment generator tries to change env to improve “evil” agent policy without improving “good” agent policy too much
As “good” agent improves, env need to be harder, encouraged to find an easy env that “good” cannot solve (if task reward is also time-based)},
archivePrefix = {arXiv},
arxivId = {2012.02096v1},
author = {Dennis, Michael and Jaques, Natasha and Vinitsky, Eugene and Bayen, Alexandre and Russell, Stuart and Critch, Andrew and Levine, Sergey},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {2012.02096v1},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dennis et al. - 2020 - Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design.pdf:pdf},
keywords = {adversarial,env-type:parametrized,env:gridworld,goal reachability},
mendeley-tags = {adversarial,env-type:parametrized,env:gridworld,goal reachability},
number = {NeurIPS},
title = {{Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design}},
year = {2020}
}
@article{Narvekar2020a,
abstract = {Reinforcement learning (RL) is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning (CL) in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.},
archivePrefix = {arXiv},
arxivId = {2003.04960},
author = {Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
eprint = {2003.04960},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar et al. - 2020 - Curriculum Learning for Reinforcement Learning Domains A Framework and Survey.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {curriculum learning,reinforcement learning,transfer learning},
month = {mar},
pages = {1--50},
title = {{Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey}},
url = {http://arxiv.org/abs/2003.04960},
volume = {21},
year = {2020}
}
@misc{Zintgraf2019,
abstract = {Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent's uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We also evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher return during training than existing methods.},
archivePrefix = {arXiv},
arxivId = {1910.08348},
author = {Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
booktitle = {arXiv},
eprint = {1910.08348},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zintgraf et al. - 2019 - variBAD A very good method for bayes-adaptive deep RL via meta-learning.pdf:pdf},
keywords = {env:gridworld,env:mujoco},
mendeley-tags = {env:gridworld,env:mujoco},
title = {{variBAD: A very good method for bayes-adaptive deep RL via meta-learning}},
year = {2019}
}
@article{Liu2018,
abstract = {By simulating the easy-to-hard learning manners of humans/animals, the learning regimes called curriculum learning{\~{}}(CL) and self-paced learning{\~{}}(SPL) have been recently investigated and invoked broad interests. However, the intrinsic mechanism for analyzing why such learning regimes can work has not been comprehensively investigated. To this issue, this paper proposes a concave conjugacy theory for looking into the insight of CL/SPL. Specifically, by using this theory, we prove the equivalence of the SPL regime and a latent concave objective, which is closely related to the known non-convex regularized penalty widely used in statistics and machine learning. Beyond the previous theory for explaining CL/SPL insights, this new theoretical framework on one hand facilitates two direct approaches for designing new SPL models for certain tasks, and on the other hand can help conduct the latent objective of self-paced curriculum learning, which is the advanced version of both CL/SPL and possess advantages of both learning regimes to a certain extent. This further facilitates a theoretical understanding for SPCL, instead of only CL/SPL as conventional. Under this theory, we attempt to attain intrinsic latent objectives of two curriculum forms, the partial order and group curriculums, which easily follow the theoretical understanding of the corresponding SPCL regimes.},
archivePrefix = {arXiv},
arxivId = {1805.08096},
author = {Liu, Shiqi and Ma, Zilu and Meng, Deyu},
eprint = {1805.08096},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Ma, Meng - 2018 - Understanding Self-Paced Learning under Concave Conjugacy Theory.pdf:pdf},
journal = {Communications in Information and Systems},
month = {may},
number = {1},
pages = {1--35},
publisher = {International Press of Boston},
title = {{Understanding Self-Paced Learning under Concave Conjugacy Theory}},
url = {http://arxiv.org/abs/1805.08096},
volume = {18},
year = {2018}
}
@article{Zhou2020,
abstract = {Deep neural networks are highly effective when a large number of labeled samples are available but fail with few-shot classification tasks. Recently, meta-learning methods have received much attention, which train a meta-learner on massive additional tasks to gain the knowledge to instruct the few-shot classification. Usually, the training tasks are randomly sampled and performed indiscriminately, often making the meta-learner stuck into a bad local optimum. Some works in the optimization of deep neural networks have shown that a better arrangement of training data can make the classifier converge faster and perform better. Inspired by this idea, we propose an easy-to-hard expert meta-training strategy to arrange the training tasks properly, where easy tasks are preferred in the first phase, then, hard tasks are emphasized in the second phase. A task hardness aware module is designed and integrated into the training procedure to estimate the hardness of a task based on the distinguishability of its categories. In addition, we explore multiple hardness measurements including the semantic relation, the pairwise Euclidean distance, the Hausdorff distance, and the Hilbert-Schmidt independence criterion. Experimental results on the miniImageNet and tieredImageNetSketch datasets show that the meta-learners can obtain better results with our expert training strategy.},
archivePrefix = {arXiv},
arxivId = {2007.06240},
author = {Zhou, Yucan and Wang, Yu and Cai, Jianfei and Zhou, Yu and Hu, Qinghua and Wang, Weiping},
eprint = {2007.06240},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2020 - Expert Training Task Hardness Aware Meta-Learning for Few-Shot Classification.pdf:pdf},
keywords = {Curriculum Training,Few-Shot Image Classification,Index Terms-Meta-Learning,Task Hardness Estimation},
title = {{Expert Training: Task Hardness Aware Meta-Learning for Few-Shot Classification}},
url = {http://arxiv.org/abs/2007.06240},
year = {2020}
}
@misc{Zhang2020,
abstract = {Continually solving new, unsolved tasks is the key to learning diverse behaviors. Through reinforcement learning (RL), we have made massive strides towards solving tasks that have a single goal. However, in the multi-task domain, where an agent needs to reach multiple goals, the choice of training goals can largely affect sample efficiency. When biological agents learn, there is often an organized and meaningful order to which learning happens. Inspired by this, we propose setting up an automatic curriculum for goals that the agent needs to solve. Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a significantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy. This simple technique samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement. We evaluate our method across 13 multi-goal robotic tasks and 5 navigation tasks, and demonstrate performance gains over current state-of-the-art methods.},
annote = {Pick goals by looking at value function estimation.
Specifically, estimated confidence of V is indicative of difficulty (hard/easy=high confidence).
To estimate confidence, look at disagreement of ensamble.},
archivePrefix = {arXiv},
arxivId = {2006.09641},
author = {Zhang, Yunzhi and Abbeel, Pieter and Pinto, Lerrel},
booktitle = {arXiv},
eprint = {2006.09641},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Abbeel, Pinto - Unknown - Automatic Curriculum Learning through Value Disagreement.pdf:pdf},
keywords = {env-type:parametrized,env:gripper,env:mujoco,medium difficulty},
mendeley-tags = {env-type:parametrized,env:gripper,env:mujoco,medium difficulty},
title = {{Automatic curriculum learning through value disagreement}},
url = {https://github.com/zzyunzhi/vds.},
year = {2020}
}
@misc{Raghu2019,
abstract = {An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains – is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of a MAML-trained network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.},
archivePrefix = {arXiv},
arxivId = {1909.09157},
author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
booktitle = {arXiv},
eprint = {1909.09157},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raghu et al. - 2019 - Rapid learning or feature reuse towards understanding the effectiveness of MAML.pdf:pdf},
keywords = {alg:maml,env:mujoco,generalization},
mendeley-tags = {alg:maml,env:mujoco,generalization},
title = {{Rapid learning or feature reuse? towards understanding the effectiveness of MAML}},
year = {2019}
}
@article{Zhou2020a,
abstract = {Deep neural networks are highly effective when a large number of labeled samples are available but fail with few-shot classification tasks. Recently, meta-learning methods have received much attention, which train a meta-learner on massive additional tasks to gain the knowledge to instruct the few-shot classification. Usually, the training tasks are randomly sampled and performed indiscriminately, often making the meta-learner stuck into a bad local optimum. Some works in the optimization of deep neural networks have shown that a better arrangement of training data can make the classifier converge faster and perform better. Inspired by this idea, we propose an easy-to-hard expert meta-training strategy to arrange the training tasks properly, where easy tasks are preferred in the first phase, then, hard tasks are emphasized in the second phase. A task hardness aware module is designed and integrated into the training procedure to estimate the hardness of a task based on the distinguishability of its categories. In addition, we explore multiple hardness measurements including the semantic relation, the pairwise Euclidean distance, the Hausdorff distance, and the Hilbert-Schmidt independence criterion. Experimental results on the miniImageNet and tieredImageNetSketch datasets show that the meta-learners can obtain better results with our expert training strategy.},
archivePrefix = {arXiv},
arxivId = {2007.06240},
author = {Zhou, Yucan and Wang, Yu and Cai, Jianfei and Zhou, Yu and Hu, Qinghua and Wang, Weiping},
eprint = {2007.06240},
file = {::},
keywords = {Curriculum Training,Few-Shot Image Classification,Index Terms-Meta-Learning,Task Hardness Estimation},
month = {jul},
title = {{Expert Training: Task Hardness Aware Meta-Learning for Few-Shot Classification}},
url = {http://arxiv.org/abs/2007.06240},
year = {2020}
}
@article{Duan2016,
abstract = {Deep reinforcement learning (deep RL) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network (RNN) and learn it from data. In our proposed method, RL{\$}{\^{}}2{\$}, the algorithm is encoded in the weights of the RNN, which are learned slowly through a general-purpose ("slow") RL algorithm. The RNN receives all information a typical RL algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process (MDP). The activations of the RNN store the state of the "fast" RL algorithm on the current (previously unseen) MDP. We evaluate RL{\$}{\^{}}2{\$} experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite MDPs. After RL{\$}{\^{}}2{\$} is trained, its performance on new MDPs is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test RL{\$}{\^{}}2{\$} on a vision-based navigation task and show that it scales up to high-dimensional problems.},
archivePrefix = {arXiv},
arxivId = {1611.02779},
author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1611.02779},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duan et al. - 2016 - RL{\$}2{\$} Fast Reinforcement Learning via Slow Reinforcement Learning.pdf:pdf},
keywords = {alg:RL2,env:3dmaze},
mendeley-tags = {alg:RL2,env:3dmaze},
month = {nov},
title = {{RL{\$}{\^{}}2{\$}: Fast Reinforcement Learning via Slow Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02779},
year = {2016}
}
@inproceedings{Pentina2014,
abstract = {Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far. In this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.},
archivePrefix = {arXiv},
arxivId = {1311.2838},
author = {Pentina, Anastasia and Lampert, Christoph H},
booktitle = {31st International Conference on Machine Learning, ICML 2014},
eprint = {1311.2838},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pentina, Lampert - 2014 - A PAC-bayesian bound for lifelong learning.pdf:pdf},
isbn = {9781634393973},
pages = {2656--2664},
title = {{A PAC-bayesian bound for lifelong learning}},
volume = {3},
year = {2014}
}
@techreport{Mehta2020,
abstract = {Gradient-based meta-learners such as Model-Agnostic Meta-Learning (MAML) have shown strong few-shot performance in supervised and reinforcement learning settings. However, specifically in the case of meta-reinforcement learning (meta-RL), we can show that gradient-based meta-learners are sensitive to task distributions. With the wrong curriculum, agents suffer the effects of meta-overfitting, shallow adaptation, and adaptation instability. In this work, we begin by highlighting intriguing failure cases of gradient-based meta-RL and show that task distributions can wildly affect algorithmic outputs, stability, and performance. To address this problem, we leverage insights from recent literature on domain randomization and propose meta Active Domain Randomization (meta-ADR), which learns a curriculum of tasks for gradient-based meta-RL in a similar as ADR does for sim2real transfer. We show that this approach induces more stable policies on a variety of simulated locomotion and navigation tasks. We assess in- and out-of-distribution generalization and find that the learned task distributions, even in an unstructured task space, greatly improve the adaptation performance of MAML. Finally, we motivate the need for better benchmarking in meta-RL that prioritizes generalization over single-task adaption performance.},
annote = {Use previous work (active domain randomization) to pick tasks
Use MAML/PEARL/other gradient method to adapt to new task},
archivePrefix = {arXiv},
arxivId = {2002.07956},
author = {Mehta, Bhairav and Deleu, Tristan and Raparthy, Sharath Chandra and Pal, Chris J. and Paull, Liam},
booktitle = {arXiv},
eprint = {2002.07956},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehta et al. - 2020 - Curriculum in gradient-based meta-reinforcement learning.pdf:pdf},
keywords = {diversity:discriminator,env-type:parametrized,env:mujoco},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:mujoco},
title = {{Curriculum in gradient-based meta-reinforcement learning}},
year = {2020}
}
@inproceedings{Khodak2019,
abstract = {We build a theoretical framework for designing and understanding practical meta-learning methods that integrates sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their meta-test-time performance on standard problems in few-shot learning and federated learning.},
archivePrefix = {arXiv},
arxivId = {1906.02717},
author = {Khodak, Mikhail and Balcan, Maria Florina and Talwalkar, Ameet},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1906.02717},
file = {::},
issn = {10495258},
title = {{Adaptive gradient-based meta-learning methods}},
volume = {32},
year = {2019}
}
@techreport{Justesen2018,
abstract = {Deep reinforcement learning (RL) has shown impressive results in a variety of domains, learning directly from high-dimensional sensory streams. However, when neural networks are trained in a fixed environment, such as a single level in a video game, they will usually overfit and fail to generalize to new levels. When RL models overfit, even slight modifications to the environment can result in poor agent performance. This paper explores how procedurally generated levels during training can increase generality. We show that for some games procedural level generation enables generalization to new levels within the same distribution. Additionally, it is possible to achieve better performance with less data by manipulating the difficulty of the levels in response to the performance of the agent. The generality of the learned behaviors is also evaluated on a set of human-designed levels. The results suggest that the ability to generalize to human-designed levels highly depends on the design of the level generators. We apply dimensionality reduction and clustering techniques to visualize the generators' distributions of levels and analyze to what degree they can produce levels similar to those designed by a human.},
annote = {Increase difficulty each time agent achieves goal, decrease every time it doesn't (env difficulty defined by manual specification of requirements)},
archivePrefix = {arXiv},
arxivId = {1806.10729},
author = {Justesen, Niels and Torrado, Ruben Rodriguez and Bontrager, Philip and Khalifa, Ahmed and Togelius, Julian and Risi, Sebastian},
booktitle = {arXiv},
eprint = {1806.10729},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Justesen et al. - 2018 - Illuminating generalization in deep reinforcement learning through procedural level generation.pdf:pdf},
isbn = {1806.10729v5},
keywords = {env-type:parametrized,env:custom,reward weights},
mendeley-tags = {env-type:parametrized,env:custom,reward weights},
title = {{Illuminating generalization in deep reinforcement learning through procedural level generation}},
url = {https://contest.openai.com/},
year = {2018}
}
@inproceedings{Pentina2015a,
abstract = {In this work we aim at extending the theoretical foundations of lifelong learning. Previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d. from a task environment or limited to strongly constrained data distributions. Instead, we study two scenarios when lifelong learning is possible, even though the observed tasks do not form an i.i.d. sample: first, when they are sampled from the same environment, but possibly with dependencies, and second, when the task environment is allowed to change over time in a consistent way. In the first case we prove a PAC-Bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d. case. For the second scenario we propose to learn an inductive bias in form of a transfer procedure. We present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm.},
author = {Pentina, Anastasia and Lampert, Christoph H},
booktitle = {Advances in Neural Information Processing Systems},
file = {::},
issn = {10495258},
pages = {1540--1548},
title = {{Lifelong learning with non-i.i.d. tasks}},
volume = {2015-Janua},
year = {2015}
}
@techreport{Al-Shedivat2017,
abstract = {The ability to continuously learn and adapt from limited experience in nonstationary environments is an important milestone on the path towards general intelligence. In this paper, we cast the problem of continuous adaptation into the learning-to-learn framework. We develop a simple gradient-based meta-learning algorithm suitable for adaptation in dynamically changing and adversarial scenarios. Additionally, we design a new multi-agent competitive environment, RoboSumo, and define iterated adaptation games for testing various aspects of continuous adaptation. We demonstrate that meta-learning enables significantly more efficient adaptation than reactive baselines in the few-shot regime. Our experiments with a population of agents that learn and compete suggest that meta-learners are the fittest.},
annote = {MAML with changing task distribution due to self-play.
Use something like IS to handle mismatch},
archivePrefix = {arXiv},
arxivId = {1710.03641},
author = {Al-Shedivat, Maruan and Bansal, Trapit and Burda, Yura and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter},
booktitle = {arXiv},
eprint = {1710.03641},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Al-Shedivat et al. - 2017 - Continuous adaptation via meta-learning in nonstationary and competitive environments.pdf:pdf},
keywords = {adversarial,env-type:custom,env:custom},
mendeley-tags = {adversarial,env-type:custom,env:custom},
title = {{Continuous adaptation via meta-learning in nonstationary and competitive environments}},
url = {https://goo.gl/tboqaN.},
year = {2017}
}
@article{Sukhbaatar2017,
abstract = {We describe a simple scheme that allows an agent to learn about its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on two kinds of environments: (nearly) reversible environments and environments that can be reset. Alice will "propose" the task by doing a sequence of actions and then Bob must undo or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When Bob is deployed on an RL task within the environment, this unsupervised training reduces the number of supervised episodes needed to learn, and in some cases converges to a higher reward.},
archivePrefix = {arXiv},
arxivId = {1703.05407},
author = {Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
eprint = {1703.05407},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sukhbaatar et al. - 2017 - Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play.pdf:pdf},
month = {mar},
number = {i},
pages = {1--16},
title = {{Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play}},
url = {http://arxiv.org/abs/1703.05407},
year = {2017}
}
@misc{Reny2019,
abstract = {Goal-oriented reinforcement learning has recently been a practical framework for robotic manipulation tasks, in which an agent is required to reach a certain goal defined by a function on the state space. However, the sparsity of such reward definition makes traditional reinforcement learning algorithms very inefficient. Hindsight Experience Replay (HER), a recent advance, has greatly improved sample efficiency and practical applicability for such problems. It exploits previous replays by constructing imaginary goals in a simple heuristic way, acting like an implicit curriculum to alleviate the challenge of sparse reward signal. In this paper, we introduce Hindsight Goal Generation (HGG), a novel algorithmic framework that generates valuable hindsight goals which are easy for an agent to achieve in the short term and are also potential for guiding the agent to reach the actual goal in the long term. We have extensively evaluated our goal generation algorithm on a number of robotic manipulation tasks and demonstrated substantially improvement over the original HER in terms of sample efficiency.},
annote = {HER with goals close to the real goal.
Force goal diversity by ensuring that for each trajectory in the buffer, at most one state is directly connected to the start state.},
archivePrefix = {arXiv},
arxivId = {1906.04279},
author = {Reny, Zhizhou and Dongy, Kefan and Zhou, Yuan and Liu, Qiang and Peng, Jian},
booktitle = {arXiv},
eprint = {1906.04279},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren et al. - Unknown - Exploration via Hindsight Goal Generation.pdf:pdf},
keywords = {diversity:other,env-type:randomized,env:gripper,experience replay,medium difficulty},
mendeley-tags = {diversity:other,env-type:randomized,env:gripper,experience replay,medium difficulty},
title = {{Exploration via hindsight goal generation}},
year = {2019}
}
@techreport{Wang2020,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
eprint = {2010.13166},
file = {::},
keywords = {Example Reweighting,Index Terms Curriculum Learning,Machine Learning,Self-Paced Learning,Training Strategy},
title = {{A Comprehensive Survey on Curriculum Learning}},
url = {http://arxiv.org/abs/2010.13166},
year = {2020}
}
@article{Soviany2021,
abstract = {Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.},
archivePrefix = {arXiv},
arxivId = {2101.10382},
author = {Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
eprint = {2101.10382},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Soviany et al. - 2021 - Curriculum Learning A Survey.pdf:pdf},
month = {jan},
title = {{Curriculum Learning: A Survey}},
url = {http://arxiv.org/abs/2101.10382},
year = {2021}
}
@techreport{Portelas2020,
abstract = {A major challenge in the Deep RL (DRL) community is to train agents able to generalize their control policy over situations never seen in training. Training on diverse tasks has been identified as a key ingredient for good generalization, which pushed researchers towards using rich procedural task generation systems controlled through complex continuous parameter spaces. In such complex task spaces, it is essential to rely on some form of Automatic Curriculum Learning (ACL) to adapt the task sampling distribution to a given learning agent, instead of randomly sampling tasks, as many could end up being either trivial or unfeasible. Since it is hard to get prior knowledge on such task spaces, many ACL algorithms explore the task space to detect progress niches over time, a costly tabula-rasa process that needs to be performed for each new learning agents, although they might have similarities in their capabilities profiles. To address this limitation, we introduce the concept of Meta-ACL, and formalize it in the context of black-box RL learners, i.e. algorithms seeking to generalize curriculum generation to an (unknown) distribution of learners. In this work, we present AGAIN, a first instantiation of Meta-ACL, and showcase its benefits for curriculum generation over classical ACL in multiple simulated environments including procedurally generated parkour environments with learners of varying morphologies. Videos and code are available at https://sites.google.com/view/meta-acl .},
annote = {Only seems to work for multiple students.

Add “knowledge component” KC per student, the student's return on uniformly picked tasks (pre-tests)
Pre-train student, pre-test it, find k most similar students by KC distance, get the trajectory of the best one (in terms of score on KC after training), use that trajectory to pick task distribution with a bandit},
archivePrefix = {arXiv},
arxivId = {2011.08463},
author = {Portelas, R{\'{e}}my and Romac, Cl{\'{e}}ment and Hofmann, Katja and Oudeyer, Pierre-Yves},
eprint = {2011.08463},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Portelas et al. - 2020 - Meta Automatic Curriculum Learning.pdf:pdf},
issn = {23318422},
keywords = {Automatic Curriculum Learning,Deep Reinforcement Learning,env-type:parametrized,env:pybullet,reward weights},
mendeley-tags = {env-type:parametrized,env:pybullet,reward weights},
title = {{Meta Automatic Curriculum Learning}},
url = {http://arxiv.org/abs/2011.08463},
year = {2020}
}
@techreport{Deepmind2018,
abstract = {The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a return. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We discuss a gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art performance. The central goal of reinforcement learning (RL) is to optimise the agent's return (cumulative reward); this is typically achieved by a combination of prediction and control. The prediction subtask is to estimate the value function-the expected return from any given state. Ideally, this would be achieved by updating an approximate value function towards the true value function. The control subtask is to optimise the agent's policy for selecting actions, so as to maximise the value function. Ideally, the policy would simply be updated in the direction that increases the true value function. However, the true value function is unknown and therefore, for both prediction and control, a sampled return is instead used as a proxy. A large family of RL algorithms [Sutton, 1988, Rummery and Niranjan, 1994, van Seijen et al., 2009, Sutton and Barto, 2018], including several state-of-the-art deep RL algorithms [Mnih et al., 2015, van Hasselt et al., 2016, Harutyunyan et al., 2016, Hessel et al., 2018, Espeholt et al., 2018], are characterised by different choices of the return. The discount factor $\gamma$ determines the timescale of the return. A discount factor close to $\gamma$ = 1 provides a long-sighted goal that accumulates rewards far into the future, while a discount factor close to $\gamma$ = 0 provides a shortsighted goal that prioritises short-term rewards. Even in problems where long-sightedness is clearly desired, it is frequently observed that discounts $\gamma$ {\textless} 1 achieve better results [Prokhorov and Wunsch, 1997], especially during early learning. It is known that many algorithms converge faster with lower discounts [Bertsekas and Tsitsiklis, 1996], but of course too low a discount can lead to highly sub-optimal policies that are too myopic. In practice it can be better to first optimise for a myopic horizon, e.g., with $\gamma$ = 0 at first, and then to repeatedly increase the discount only after learning is somewhat successful [Prokhorov and Wunsch, 1997]. The return may also be bootstrapped at different time horizons. An n-step return accumulates rewards over n time-steps and then adds the value function at the nth time-step. The $\lambda$-return [Sutton, 1988, Sutton and Barto, 2018] is a geometrically weighted combination of n-step returns. In either case, the meta-parameter n or $\lambda$ can be important to the performance of the algorithm, trading off bias and variance. Many researchers have sought to automate the selection of these parameters [Kearns and Singh,},
archivePrefix = {arXiv},
arxivId = {1805.09801v1},
author = {Deepmind, Zhongwen Xu and Van, Hado and Deepmind, Hasselt and Deepmind, David Silver},
eprint = {1805.09801v1},
file = {::},
title = {{Meta-Gradient Reinforcement Learning}},
year = {2018}
}
@misc{Xu2020a,
abstract = {Continuously learning to solve unseen tasks with limited experience has been extensively pursued in meta-learning and continual learning, but with restricted assumptions such as accessible task distributions, independently and identically distributed tasks, and clear task delineations. However, real-world physical tasks frequently violate these assumptions, resulting in performance degradation. This paper proposes a continual online model-based reinforcement learning approach that does not require pre-training to solve task-agnostic problems with unknown task boundaries. We maintain a mixture of experts to handle nonstationarity, and represent each different type of dynamics with a Gaussian Process to efficiently leverage collected data and expressively model uncertainty. We propose a transition prior to account for the temporal dependencies in streaming data and update the mixture online via sequential variational inference. Our approach reliably handles the task distribution shift by generating new models for never-before-seen dynamics and reusing old models for previously seen dynamics. In experiments, our approach outperforms alternative methods in non-stationary tasks, including classic control with changing dynamics and decision making in different driving scenarios. Codes available at: https://github.com/mxu34/mbrl-gpmm.},
archivePrefix = {arXiv},
arxivId = {2006.11441},
author = {Xu, Mengdi and Ding, Wenhao and Zhu, Jiacheng and Liu, Zuxin and Chen, Baiming and Zhao, Ding},
booktitle = {arXiv},
eprint = {2006.11441},
file = {::},
issn = {23318422},
title = {{Task-agnostic online reinforcement learning with an infinite mixture of Gaussian processes}},
url = {https://github.com/mxu34/mbrl-gpmm.},
year = {2020}
}
@article{Curi2020,
abstract = {Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efficient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty. However, while most algorithms distinguish these two uncertainties for learning the model, they ignore it when optimizing the policy, which leads to greedy and insufficient exploration. At the same time, there are no practical solvers for optimistic exploration algorithms. In this paper, we propose a practical optimistic exploration algorithm (H-UCRL). H-UCRL reparameterizes the set of plausible models and hallucinates control directly on the epistemic uncertainty. By augmenting the input space with the hallucinated inputs, H-UCRL can be solved using standard greedy planners. Furthermore, we analyze H-UCRL and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration significantly speeds-up learning when there are penalties on actions, a setting that is notoriously difficult for existing model-based reinforcement learning algorithms.},
annote = {Motivtion: (1) Previous MBRL work in tabular settings, or are computationally intractable (TS, UCRL), or do not explore effectively. (2) MB approaches can distinguish between aleatoric and epistemic uncertainty. 

Algorithm: essentially UCRL where confidence is derived from a probabilistic DNN, and a computationally tractable algorithm is suggested for solving the expanded MDP in UCRL.  

Method: reduce exploration to greedy exploitation over a re-parametrized cost. 

Novelty: first optimistic exploration algorithm for DNNs.  

Theory: regret bounds provided.},
archivePrefix = {arXiv},
arxivId = {2006.08684},
author = {Curi, Sebastian and Berkenkamp, Felix and Krause, Andreas},
eprint = {2006.08684},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Curi, Berkenkamp, Krause - Unknown - Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning.pdf:pdf},
issn = {23318422},
journal = {arXiv},
month = {jun},
title = {{Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning}},
url = {http://arxiv.org/abs/2006.08684},
year = {2020}
}
@article{Milano2021,
abstract = {We demonstrate how an evolutionary algorithm can be extended with a curriculum learning process that selects automatically the environmental conditions in which the evolving agents are evaluated. The environmental conditions are selected so to adjust the level of difficulty to the ability level of the current evolving agents and so to challenge the weaknesses of the evolving agents. The method does not require domain knowledge and does not introduce additional hyperparameters. The results collected on two benchmark problems, that require to solve a task in significantly varying environmental conditions, demonstrate that the method proposed outperforms conventional algorithms and generates solutions that are robust to variations},
archivePrefix = {arXiv},
arxivId = {2102.08849},
author = {Milano, Nicola and Nolfi, Stefano},
eprint = {2102.08849},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Milano, Nolfi - 2021 - Automated Curriculum Learning for Embodied Agents A Neuroevolutionary Approach.pdf:pdf},
month = {feb},
title = {{Automated Curriculum Learning for Embodied Agents: A Neuroevolutionary Approach}},
url = {http://arxiv.org/abs/2102.08849},
year = {2021}
}
@techreport{Srinivasan2019,
abstract = {In this paper, we present a technique that improves the process of training an agent (using RL) for instruction following. We develop a training curriculum that uses a nominal number of expert demonstrations and trains the agent in a manner that draws parallels from one of the ways in which humans learn to perform complex tasks, i.e by starting from the goal and working backwards. We test our method on the BabyAI platform and show an improvement in sample efficiency for some of its tasks compared to a PPO (proximal policy optimization) baseline.},
annote = {2019 paper by Bengio's group
Similar approach to reverse curriculum generation but for discrete state/action
Use expert demonstrations to find what close states are (since random walk can get us back to the goal)},
archivePrefix = {arXiv},
arxivId = {1912.00444},
author = {Srinivasan, Anirudh and Chevalier-Boisvert, Maxime and Bahdanau, Dzmitry and Bengio, Yoshua},
booktitle = {arXiv},
eprint = {1912.00444},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srinivasan et al. - 2019 - Automated curriculum generation for policy gradients from demonstrations.pdf:pdf},
keywords = {alg:ppo,env-type:custom,env:toy,fixed curriculum,medium difficulty},
mendeley-tags = {alg:ppo,env-type:custom,env:toy,fixed curriculum,medium difficulty},
title = {{Automated curriculum generation for policy gradients from demonstrations}},
year = {2019}
}
@article{Lakshminarayanan2016,
abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
annote = {Paper using adversarial ensambles to approximate bayesian NN for ML},
archivePrefix = {arXiv},
arxivId = {1612.01474},
author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
eprint = {1612.01474},
file = {::},
month = {dec},
title = {{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}},
url = {http://arxiv.org/abs/1612.01474},
year = {2016}
}
@article{Wang2020a,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
eprint = {2010.13166},
file = {::},
title = {{A Comprehensive Survey on Curriculum Learning}},
url = {http://arxiv.org/abs/2010.13166},
year = {2020}
}
@techreport{Pan2019,
abstract = {Deep reinforcement learning has recently made significant progress in solving computer games and robotic control tasks. A known problem, though, is that policies overfit to the training environment and may not avoid rare, catastrophic events such as automotive accidents. A classical technique for improving the robustness of reinforcement learning algorithms is to train on a set of randomized environments, but this approach only guards against common situations. Recently, robust adversarial reinforcement learning (RARL) was developed, which allows efficient applications of random and systematic perturbations by a trained adversary. A limitation of RARL is that only the expected control objective is optimized; there is no explicit modeling or optimization of risk. Thus the agents do not consider the probability of catastrophic events (i.e., those inducing abnormally large negative reward), except through their effect on the expected objective. In this paper we introduce risk-averse robust adversarial reinforcement learning (RARARL), using a risk-averse protagonist and a risk-seeking adversary. We test our approach on a self-driving vehicle controller. We use an ensemble of policy networks to model risk as the variance of value functions. We show through experiments that a risk-averse agent is better equipped to handle a risk-seeking adversary, and experiences substantially fewer crashes compared to agents trained without an adversary. Supplementary materials are available at https://sites.google.com/view/rararl.},
archivePrefix = {arXiv},
arxivId = {1904.00511},
author = {Pan, Xinlei and Seita, Daniel and Gao, Yang and Canny, John},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2019.8794293},
eprint = {1904.00511},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan et al. - 2019 - Risk averse robust adversarial reinforcement learning.pdf:pdf},
isbn = {9781538660263},
issn = {10504729},
keywords = {adversarial},
mendeley-tags = {adversarial},
pages = {8522--8528},
title = {{Risk averse robust adversarial reinforcement learning}},
volume = {2019-May},
year = {2019}
}
@article{Nichol2018,
abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
archivePrefix = {arXiv},
arxivId = {1803.02999},
author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
eprint = {1803.02999},
file = {::},
title = {{On First-Order Meta-Learning Algorithms}},
url = {http://arxiv.org/abs/1803.02999},
year = {2018}
}
@article{Rothfuss2020,
abstract = {Meta-learning can successfully acquire useful inductive biases from data. Yet, its generalization properties to unseen learning tasks are poorly understood. Particularly if the number of meta-training tasks is small, this raises concerns about overfitting. We provide a theoretical analysis using the PAC-Bayesian framework and derive novel generalization bounds for meta-learning. Using these bounds, we develop a class of PAC-optimal meta-learning algorithms with performance guarantees and a principled meta-level regularization. Unlike previous PAC-Bayesian meta-learners, our method results in a standard stochastic optimization problem which can be solved efficiently and scales well. When instantiating our PAC-optimal hyper-posterior (PACOH) with Gaussian processes and Bayesian Neural Networks as base learners, the resulting methods yield state-of-the-art performance, both in terms of predictive accuracy and the quality of uncertainty estimates. Thanks to their principled treatment of uncertainty, our meta-learners can also be successfully employed for sequential decision problems.},
archivePrefix = {arXiv},
arxivId = {2002.05551},
author = {Rothfuss, Jonas and Fortuin, Vincent and Josifoski, Martin and Krause, Andreas},
eprint = {2002.05551},
file = {::},
issn = {2331-8422},
month = {feb},
title = {{PACOH: Bayes-Optimal Meta-Learning with PAC-Guarantees}},
url = {https://arxiv.org/abs/2002.05551v5 http://arxiv.org/abs/2002.05551},
year = {2020}
}
@misc{Cobbe2018,
abstract = {In this paper, we investigate the problem of overfitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
author = {Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
booktitle = {arXiv},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cobbe et al. - 2018 - Quantifying Generalization in Reinforcement Learning.pdf:pdf},
issn = {2640-3498},
keywords = {alg:impala,env:procgen,generalization},
mendeley-tags = {alg:impala,env:procgen,generalization},
month = {may},
pages = {1282--1289},
publisher = {PMLR},
title = {{Quantifying Generalization in Reinforcement Learning}},
url = {http://proceedings.mlr.press/v97/cobbe19a.html},
year = {2018}
}
@inproceedings{Narvekar2017,
abstract = {Transfer learning in reinforcement learning is an area of research that seeks to speed up or improve learning of a complex target task, by leveraging knowledge from one or more source tasks. This thesis will extend the concept of transfer learning to curriculum learning, where the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We discuss completed work on this topic, including methods for semi-automatically generating source tasks tailored to an agent and the characteristics of a target domain, and automatically sequencing such tasks into a curriculum. Finally, we also present ideas for future work.},
author = {Narvekar, Sanmit},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/757},
file = {::},
isbn = {9780999241103},
issn = {10450823},
pages = {5195--5196},
title = {{Curriculum learning in reinforcement learning}},
year = {2017}
}
@techreport{Racaniere2019,
abstract = {Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environments these correlations are often too small, or rewarding events are too infrequent to make learning feasible. Human education instead relies on curricula-the breakdown of tasks into simpler, static challenges with dense rewards-to build up to complex behaviors. While curricula are also useful for artificial agents, handcrafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct useful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a single goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.},
annote = {ICLR 2020, deepmind
Same env with different params, env defined by free parameters (where to place things) - pretty big variations
Teacher-student (called here setter-solver) system, need to define/learn goal validity (can an expert solver solve this task), goal feasibility (can the current student solve this task), goal coverage (exploration of different goals)
Teacher (setter) is a generator that gets desired feasibility (difficulty), and a “judge” is discriminator for feasibility (supervised prediction of env solvable probability) -{\textgreater} GAN
Student (solver) gets sparse reward - 1 if got to goal, 0 if failed (enough time passed)
Goal Validity measured with loss function for probability of student getting to a goal close to specified goal (state and action space assumed continuous)
Goal coverage measured with loss by entropy (info gain) of new task for generator (so if generated task is similar to others in same difficulty, loss is high)
Total teacher (setter) loss is combination of all three losses (with equal weight)
Add additional discrimantion loss for student to discriminate teacher-generated goals from real goals (to avoid problems with easy goals, needed because env changes are big)
Ablation shows validity and coverage are most important},
archivePrefix = {arXiv},
arxivId = {1909.12892},
author = {Racani{\`{e}}re, S{\'{e}}bastien and Lampinen, Andrew K. and Santoro, Adam and Reichert, David P. and Firoiu, Vlad and Lillicrap, Timothy P.},
booktitle = {arXiv},
eprint = {1909.12892},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Racani{\`{e}}re et al. - 2019 - Automated curricula through setter-solver interactions.pdf:pdf},
keywords = {diversity:entropy,env-type:parametrized,env:custom,goal reachability},
mendeley-tags = {diversity:entropy,env-type:parametrized,env:custom,goal reachability},
title = {{Automated curricula through setter-solver interactions}},
year = {2019}
}
@techreport{Narvekar2020,
abstract = {Curriculum learning for reinforcement learning (RL) is an active area of research that seeks to speed up training of RL agents on a target task by first training them through a series of progressively more challenging source tasks. Each task in this sequence builds upon skills learned in previous tasks to gradually develop the repertoire needed to solve the final task. Over the past few years, many automated methods to develop cur-ricula have been developed. However, they all have one key limitation: the curriculum must be regenerated from scratch for each new agent or task encountered. In many cases, this generation process can be very expensive. However, there is structure that can be exploited between tasks and agents, such that knowledge gained developing a curriculum for one task can be reused to speed up creating a curriculum for a new task. In this paper, we present a method to generalize a curriculum learned for one set of tasks to a novel set of unseen tasks.},
annote = {Try to use a learned curriculum for one task to build a curriculum for another task (curriculum policy)
Grid worlds with manual task set designed such that solving each task gives a skill for the target task
Show that solving sub-tasks creates learning (even if the exact configuration of the test is not in the train, but all the needed skills are)},
author = {Narvekar, Sanmit and Stone, Peter},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar, Stone - 2020 - Generalizing Curricula for Reinforcement Learning.pdf:pdf},
keywords = {alg:dqn,custom mdp,env-type:set,env:gridworld},
mendeley-tags = {alg:dqn,custom mdp,env-type:set,env:gridworld},
title = {{Generalizing Curricula for Reinforcement Learning}},
year = {2020}
}
@techreport{Li2019,
abstract = {We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple datasets collected from different tasks, we train a multi-task policy to perform well in unseen tasks sampled from the same distribution. The task identities of the unseen tasks are not provided. To perform well, the policy must infer the task identity from collected transitions by modelling its dependency on states, actions and rewards. Because the different datasets may have state-action distributions with large divergence, the task inference module can learn to ignore the rewards and spuriously correlate {\$}\backslashtextit{\{}only{\}}{\$} state-action pairs to the task identity, leading to poor test time performance. To robustify task inference, we propose a novel application of the triplet loss. To mine hard negative examples, we relabel the transitions from the training tasks by approximating their reward functions. When we allow further training on the unseen tasks, using the trained policy as an initialization leads to significantly faster convergence compared to randomly initialized policies (up to {\$}80\backslash{\%}{\$} improvement and across 5 different Mujoco task distributions). We name our method {\$}\backslashtextbf{\{}MBML{\}}{\$} ({\$}\backslashtextbf{\{}M{\}}\backslashtext{\{}ulti-task{\}}{\$} {\$}\backslashtextbf{\{}B{\}}\backslashtext{\{}atch{\}}{\$} RL with {\$}\backslashtextbf{\{}M{\}}\backslashtext{\{}etric{\}}{\$} {\$}\backslashtextbf{\{}L{\}}\backslashtext{\{}earning{\}}{\$}).},
archivePrefix = {arXiv},
arxivId = {1909.11373},
author = {Li, Jiachen and Vuong, Quan and Liu, Shuang and Liu, Minghua and Ciosek, Kamil and Ross, Keith and Christensen, Henrik Iskov and Su, Hao},
eprint = {1909.11373},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2019 - Multi-task Batch Reinforcement Learning with Metric Learning.pdf:pdf},
keywords = {alg:mbml,alg:pearl,env:d4rl,env:mujoco,task policy combination},
mendeley-tags = {alg:mbml,alg:pearl,env:d4rl,env:mujoco,task policy combination},
title = {{Multi-task Batch Reinforcement Learning with Metric Learning}},
url = {http://arxiv.org/abs/1909.11373},
year = {2019}
}
@misc{Portelas2019,
abstract = {We consider the problem of how a teacher algorithm can enable an unknown Deep Reinforcement Learning (DRL) student to become good at a skill over a wide range of diverse environments. To do so, we study how a teacher algorithm can learn to generate a learning curriculum, whereby it sequentially samples parameters controlling a stochastic procedural generation of environments. Because it does not initially know the capacities of its student, a key challenge for the teacher is to discover which environments are easy, difficult or unlearnable, and in what order to propose them to maximize the efficiency of learning over the learnable ones. To achieve this, this problem is transformed into a surrogate continuous bandit problem where the teacher samples environments in order to maximize absolute learning progress of its student. We present a new algorithm modeling absolute learning progress with Gaussian mixture models (ALP-GMM). We also adapt existing algorithms and provide a complete study in the context of DRL. Using parameterized variants of the BipedalWalker environment, we study their efficiency to personalize a learning curriculum for different learners (embodiments), their robustness to the ratio of learnable/unlearnable environments, and their scalability to non-linear and high-dimensional parameter spaces. Videos and code are available at https://github.com/flowersteam/teachDeepRL.},
annote = {ALP-GMM},
archivePrefix = {arXiv},
arxivId = {1910.07224},
author = {Portelas, R{\'{e}}my and Colas, C{\'{e}}dric and Hofmann, Katja and Oudeyer, Pierre Yves},
booktitle = {arXiv},
eprint = {1910.07224},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Portelas et al. - 2019 - Teacher algorithms for curriculum learning of Deep RL in continuously parameterized environments(2).pdf:pdf},
keywords = {Curiosity,Curriculum Learning,Deep Reinforcement Learning,Learning Progress,Parameterized Procedural Environments,Teacher-Student Learning,env-type:parametrized,env:pybullet},
mendeley-tags = {env-type:parametrized,env:pybullet},
month = {oct},
publisher = {arXiv},
title = {{Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments}},
url = {http://arxiv.org/abs/1910.07224},
year = {2019}
}
@inproceedings{Khodak2019b,
abstract = {We build a theoretical framework for designing and understanding practical meta-learning methods that integrates sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their meta-test-time performance on standard problems in few-shot learning and federated learning.},
archivePrefix = {arXiv},
arxivId = {1906.02717},
author = {Khodak, Mikhail and Balcan, Maria Florina and Talwalkar, Ameet},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1906.02717},
file = {::},
issn = {10495258},
title = {{Adaptive gradient-based meta-learning methods}},
volume = {32},
year = {2019}
}
@article{Jain2017,
abstract = {In this work, we propose several online methods to build a $\backslash$emph{\{}learning curriculum{\}} from a given set of target-task-specific training tasks in order to speed up reinforcement learning (RL). These methods can decrease the total training time needed by an RL agent compared to training on the target task from scratch. Unlike traditional transfer learning, we consider creating a sequence from several training tasks in order to provide the most benefit in terms of reducing the total time to train. Our methods utilize the learning trajectory of the agent on the curriculum tasks seen so far to decide which tasks to train on next. An attractive feature of our methods is that they are weakly coupled to the choice of the RL algorithm as well as the transfer learning method. Further, when there is domain information available, our methods can incorporate such knowledge to further speed up the learning. We experimentally show that these methods can be used to obtain suitable learning curricula that speed up the overall training time on two different domains.},
annote = {Pick next task according to metric (run policy and pick task with highest return)
If task has pre-defined features, can also learn this (regression).
Very basic, toy tasks, weak baseline},
archivePrefix = {arXiv},
arxivId = {1703.07853},
author = {Jain, Vikas and Tulabandhula, Theja},
eprint = {1703.07853},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jain, Tulabandhula - 2017 - Faster Reinforcement Learning Using Active Simulators.pdf:pdf},
journal = {arXiv},
keywords = {env-type:set,env:gridworld,reward weights},
mendeley-tags = {env-type:set,env:gridworld,reward weights},
month = {mar},
title = {{Faster Reinforcement Learning Using Active Simulators}},
url = {http://arxiv.org/abs/1703.07853},
year = {2017}
}
@misc{Klink2020,
abstract = {Generalization and reuse of agent behaviour across a variety of learning tasks promises to carry the next wave of breakthroughs in Reinforcement Learning (RL). The field of Curriculum Learning proposes strategies that aim to support a learn-ing agent by exposing it to a tailored series of tasks throughout learning, e.g. by progressively increasing their complexity. In this paper, we con-sider recently established results in Curriculum Learning for episodic RL, proposing an extension that is easily integrated with well-known RL al-gorithms and providing a theoretical formulation from an RL-as-Inference perspective. We evalu-ate the proposed scheme with different Deep RL algorithms on representative tasks, demonstrat-ing that it is capable of significantly improving learning performance.},
annote = {Use KL divergence between steps and known test distribution to pick tasks that have good start conditions (high V) and are "close" to test distribution.
Curriculum induced by restricting change of task distributions as well as the task distribution itself.},
archivePrefix = {arXiv},
arxivId = {2004.11812},
author = {Klink, Pascal and D'Eramo, Carlo and Peters, Jan and Pajarinen, Joni},
booktitle = {arXiv},
eprint = {2004.11812},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klink et al. - Unknown - Self-Paced Deep Reinforcement Learning.pdf:pdf},
keywords = {env-type:parametrized,env:gripper,env:mujoco,reward weights},
mendeley-tags = {env-type:parametrized,env:gripper,env:mujoco,reward weights},
title = {{Self-paced deep reinforcement learning}},
year = {2020}
}
@inproceedings{Pentina2015,
abstract = {Sharing information between multiple tasks enables algorithms to achieve good generalization performance even from small amounts of training data. However, in a realistic scenario of multi-task learning not all tasks are equally related to each other, hence it could be advantageous to transfer information only between the most related tasks. In this work we propose an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly. Subsequently, we address the question of curriculum learning of tasks, i.e. finding the best order of tasks to be learned. Our approach is based on a generalization bound criterion for choosing the task order that optimizes the average expected classification performance over all tasks. Our experimental results show that learning multiple related tasks sequentially can be more effective than learning them jointly, the order in which tasks are being solved affects the overall performance, and that our model is able to automatically discover a favourable order of tasks.},
archivePrefix = {arXiv},
arxivId = {1412.1353},
author = {Pentina, Anastasia and Sharmanska, Viktoriia and Lampert, Christoph H},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7299188},
eprint = {1412.1353},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pentina, Sharmanska, Lampert - 2015 - Curriculum learning of multiple tasks.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
pages = {5492--5500},
title = {{Curriculum learning of multiple tasks}},
volume = {07-12-June},
year = {2015}
}
@article{Catoni2007,
abstract = {This monograph deals with adaptive supervised classification, using tools
borrowed from statistical mechanics and information theory, stemming from the
PACBayesian approach pioneered by David McAllester and applied to a conception
of statistical learning theory forged by Vladimir Vapnik. Using convex analysis
on the set of posterior probability measures, we show how to get local measures
of the complexity of the classification model involving the relative entropy of
posterior distributions with respect to Gibbs posterior measures. We then
discuss relative bounds, comparing the generalization error of two
classification rules, showing how the margin assumption of Mammen and Tsybakov
can be replaced with some empirical measure of the covariance structure of the
classification model.We show how to associate to any posterior distribution an
effective temperature relating it to the Gibbs prior distribution with the same
level of expected error rate, and how to estimate this effective temperature
from data, resulting in an estimator whose expected error rate converges
according to the best possible power of the sample size adaptively under any
margin and parametric complexity assumptions. We describe and study an
alternative selection scheme based on relative bounds between estimators, and
present a two step localization technique which can handle the selection of a
parametric model from a family of those. We show how to extend systematically
all the results obtained in the inductive setting to transductive learning, and
use this to improve Vapnik's generalization bounds, extending them to the case
when the sample is made of independent non-identically distributed pairs of
patterns and labels. Finally we review briefly the construction of Support
Vector Machines and show how to derive generalization bounds for them,
measuring the complexity either through the number of support vectors or
through the value of the transductive or inductive margin.},
archivePrefix = {arXiv},
arxivId = {0712.0248},
author = {Catoni, Olivier},
doi = {10.1214/074921707000000391},
eprint = {0712.0248},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Catoni - 2007 - Pac-Bayesian Supervised Classification The Thermodynamics of Statistical Learning.pdf:pdf},
journal = {IMS Lecture Notes Monograph Series},
month = {dec},
number = {13},
pages = {1--163},
publisher = {Institute of Mathematical Statistics},
title = {{Pac-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning}},
url = {https://arxiv.org/abs/0712.0248v1},
volume = {56},
year = {2007}
}
@inproceedings{Rakelly2019,
abstract = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100x as well as in asymptotic performance on several meta-RL benchmarks.},
archivePrefix = {arXiv},
arxivId = {1903.08254},
author = {Rakelly, Kate and Zhou, Aurick and Quiilen, Deirdre and Finn, Chelsea and Levine, Sergey},
booktitle = {36th International Conference on Machine Learning, ICML 2019},
eprint = {1903.08254},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rakelly et al. - 2019 - Efficient off-policy meta-reinforcement learning via probabilistic context variables(2).pdf:pdf},
isbn = {9781510886988},
keywords = {alg:pearl,env:mujoco},
mendeley-tags = {alg:pearl,env:mujoco},
month = {mar},
pages = {9291--9301},
publisher = {International Machine Learning Society (IMLS)},
title = {{Efficient off-policy meta-reinforcement learning via probabilistic context variables}},
url = {http://arxiv.org/abs/1903.08254},
volume = {2019-June},
year = {2019}
}
@techreport{Fang2020,
abstract = {We introduce Adaptive Procedural Task Generation (APT-Gen), an approach for progressively generating a sequence of tasks as curricula to facilitate reinforcement learning in hard-exploration problems. At the heart of our approach, a task generator learns to create tasks via a black-box procedural generation module by adaptively sampling from the parameterized task space. To enable curriculum learning in the absence of a direct indicator of learning progress, the task generator is trained by balancing the agent's expected return in the generated tasks and their similarities to the target task. Through adversarial training, the similarity between the generated tasks and the target task is adaptively estimated by a task discriminator defined on the agent's behaviors. In this way, our approach can efficiently generate tasks of rich variations for target tasks of unknown parameterization or not covered by the predefined task space. Experiments demonstrate the effectiveness of our approach through quantitative and qualitative analysis in various scenarios.},
archivePrefix = {arXiv},
arxivId = {2007.00350},
author = {Fang, Kuan and Zhu, Yuke and Savarese, Silvio and Fei-Fei, Li},
booktitle = {arXiv},
eprint = {2007.00350},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang et al. - 2020 - Adaptive Procedural Task Generation for Hard-Exploration Problems.pdf:pdf},
keywords = {diversity:discriminator,env-type:parametrized,env:gridworld},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:gridworld},
title = {{Adaptive Procedural Task Generation for Hard-Exploration Problems}},
year = {2020}
}
@article{Wang2020b,
abstract = {Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in tool, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing, etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of Difficulty Measurer + Training Scheduler and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. Finally, we present brief discussions on the relationships between CL and other methods, and point out potential future research directions deserving further investigations.},
archivePrefix = {arXiv},
arxivId = {2010.13166},
author = {Wang, Xin and Chen, Yudong and Zhu, Wenwu},
eprint = {2010.13166},
file = {::},
keywords = {Example Reweighting,Index Terms Curriculum Learning,Machine Learning,Self-Paced Learning,Training Strategy},
title = {{A Comprehensive Survey on Curriculum Learning}},
url = {http://arxiv.org/abs/2010.13166},
year = {2020}
}
@inproceedings{Narvekar2016,
abstract = {Transfer learning in reinforcement learning has been an active area of research over the past decade. In transfer learning, training on a source task is leveraged to speed up or otherwise improve learning on a target task. This paper presents the more ambitious problem of curriculum learning in reinforcement learning, in which the goal is to design a sequence of source tasks for an agent to train on, such that final performance or learning speed is improved. We take the position that each stage of such a curriculum should be tailored to the current ability of the agent in order to promote learning new behaviors. Thus, as a first step towards creating a curriculum, the trainer must be able to create novel, agent-specific source tasks. We explore how such a space of useful tasks can be created using a parameterized model of the domain and observed trajectories on the target task. We experimentally show that these methods can be used to form components of a curriculum and that such a curriculum can be used successfully for transfer learning in 2 challenging multiagent reinforcement learning domains.},
annote = {Pick tasks according to agent skill.
Apply task/action simplification (limit env param space or remove actions that are specified as mistakes, also pick start states where trajectories give good return.
Approaches that require env-specific tailoring},
author = {Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narvekar et al. - Unknown - Source Task Creation for Curriculum Learning.pdf:pdf},
isbn = {9781450342391},
issn = {15582914},
keywords = {Curriculum learning,Reinforcement learning,Transfer learning,env-type:parametrized,env:custom},
mendeley-tags = {env-type:parametrized,env:custom},
pages = {566--574},
title = {{Source task creation for curriculum learning}},
url = {www.ifaamas.org},
year = {2016}
}
@techreport{Feng2020,
abstract = {Despite significant progress in general AI planning, certain domains remain out of reach of current AI planning systems. Sokoban is a PSPACE-complete planning task and represents one of the hardest domains for current AI planners. Even domain-specific specialized search methods fail quickly due to the exponential search complexity on hard instances. Our approach based on deep reinforcement learning augmented with a curriculum-driven method is the first one to solve hard instances within one day of training while other modern solvers cannot solve these instances within any reasonable time limit. In contrast to prior efforts, which use carefully handcrafted pruning techniques, our approach automatically uncovers domain structure. Our results reveal that deep RL provides a promising framework for solving previously unsolved AI planning problems, provided a proper training curriculum can be devised.},
annote = {Curriculum is for each new task (=board)
A fixed stopping point to ensure it terminates
Curriculum is domain-specific (number of boxes) and increased over time (difficulty increase)
Use MCTS to solve tasks, and do alphaZero style update},
archivePrefix = {arXiv},
arxivId = {2006.02689},
author = {Feng, Dieqiao and Gomes, Carla P. and Selman, Bart},
booktitle = {arXiv},
doi = {10.24963/ijcai.2020/304},
eprint = {2006.02689},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng, Gomes, Selman - 2020 - Solving Hard AI Planning Instances Using Curriculum-Driven Deep Reinforcement Learning.pdf:pdf},
isbn = {9780999241165},
issn = {10450823},
keywords = {env-type:parametrized,env:custom,fixed curriculum},
mendeley-tags = {env-type:parametrized,env:custom,fixed curriculum},
title = {{Solving Hard AI Planning Instances Using Curriculum-Driven Deep Reinforcement Learning}},
year = {2020}
}
@techreport{Wang2019,
abstract = {While the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. We test POET in a 2-D bipedal-walking obstacle-course domain in which POET can modify the types of challenges and their difficulty. At the same time, a neural network controlling a biped walker is optimized for each environment. The results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions.},
archivePrefix = {arXiv},
arxivId = {1901.01753},
author = {Wang, Rui and Lehman, Joel and Clune, Jeff and Stanley, Kenneth O.},
booktitle = {arXiv},
eprint = {1901.01753},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2019 - Paired Open-Ended Trailblazer (POET) Endlessly generating increasingly complex and diverse learning environments an.pdf:pdf},
keywords = {alg:ES,env-type:parametrized,env:pybullet},
mendeley-tags = {alg:ES,env-type:parametrized,env:pybullet},
title = {{Paired Open-Ended Trailblazer (POET): Endlessly generating increasingly complex and diverse learning environments and their solutions}},
year = {2019}
}
@techreport{Veeriah,
abstract = {Temporal abstractions in the form of options have been shown to help reinforcement learning (RL) agents learn faster. However, despite prior work on this topic, the problem of discovering options through interaction with an environment remains a challenge. In this paper, we introduce a novel meta-gradient approach for discovering useful options in multi-task RL environments. Our approach is based on a manager-worker decomposition of the RL agent, in which a manager max-imises rewards from the environment by learning a task-dependent policy over both a set of task-independent discovered-options and primitive actions. The option-reward and termination functions that define a subgoal for each option are parameterised as neural networks and trained via meta-gradients to maximise their usefulness. Empirical analysis on gridworld and DeepMind Lab tasks show that: (1) our approach can discover meaningful and diverse temporally-extended options in multi-task RL domains, (2) the discovered options are frequently used by the agent while learning to solve the training tasks, and (3) that the discovered options help a randomly initialised manager learn faster in completely new tasks.},
archivePrefix = {arXiv},
arxivId = {2102.06741v1},
author = {Veeriah, Vivek and Zahavy, Tom and Hessel, Matteo and Xu, Zhongwen and Oh, Junhyuk and Kemaev, Iurii and {Van Hasselt}, Hado and Silver, David and Singh, Satinder},
eprint = {2102.06741v1},
file = {::},
title = {{Discovery of Options via Meta-Learned Subgoals}}
}
@article{Khodak2019a,
abstract = {We build a theoretical framework for designing and understanding practical meta-learning methods that integrates sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their meta-test-time performance on standard problems in few-shot learning and federated learning.},
archivePrefix = {arXiv},
arxivId = {1906.02717},
author = {Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet},
eprint = {1906.02717},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
title = {{Adaptive Gradient-Based Meta-Learning Methods}},
url = {http://arxiv.org/abs/1906.02717},
volume = {32},
year = {2019}
}
@article{Zhang2020a,
abstract = {This paper gives a review of concentration inequalities which are widely
employed in non-asymptotical analyses of mathematical statistics in a wide
range of settings, from distribution-free to distribution-dependent, from
sub-Gaussian to sub-exponential, sub-Gamma, and sub-Weibull random variables,
and from the mean to the maximum concentration. This review provides results in
these settings with some fresh new results. Given the increasing popularity of
high-dimensional data and inference, results in the context of high-dimensional
linear and Poisson regressions are also provided. We aim to illustrate the
concentration inequalities with known constants and to improve existing bounds
with sharper constants.},
archivePrefix = {arXiv},
arxivId = {2011.02258},
author = {Zhang, Huiming and Chen, Song Xi},
doi = {10.4208/cmr.2020-0041},
eprint = {2011.02258},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Chen - 2020 - Concentration Inequalities for Statistical Inference.pdf:pdf},
journal = {Communications in Mathematical Research},
keywords = {60G50,62E17,constants-specified inequalities,finite-sample theory,heavy-tailed distri-butions,high-dimensional estimation and testing,random matrices AMS Subject Classifications: 60F10,sub-Weibull random variables},
month = {nov},
number = {1},
pages = {1--85},
publisher = {Global Science Press},
title = {{Concentration Inequalities for Statistical Inference}},
url = {https://arxiv.org/abs/2011.02258v3},
volume = {37},
year = {2020}
}
@misc{Laskin2020,
abstract = {Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
annote = {Generalization by adding random noise to observations/state},
archivePrefix = {arXiv},
arxivId = {2004.14990},
author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
booktitle = {arXiv},
eprint = {2004.14990},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laskin et al. - 2020 - Reinforcement learning with augmented data.pdf:pdf},
keywords = {env:procgen,generalization},
mendeley-tags = {env:procgen,generalization},
title = {{Reinforcement learning with augmented data}},
url = {https://www.github.com/MishaLaskin/rad.},
year = {2020}
}
@article{Gong2016,
abstract = {Semi-supervised image classification aims to classify a large quantity of unlabeled images by typically harnessing scarce labeled images. Existing semi-supervised methods often suffer from inadequate classification accuracy when encountering difficult yet critical images, such as outliers, because they treat all unlabeled images equally and conduct classifications in an imperfectly ordered sequence. In this paper, we employ the curriculum learning methodology by investigating the difficulty of classifying every unlabeled image. The reliability and the discriminability of these unlabeled images are particularly investigated for evaluating their difficulty. As a result, an optimized image sequence is generated during the iterative propagations, and the unlabeled images are logically classified from simple to difficult. Furthermore, since images are usually characterized by multiple visual feature descriptors, we associate each kind of features with a teacher, and design a multi-modal curriculum learning (MMCL) strategy to integrate the information from different feature modalities. In each propagation, each teacher analyzes the difficulties of the currently unlabeled images from its own modality viewpoint. A consensus is subsequently reached among all the teachers, determining the currently simplest images (i.e., a curriculum), which are to be reliably classified by the multi-modal learner. This well-organized propagation process leveraging multiple teachers and one learner enables our MMCL to outperform five state-of-the-art methods on eight popular image data sets.},
annote = {Not very relevant, previous paper that gets expanded},
author = {Gong, Chen and Tao, Dacheng and Maybank, Stephen J. and Liu, Wei and Kang, Guoliang and Yang, Jie},
doi = {10.1109/TIP.2016.2563981},
file = {:C$\backslash$:/Users/liori/Downloads/07465792.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Curriculum learning,Image classification,Multi-modal,Semi-supervised learning},
month = {jul},
number = {7},
pages = {3249--3260},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Multi-Modal Curriculum Learning for Semi-Supervised Image Classification}},
volume = {25},
year = {2016}
}
@article{Anonymous2021,
abstract = {Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the $\backslash$emph{\{}implicit curricula{\}} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of $\backslash$emph{\{}explicit curricula{\}}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.},
archivePrefix = {arXiv},
arxivId = {2012.03107},
author = {Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
eprint = {2012.03107},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Dyer, Neyshabur - 2020 - When Do Curricula Work(2).pdf:pdf},
journal = {ICLR submission},
month = {dec},
number = {April},
pages = {80--89},
title = {{When Do Curricula Work?}},
url = {http://arxiv.org/abs/2012.03107},
year = {2020}
}
@article{OpenAI2021,
abstract = {We train a single, goal-conditioned policy that can solve many robotic manipulation tasks, including tasks with previously unseen goals and objects. We rely on asymmetric self-play for goal discovery, where two agents, Alice and Bob, play a game. Alice is asked to propose challenging goals and Bob aims to solve them. We show that this method can discover highly diverse and complex goals without any human priors. Bob can be trained with only sparse rewards, because the interaction between Alice and Bob results in a natural curriculum and Bob can learn from Alice's trajectory when relabeled as a goal-conditioned demonstration. Finally, our method scales, resulting in a single policy that can generalize to many unseen tasks such as setting a table, stacking blocks, and solving simple puzzles. Videos of a learned policy is available at https://robotics-self-play.github.io.},
archivePrefix = {arXiv},
arxivId = {2101.04882},
author = {OpenAI, OpenAI and Plappert, Matthias and Sampedro, Raul and Xu, Tao and Akkaya, Ilge and Kosaraju, Vineet and Welinder, Peter and D'Sa, Ruben and Petron, Arthur and Pinto, Henrique Ponde de Oliveira and Paino, Alex and Noh, Hyeonwoo and Weng, Lilian and Yuan, Qiming and Chu, Casey and Zaremba, Wojciech},
eprint = {2101.04882},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/OpenAI et al. - 2021 - Asymmetric self-play for automatic goal discovery in robotic manipulation.pdf:pdf},
month = {jan},
number = {c},
pages = {1--12},
title = {{Asymmetric self-play for automatic goal discovery in robotic manipulation}},
url = {http://arxiv.org/abs/2101.04882},
year = {2021}
}
@techreport{Kaddour2020,
abstract = {Data-efficient learning algorithms are essential in many practical applications where data collection is expensive, e.g., in robotics due to the wear and tear. To address this problem, meta-learning algorithms use prior experience about tasks to learn new, related tasks efficiently. Typically, a set of training tasks is assumed given or randomly chosen. However, this setting does not take into account the sequential nature that naturally arises when training a model from scratch in real-life: how do we collect a set of training tasks in a data-efficient manner? In this work, we introduce task selection based on prior experience into a meta-learning algorithm by conceptualizing the learner and the active meta-learning setting using a probabilistic latent variable model. We provide empirical evidence that our approach improves data-efficiency when compared to strong baselines on simulated robotic experiments.},
annote = {October 2020 paper, university college London
Learn task dataset embedding(trajectories to latent)
Pick new task from embedded space that maximizes “utility”
Task “utility” is defined by an information metric, like posterior task probability for all training tasks (so a good task distribution is one that is very different from the training)},
archivePrefix = {arXiv},
arxivId = {2007.08949},
author = {Kaddour, Jean and S{\ae}mundsson, Steind{\'{o}}r and Deisenroth, Marc Peter},
booktitle = {arXiv},
eprint = {2007.08949},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaddour, S{\ae}mundsson, Deisenroth - 2020 - Probabilistic Active Meta-Learning.pdf:pdf},
keywords = {diversity:latent+entropy,env-type:parametrized,env:toy},
mendeley-tags = {diversity:latent+entropy,env-type:parametrized,env:toy},
title = {{Probabilistic Active Meta-Learning}},
year = {2020}
}
@article{Meng2017,
abstract = {Self-paced learning (SPL) is a recently proposed methodology designed by mimicking through the learning principle of humans/animals. A variety of SPL realization schemes have been designed for different computer vision and pattern recognition tasks, and empirically demonstrated to be effective in these applications. However, the literature is in lack of the theoretical understanding of SPL. Regarding this research gap, this study attempts to provide some new theoretical understanding of the SPL scheme. Specifically, we prove that the solution strategy on SPL accords with a majorization minimization algorithm implemented on an implicit objective function. Furthermore, we found that the loss function contained in this implicit objective has a similar configuration with the non-convex regularized penalty (NCRP) known in statistics and machine learning. Such connection inspires us to discover more intrinsic relationships between the SPL regimes and the NCRP forms, like smoothly clipped absolute deviation (SCAD), logarithmic penalty (LOG) and non-convex exponential penalty (EXP). The insight of the robustness under SPL can then be finely explained. We also analyze the capability of SPL regarding its easy loss-prior-embedding property, and provide an insightful interpretation of the effectiveness mechanism under current SPL variations. Moreover, we design a group-partial-order loss prior, which is especially useful for weakly labeled large-scale data processing tasks. By applying SPL with this loss prior to the FCVID dataset, which is currently one of the largest manually annotated video dataset, our method achieves state-of-the-art performance above existing methods, which further supports the proposed theoretical arguments.},
author = {Meng, Deyu and Zhao, Qian and Jiang, Lu},
doi = {10.1016/j.ins.2017.05.043},
file = {:C$\backslash$:/Users/liori/Downloads/1-s2.0-S0020025517307521-main.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Curriculum learning,Multimedia event detection,Non-convex regularized penalty,Self-paced learning},
month = {nov},
pages = {319--328},
publisher = {Elsevier Inc.},
title = {{A theoretical understanding of self-paced learning}},
volume = {414},
year = {2017}
}
@article{Jiang2017,
abstract = {Recent deep networks are capable of memorizing the entire data even when the labels are completely random. To overcome the overfitting on corrupted labels, we propose a novel technique of learning another neural network, called MentorNet, to supervise the training of the base deep networks, namely, StudentNet. During training, MentorNet provides a curriculum (sample weighting scheme) for StudentNet to focus on the sample the label of which is probably correct. Unlike the existing curriculum that is usually predefined by human experts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Experimental results demonstrate that our approach can significantly improve the generalization performance of deep networks trained on corrupted training data. Notably, to the best of our knowledge, we achieve the best-published result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels. The code are at https://github.com/google/mentornet},
archivePrefix = {arXiv},
arxivId = {1712.05055},
author = {Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
eprint = {1712.05055},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2017 - MentorNet Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels.pdf:pdf},
month = {dec},
title = {{MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels}},
url = {https://github.com/google/mentornet. http://arxiv.org/abs/1712.05055},
year = {2017}
}
@article{Wu2020,
abstract = {Inspired by human learning, researchers have proposed ordering examples during training based on their difficulty. Both curriculum learning, exposing a network to easier examples early in training, and anti-curriculum learning, showing the most difficult examples first, have been suggested as improvements to the standard i.i.d. training. In this work, we set out to investigate the relative benefits of ordered learning. We first investigate the $\backslash$emph{\{}implicit curricula{\}} resulting from architectural and optimization bias and find that samples are learned in a highly consistent order. Next, to quantify the benefit of $\backslash$emph{\{}explicit curricula{\}}, we conduct extensive experiments over thousands of orderings spanning three kinds of learning: curriculum, anti-curriculum, and random-curriculum -- in which the size of the training dataset is dynamically increased over time, but the examples are randomly ordered. We find that for standard benchmark datasets, curricula have only marginal benefits, and that randomly ordered samples perform as well or better than curricula and anti-curricula, suggesting that any benefit is entirely due to the dynamic training set size. Inspired by common use cases of curriculum learning in practice, we investigate the role of limited training time budget and noisy data in the success of curriculum learning. Our experiments demonstrate that curriculum, but not anti-curriculum can indeed improve the performance either with limited training time budget or in existence of noisy data.},
archivePrefix = {arXiv},
arxivId = {2012.03107},
author = {Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
eprint = {2012.03107},
file = {::},
month = {dec},
title = {{When Do Curricula Work?}},
url = {http://arxiv.org/abs/2012.03107},
year = {2020}
}
@article{Matiisen2020,
abstract = {We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task, and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e., where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student's performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with long short-term memory (LSTM) and navigation in Minecraft. Our automatically ordered curriculum of submazes enabled to solve a Minecraft maze that could not be solved at all when training directly on that maze, and the learning was an order of magnitude faster than a uniform sampling of those submazes.},
annote = {2017 openAi paper, Cited by most interesting papers 
A teacher chooses tasks from a known set to give to a student -{\textgreater} POMDP since the student state is unknown and we only see the score
Need to define: task difficulty, task mastery threshold, have a method that re-introduces old tasks (neural network forgetting)
Focus on algorithms that give tasks the student will improve quickly (high learning curve for reward) and are probabilistic
Fast improvement -{\textgreater} slow down when approaching optimal -{\textgreater} new task now shows faster improvement
Ideally start and end when all tasks are uniform
Use absolute value of curve to handle forgetting},
archivePrefix = {arXiv},
arxivId = {1707.00183},
author = {Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
doi = {10.1109/TNNLS.2019.2934906},
eprint = {1707.00183},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matiisen et al. - 2020 - Teacher-Student Curriculum Learning.pdf:pdf},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Active learning,curriculum learning,deep reinforcement learning,env-type:randomized,env:minecraft,learning progress,reward weights},
mendeley-tags = {env-type:randomized,env:minecraft,reward weights},
number = {9},
pages = {3732--3740},
pmid = {31502993},
publisher = {IEEE},
title = {{Teacher-Student Curriculum Learning}},
volume = {31},
year = {2020}
}
@article{Raileanu2020,
abstract = {Exploration in sparse reward environments remains one of the key challenges of model-free reinforcement learning. Instead of solely relying on extrinsic rewards provided by the environment, many state-of-the-art methods use intrinsic rewards to encourage exploration. However, we show that existing methods fall short in procedurally-generated environments where an agent is unlikely to visit a state more than once. We propose a novel type of intrinsic reward which encourages the agent to take actions that lead to significant changes in its learned state representation. We evaluate our method on multiple challenging procedurally-generated tasks in MiniGrid, as well as on tasks with high-dimensional observations used in prior work. Our experiments demonstrate that this approach is more sample efficient than existing exploration methods, particularly for procedurally-generated MiniGrid environments. Furthermore, we analyze the learned behavior as well as the intrinsic reward received by our agent. In contrast to previous approaches, our intrinsic reward does not diminish during the course of training and it rewards the agent substantially more for interacting with objects that it can control.},
annote = {Motivtion: intrinsic rewards based on state visit count don't work for procedurally-generated environments, and rewards based on surprise diminish as training continues. A method based on actions with high impact

Algorithm: Learn a dynmaics model with both forward (next state prediction) and inverse (action prediction), and use it to estimate differences between states

Method: high diference in state representation as impact, state visit count to prevent looping

Novelty: an interesting combination of both major intinsic rewards for exploration, learning a state representation via model learning

Theory: none},
archivePrefix = {arXiv},
arxivId = {2002.12292},
author = {Raileanu, Roberta and Rockt{\"{a}}schel, Tim},
eprint = {2002.12292},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raileanu, Rockt{\"{a}}schel - 2020 - RIDE Rewarding Impact-Driven Exploration for Procedurally-Generated Environments.pdf:pdf},
issn = {23318422},
month = {feb},
title = {{RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments}},
url = {http://arxiv.org/abs/2002.12292},
year = {2020}
}
@techreport{Gutierrez2020,
abstract = {In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of tasks to prepare for and learn faster in new, unseen, but related tasks. The training tasks are usually hand-crafted to be representative of the expected distribution of test tasks and hence all used in training. We show that given a set of training tasks, learning can be both faster and more effective (leading to better performance in the test tasks), if the training tasks are appropriately selected. We propose a task selection algorithm, Information-Theoretic Task Selection (ITTS), based on information theory, which optimizes the set of tasks used for training in meta-RL, irrespectively of how they are generated. The algorithm establishes which training tasks are both sufficiently relevant for the test tasks, and different enough from one another. We reproduce different meta-RL experiments from the literature and show that ITTS improves the final performance in all of them.},
annote = {2020 NeuroIPS
MetaRL, pre-defined training tasks, test task from same distribution
Method to choose the next task (curriculum) and also terminate
Assumes access to optimal policy for each task
Assumes a validation set of different tasks in addition to training tasks, pick training task set of different (KL divergence of optimal policies is big) and relevant (for any validation task, do X steps with policy, learn Y episodes to adapt to validation task, assert entropy of the adapted policy is lower = info gain from adapting = policy closer to deterministic after adapting)
It's much better than MAML/RL{\^{}}2 without task selection, but this is super inefficient},
archivePrefix = {arXiv},
arxivId = {2011.01054},
author = {Gutierrez, Ricardo Luna and Leonetti, Matteo},
eprint = {2011.01054},
keywords = {alg:RL2,alg:maml,diversity:entropy,env-type:set,env:toy},
mendeley-tags = {alg:RL2,alg:maml,diversity:entropy,env-type:set,env:toy},
title = {{Information-theoretic Task Selection for Meta-Reinforcement Learning}},
url = {http://arxiv.org/abs/2011.01054},
year = {2020}
}
@techreport{Jiang2020,
abstract = {Simulated environments with procedurally generated content have become popular benchmarks for testing systematic generalization of reinforcement learning agents. Every level in such an environment is algorithmically created, thereby exhibiting a unique configuration of underlying factors of variation, such as layout, positions of entities, asset appearances, or even the rules governing environment transitions. Fixed sets of training levels can be determined to aid comparison and reproducibility, and test levels can be held out to evaluate the generalization and robustness of agents. While prior work samples training levels in a direct way (e.g.{\~{}}uniformly) for the agent to learn from, we investigate the hypothesis that different levels provide different learning progress for an agent at specific times during training. We introduce Prioritized Level Replay, a general framework for estimating the future learning potential of a level given the current state of the agent's policy. We find that temporal-difference (TD) errors, while previously used to selectively sample past transitions, also prove effective for scoring a level's future learning potential in generating entire episodes that an agent would experience when replaying it. We report significantly improved sample-efficiency and generalization on the majority of Procgen Benchmark environments as well as two challenging MiniGrid environments. Lastly, we present a qualitative analysis showing that Prioritized Level Replay induces an implicit curriculum, taking the agent gradually from easier to harder levels.},
annote = {Off-policy HER-like sampling fix.
Assign env replay probability based on the TD-error of the agent
Add some optimistic “how long ago we last replayed this level” bonus},
archivePrefix = {arXiv},
arxivId = {2010.03934},
author = {Jiang, Minqi and Grefenstette, Ed and Rockt{\"{a}}schel, Tim},
eprint = {2010.03934},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang, Grefenstette, Rockt{\"{a}}schel - 2020 - Prioritized Level Replay.pdf:pdf},
issn = {23318422},
keywords = {env-type:parametrized,env:procgen,reward weights},
mendeley-tags = {env-type:parametrized,env:procgen,reward weights},
title = {{Prioritized Level Replay}},
url = {http://arxiv.org/abs/2010.03934},
year = {2020}
}
@inproceedings{Wohlke2020,
abstract = {Sparse reward problems present a challenge for reinforcement learning (RL) agents. Previous work has shown that choosing start states according to a curriculum can significantly improve the learning performance. We observe that many existing curriculum generation algorithms rely on two key components: Performance measure estimation and a start selection policy. Therefore, we propose a unifying framework for performance-based start state curricula in RL, which allows to analyze and compare the performance influence of the two key components. Furthermore, a new start state selection policy using spatial performance measure gradients is introduced. We conduct extensive empirical evaluations to compare performance-based start state curricula and investigate the influence of performance measure model choice and estimation. Benchmarking on difficult robotic navigation tasks and a high-dimensional robotic manipulation task, we demonstrate state-of-the-art performance of our novel spatial gradient curriculum.},
annote = {Pick start state according to value function estimate.
Can reset to new start state (from predefined set) whenever.
Single fixed, sparse-reward task.
Measured on probability to reach goal given random start state.},
author = {W{\"{o}}hlke, Jan and Schmitt, Felix and van Hoof, Herke},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/W{\"{o}}hlke, Schmitt, Van Hoof - 2020 - A Performance-Based Start State Curriculum Framework for Reinforcement Learning.pdf:pdf},
isbn = {9781450375184},
issn = {15582914},
keywords = {Learning agent capabilities,Machine learning for robotics,Reinforcement learning,env-type:set,env:gridworld,env:gripper,env:mujoco,goal reachability,reward weights},
mendeley-tags = {env-type:set,env:gridworld,env:gripper,env:mujoco,goal reachability,reward weights},
pages = {1503--1511},
title = {{A performance-based start state curriculum framework for reinforcement learning}},
url = {www.ifaamas.org},
volume = {2020-May},
year = {2020}
}
@techreport{Weinshall2020,
abstract = {Curriculum Learning is motivated by human cognition, where teaching often involves gradually exposing the learner to examples in a meaningful order, from easy to hard. Although methods based on this concept have been empirically shown to improve performance of several machine learning algorithms, no theoretical analysis has been provided even for simple cases. To address this shortfall, we start by formulating an ideal definition of difficulty score-the loss of the optimal hypothesis at a given datapoint. We analyze the possible contribution of curriculum learning based on this score in two convex problems-linear regression, and binary classification by hinge loss minimization. We show that in both cases, the convergence rate of SGD optimization decreases monotonically with the difficulty score, in accordance with earlier empirical results. We also prove that when the difficulty score is fixed, the convergence rate of SGD optimization is monotonically increasing with respect to the loss of the current hypothesis at each point. We discuss how these results settle some confusion in the literature where two apparently opposing heuristics are reported to improve performance: curriculum learning in which easier points are given priority, vs hard data mining where the more difficult points are sought out.},
author = {Weinshall, Daphna and Amir, Dan},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {curriculum learning,hinge loss minimization,linear regression},
pages = {1--19},
title = {{Theory of Curriculum Learning, with Convex Loss Functions}},
url = {http://jmlr.org/papers/v21/18-751.html.},
volume = {21},
year = {2020}
}
@article{Cioba2021,
abstract = {Meta-learning models transfer the knowledge acquired from previous tasks to quickly learn new ones. They are trained on benchmarks with a fixed number of data points per task. This number is usually arbitrary and it is unknown how it affects performance at testing. Since labelling of data is expensive, finding the optimal allocation of labels across training tasks may reduce costs. Given a fixed budget of labels, should we use a small number of highly labelled tasks, or many tasks with few labels each? Should we allocate more labels to some tasks and less to others? We show that: 1) If tasks are homogeneous, there is a uniform optimal allocation, whereby all tasks get the same amount of data; 2) At fixed budget, there is a trade-off between number of tasks and number of data points per task, with a unique and constant optimum; 3) When trained separately, harder task should get more data, at the cost of a smaller number of tasks; 4) When training on a mixture of easy and hard tasks, more data should be allocated to easy tasks. Interestingly, Neuroscience experiments have shown that human visual skills also transfer better from easy tasks. We prove these results mathematically on mixed linear regression, and we show empirically that the same results hold for few-shot image classification on CIFAR-FS and mini-ImageNet. Our results provide guidance for allocating labels across tasks when collecting data for meta-learning.},
archivePrefix = {arXiv},
arxivId = {2103.08463},
author = {Cioba, Alexandru and Bromberg, Michael and Wang, Qian and Niyogi, Ritwik and Batzolis, Georgios and Garcia, Jezabel and Shiu, Da-Shan and Bernacchia, Alberto},
eprint = {2103.08463},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cioba et al. - 2021 - How to distribute data across tasks for meta-learning.pdf:pdf},
isbn = {2103.08463v2},
month = {mar},
title = {{How to distribute data across tasks for meta-learning?}},
url = {http://arxiv.org/abs/2103.08463},
year = {2021}
}
@article{Xu2020,
abstract = {Deep reinforcement learning includes a broad family of algorithms that parameterise an internal representation, such as a value function or policy, by a deep neural network. Each algorithm optimises its parameters with respect to an objective, such as Q-learning or policy gradient, that defines its semantics. In this work, we propose an algorithm based on meta-gradient descent that discovers its own objective, flexibly parameterised by a deep neural network, solely from interactive experience with its environment. Over time, this allows the agent to learn how to learn increasingly effectively. Furthermore, because the objective is discovered online, it can adapt to changes over time. We demonstrate that the algorithm discovers how to address several important issues in RL, such as bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm adapts over time to learn with greater efficiency, eventually outperforming the median score of a strong actor-critic baseline.},
archivePrefix = {arXiv},
arxivId = {2007.08433},
author = {Xu, Zhongwen and van Hasselt, Hado and Hessel, Matteo and Oh, Junhyuk and Singh, Satinder and Silver, David},
eprint = {2007.08433},
journal = {arXiv},
keywords = {env:atari},
mendeley-tags = {env:atari},
month = {jul},
title = {{Meta-Gradient Reinforcement Learning with an Objective Discovered Online}},
url = {http://arxiv.org/abs/2007.08433},
year = {2020}
}
@techreport{Bassich,
abstract = {Curriculum Learning for Reinforcement Learning is an increasingly popular technique that involves training an agent on a defined sequence of intermediate tasks, called a Curriculum, to increase the agent's performance and learning speed. This paper introduces a novel paradigm for automatic curriculum generation based on a progression of task complexity. Different progression functions are introduced, including an autonomous online task progression based on the performance of the agent. The progression function also determines how long the agent should train on each intermediate task, which is an open problem in other task-based curriculum approaches. The benefits and wide applicability of our approach are shown by empirically comparing its performance to two state-of-the-art Curriculum Learning algorithms on a grid world and on a complex simulated navigation domain.},
archivePrefix = {arXiv},
arxivId = {2008.00511v1},
author = {Bassich, Andrea and Foglino, Francesco and Leonetti, Matteo and Kudenko, Daniel},
eprint = {2008.00511v1},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bassich et al. - Unknown - Curriculum Learning with a Progression Function.pdf:pdf},
keywords = {Curriculum,Learning,Learning {\textperiodcentered},Reinforcement},
title = {{Curriculum Learning with a Progression Function}}
}
@techreport{Romac,
abstract = {Training autonomous agents able to generalize to multiple tasks is a key target of Deep Reinforcement Learning (DRL) research. In parallel to improving DRL algorithms themselves, Automatic Curriculum Learning (ACL) study how teacher algorithms can train DRL agents more efficiently by adapting task selection to their evolving abilities. While multiple standard benchmarks exist to compare DRL agents, there is currently no such thing for ACL algorithms. Thus, comparing existing approaches is difficult, as too many experimental parameters differ from paper to paper. In this work, we identify several key challenges faced by ACL algorithms. Based on these, we present TeachMyAgent (TA), a benchmark of current ACL algorithms leveraging procedural task generation. It includes 1) challenge-specific unit-tests using variants of a procedural Box2D bipedal walker environment, and 2) a new procedural Parkour environment combining most ACL challenges, making it ideal for global performance assessment. We then use TeachMyAgent to conduct a comparative study of representative existing approaches, showcasing the competitiveness of some ACL algorithms that do not use expert knowledge. We also show that the Parkour environment remains an open problem. We open-source our environments, all studied ACL algorithms (collected from open-source code or re-implemented), and DRL students in a Python package available at https://github. com/flowersteam/TeachMyAgent.},
archivePrefix = {arXiv},
arxivId = {2103.09815v1},
author = {Romac, Cl{\'{e}}ment and Portelas, R{\'{e}}my and Hofmann, Katja and Oudeyer, Pierre-Yves},
eprint = {2103.09815v1},
file = {::},
title = {{TeachMyAgent: a Benchmark for Automatic Curriculum Learning in Deep RL}},
url = {https://sites.google.com/view/teachmyagent}
}
@article{Chua2018,
abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
annote = {Motivtion: MBRL methods have poor asymptotic performance, using an ensemble (that approximates bayesian NN) can help

Algorithm: MPC with CEM sampling, the dynamics model is learned with an ensemble 

Method: 

Novelty: significant empirical improvement over model-free methods and simple model-based methods, mostly shown for complex tasks, medium timestep count

Theory: none},
archivePrefix = {arXiv},
arxivId = {1805.12114},
author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
eprint = {1805.12114},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {may},
pages = {4754--4765},
title = {{Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models}},
url = {https://github.com/kchua/handful-of-trials http://arxiv.org/abs/1805.12114},
volume = {2018-Decem},
year = {2018}
}
@article{Ha2018,
abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
annote = {Motivtion: learn dynamics model for visual tasks

Algorithm: VAE with RNN and a linear controller

Method: RNN models the transition dynamics. The VAE is only needed for visual feature encoding

Novelty: first successful model learning with RNN

Theory: none},
archivePrefix = {arXiv},
arxivId = {1803.10122},
author = {Ha, David and Schmidhuber, J{\"{u}}rgen},
doi = {10.5281/zenodo.1207631},
eprint = {1803.10122},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ha, Schmidhuber - 2018 - World Models.pdf:pdf},
month = {mar},
title = {{World Models}},
url = {https://worldmodels.github.io http://arxiv.org/abs/1803.10122 http://dx.doi.org/10.5281/zenodo.1207631},
year = {2018}
}
@article{Seldin2010,
abstract = {We derive PAC-Bayesian generalization bounds for supervised and unsupervised learning models based on clustering, such as co-clustering, matrix tri-factorization, graphical models, graph clustering , and pairwise clustering. 1 We begin with the analysis of co-clustering, which is a widely used approach to the analysis of data matrices. We distinguish among two tasks in matrix data analysis: discriminative prediction of the missing entries in data matrices and estimation of the joint probability distribution of row and column variables in co-occurrence matrices. We derive PAC-Bayesian generalization bounds for the expected out-of-sample performance of co-clustering-based solutions for these two tasks. The analysis yields regularization terms that were absent in the previous formulations of co-clustering. The bounds suggest that the expected performance of co-clustering is governed by a trade-off between its empirical performance and the mutual information preserved by the cluster variables on row and column IDs. We derive an iterative projection algorithm for finding a local optimum of this trade-off for discriminative prediction tasks. This algorithm achieved state-of-the-art performance in the MovieLens collaborative filtering task. Our co-clustering model can also be seen as matrix tri-factorization and the results provide generalization bounds, regularization terms, and new algorithms for this form of matrix factorization. The analysis of co-clustering is extended to tree-shaped graphical models, which can be used to analyze high dimensional tensors. According to the bounds, the generalization abilities of tree-shaped graphical models depend on a trade-off between their empirical data fit and the mutual information that is propagated up the tree levels. We also formulate weighted graph clustering as a prediction problem: given a subset of edge weights we analyze the ability of graph clustering to predict the remaining edge weights. The analysis of co-clustering easily extends to this problem and suggests that graph clustering should optimize the trade-off between empirical data fit and the mutual information that clusters preserve on graph nodes.},
author = {Seldin, Yevgeny and De, Seldin@tuebingen Mpg and Tishby, Naftali},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seldin, De, Tishby - 2010 - PAC-Bayesian Analysis of Co-clustering and Beyond.pdf:pdf},
journal = {Also in the School of Computer Science and Engineering},
keywords = {com-binatorial priors,density estimation *,graph clustering,graphical models,matrix tri-factorization,pairwise clustering},
pages = {3595--3646},
title = {{PAC-Bayesian Analysis of Co-clustering and Beyond}},
volume = {11},
year = {2010}
}
@techreport{Mehta2019,
abstract = {Domain randomization is a popular technique for improving domain transfer, often used in a zero-shot setting when the target domain is unknown or cannot easily be used for training. In this work, we empirically examine the effects of domain randomization on agent generalization. Our experiments show that domain randomization may lead to suboptimal, high-variance policies, which we attribute to the uniform sampling of environment parameters. We propose Active Domain Randomization, a novel algorithm that learns a parameter sampling strategy. Our method looks for the most informative environment variations within the given randomization ranges by leveraging the discrepancies of policy rollouts in randomized and reference environment instances. We find that training more frequently on these instances leads to better overall agent generalization. Our experiments across various physics-based simulated and real-robot tasks show that this enhancement leads to more robust, consistent policies.},
annote = {creates tasks by learning a “Stein Variational Policy Gradient” (SVPG) to learn “particles” (env parameters) with high entropy. 
Reward for env = how well a “discriminator” network can say that trajectory came from new env vs some reference env -{\textgreater} a policy=env is good for teaching if it's very different from the reference environment (similar notion to disagreement?)},
archivePrefix = {arXiv},
arxivId = {1904.04762},
author = {Mehta, Bhairav and Diaz, Manfred and Golemo, Florian and Pal, Christopher J. and Paull, Liams},
booktitle = {arXiv},
eprint = {1904.04762},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehta et al. - 2019 - Active domain randomization.pdf:pdf},
keywords = {Domain randomization,Reinforcement learning,Sim2real,diversity:discriminator,env-type:parametrized,env:sim2real,env:toy},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:sim2real,env:toy},
title = {{Active domain randomization}},
year = {2019}
}
@misc{Zhou2019,
abstract = {Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.},
archivePrefix = {arXiv},
arxivId = {1906.03352},
author = {Zhou, Allan and Jang, Eric and Kappler, Daniel and Herzog, Alex and Khansari, Mohi and Wohlhart, Paul and Bai, Yunfei and Kalakrishnan, Mrinal and Levine, Sergey and Finn, Chelsea},
booktitle = {arXiv},
eprint = {1906.03352},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2019 - Watch, try, learn Meta-learning from demonstrations and rewards.pdf:pdf},
keywords = {IRL,env:gripper},
mendeley-tags = {IRL,env:gripper},
title = {{Watch, try, learn: Meta-learning from demonstrations and rewards}},
url = {https://sites.google.com/view/watch-try-learn-project},
year = {2019}
}
@article{Turchetta2020,
abstract = {In safety-critical applications autonomous agents may need to learn in an environment where mistakes can be very costly. In such settings, the agent needs to behave safely not only after but also while learning. To achieve this, existing safe reinforcement learning methods make an agent rely on priors that let it avoid dangerous situations during exploration with high probability, but both the probabilistic guarantees and the smoothness assumptions inherent in the priors are not viable in many scenarios of interest such as autonomous driving. This paper presents an alternative approach inspired by human teaching, where an agent learns under the supervision of an automatic instructor that saves the agent from violating constraints during learning. In this model, we introduce the monitor that neither needs to know how to do well at the task the agent is learning nor needs to know how the environment works. Instead, it has a library of reset controllers that it activates when the agent starts behaving dangerously, preventing it from doing damage. Crucially, the choices of which reset controller to apply in which situation affect the speed of agent learning. Based on observing agents' progress, the teacher itself learns a policy for choosing the reset controllers, a curriculum, to optimize the agent's final policy reward. Our experiments use this framework in two environments to induce curricula for safe and efficient learning.},
annote = {Assume “teacher” that gets a set of unwanted states reset-distributions, if agent gets to unwanted state, teacher can move him to a safe state with the reset
Shows some safety guarantees under this new teacher constraint, if teacher has the unsafe states in its set and you can't reach an unsafe state from an unwanted (but safe) state, the agent will not violate constraints ever

Curriculum = the teacher's chosen intervention set (the agent sees this by the provided MDP)},
archivePrefix = {arXiv},
arxivId = {2006.12136},
author = {Turchetta, Matteo and Kolobov, Andrey and Shah, Shital and Krause, Andreas and Agarwal, Alekh},
eprint = {2006.12136},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turchetta et al. - 2020 - Safe Reinforcement Learning via Curriculum Induction.pdf:pdf},
journal = {arXiv},
keywords = {Safety objective,env-type:custom,env:toy,fixed curriculum},
mendeley-tags = {Safety objective,env-type:custom,env:toy,fixed curriculum},
month = {jun},
title = {{Safe Reinforcement Learning via Curriculum Induction}},
url = {http://arxiv.org/abs/2006.12136},
year = {2020}
}
@techreport{Florensa2017,
abstract = {Many relevant tasks require an agent to reach a certain state or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in “reverse,” gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.},
annote = {Given goal states, use random motion in MDP to find more states
Use new starts as new curriculum tasks, do a policy training step
Label, filter, sort tasks by difficulty (want mid-difficulty tasks - reward in given range) for new policy},
archivePrefix = {arXiv},
arxivId = {1707.05300},
author = {Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael R. and Abbeel, Pieter},
booktitle = {arXiv},
eprint = {1707.05300},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Florensa et al. - 2017 - Reverse curriculum generation for reinforcement learning.pdf:pdf},
keywords = {Automatic Curriculum Generation,Reinforcement Learning,Robotic Manipulation,alg:trpo,env-type:randomized,env:custom,fixed curriculum,medium difficulty},
mendeley-tags = {alg:trpo,env-type:randomized,env:custom,fixed curriculum,medium difficulty},
title = {{Reverse curriculum generation for reinforcement learning}},
year = {2017}
}
@article{Liu2020,
abstract = {The goal of meta-reinforcement learning (meta-RL) is to build agents that can quickly learn new tasks by leveraging prior experience on related tasks. Learning a new task often requires both exploring to gather task-relevant information and exploiting this information to solve the task. In principle, optimal exploration and exploitation can be learned end-to-end by simply maximizing task performance. However, such meta-RL approaches struggle with local optima due to a chicken-and-egg problem: learning to explore requires good exploitation to gauge the exploration's utility, but learning to exploit requires information gathered via exploration. Optimizing separate objectives for exploration and exploitation can avoid this problem, but prior meta-RL exploration objectives yield suboptimal policies that gather information irrelevant to the task. We alleviate both concerns by constructing an exploitation objective that automatically identifies task-relevant information and an exploration objective to recover only this information. This avoids local optima in end-to-end training, without sacrificing optimal exploration. Empirically, DREAM substantially outperforms existing approaches on complex meta-RL problems, such as sparse-reward 3D visual navigation. Videos of DREAM: https://ezliu.github.io/dream/},
archivePrefix = {arXiv},
arxivId = {2008.02790},
author = {Liu, Evan Zheran and Raghunathan, Aditi and Liang, Percy and Finn, Chelsea},
eprint = {2008.02790},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2020 - Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices.pdf:pdf},
isbn = {2008.02790v2},
month = {aug},
title = {{Decoupling Exploration and Exploitation for Meta-Reinforcement Learning without Sacrifices}},
url = {https://ezliu.github.io/dream/ http://arxiv.org/abs/2008.02790},
year = {2020}
}
@techreport{Gupta2018,
abstract = {Meta-learning is a powerful tool that builds on multi-task learning to learn how to quickly adapt a model to new tasks. In the context of reinforcement learning, meta-learning algorithms can acquire reinforcement learning procedures to solve new problems more efficiently by meta-learning prior tasks. The performance of meta-learning algorithms critically depends on the tasks available for meta-training: in the same way that supervised learning algorithms generalize best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We describe a general recipe for unsupervised meta-reinforcement learning, and describe an effective instantiation of this approach based on a recently proposed unsupervised exploration technique and model-agnostic meta-learning. We also discuss practical and conceptual considerations for developing unsupervised meta-learning methods. Our experimental results demonstrate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design, significantly exceeds the performance of learning from scratch, and even matches performance of meta-learning methods that use hand-specified task distributions.},
annote = {2020 Berkley paper
Trying to minimize worst case adaptation regret
Create tasks for meta-RL by defining goal states/trajectories
Latent “task” variable, try to find goals/trajectories with high mutual information to task variable},
archivePrefix = {arXiv},
arxivId = {1806.04640},
author = {Gupta, Abhishek and Finn, Chelsea and Eysenbach, Benjamin and Levine, Sergey},
booktitle = {arXiv},
eprint = {1806.04640},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta et al. - 2018 - Unsupervised meta-learning for reinforcement learning.pdf:pdf},
keywords = {diversity:discriminator,diversity:entropy,env-type:randomized,env:mujoco},
mendeley-tags = {diversity:discriminator,diversity:entropy,env-type:randomized,env:mujoco},
title = {{Unsupervised meta-learning for reinforcement learning}},
year = {2018}
}
@article{Zhou2021,
abstract = {We consider the problem of teaching via demonstrations in sequential decision-making settings. In particular, we study how to design a personalized curriculum over demonstrations to speed up the learner's convergence. We provide a unified curriculum strategy for two popular learner models: Maximum Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy Behavioral Cloning (CrossEnt-BC). Our unified strategy induces a ranking over demonstrations based on a notion of difficulty scores computed w.r.t. the teacher's optimal policy and the learner's current policy. Compared to the state of the art, our strategy doesn't require access to the learner's internal dynamics and still enjoys similar convergence guarantees under mild technical conditions. Furthermore, we adapt our curriculum strategy to teach a learner using domain knowledge in the form of task-specific difficulty scores when the teacher's optimal policy is unknown. Experiments on a car driving simulator environment and shortest path problems in a grid-world environment demonstrate the effectiveness of our proposed curriculum strategy.},
archivePrefix = {arXiv},
arxivId = {2106.04696},
author = {Yengera, Gaurav and Devidze, Rati and Kamalaruban, Parameswaran and Singla, Adish},
eprint = {2106.04696},
file = {::},
month = {jun},
title = {{Curriculum Design for Teaching via Demonstrations: Theory and Applications}},
url = {http://arxiv.org/abs/2106.04696},
volume = {130},
year = {2021}
}
@article{Saglietti2021,
abstract = {In humans and animals, curriculum learning -- presenting data in a curated order - is critical to rapid learning and effective pedagogy. Yet in machine learning, curricula are not widely used and empirically often yield only moderate benefits. This stark difference in the importance of curriculum raises a fundamental theoretical question: when and why does curriculum learning help? In this work, we analyse a prototypical neural network model of curriculum learning in the high-dimensional limit, employing statistical physics methods. Curricula could in principle change both the learning speed and asymptotic performance of a model. To study the former, we provide an exact description of the online learning setting, confirming the long-standing experimental observation that curricula can modestly speed up learning. To study the latter, we derive performance in a batch learning setting, in which a network trains to convergence in successive phases of learning on dataset slices of varying difficulty. With standard training losses, curriculum does not provide generalisation benefit, in line with empirical observations. However, we show that by connecting different learning phases through simple Gaussian priors, curriculum can yield a large improvement in test performance. Taken together, our reduced analytical descriptions help reconcile apparently conflicting empirical results and trace regimes where curriculum learning yields the largest gains. More broadly, our results suggest that fully exploiting a curriculum may require explicit changes to the loss function at curriculum boundaries.},
annote = {Shows theoretical advantage of easy first for perceptrons, where easy=few irrelevant features.
This is just "feature selection is good"},
archivePrefix = {arXiv},
arxivId = {2106.08068},
author = {Saglietti, Luca and Mannelli, Stefano Sarao and Saxe, Andrew},
eprint = {2106.08068},
month = {jun},
title = {{An Analytical Theory of Curriculum Learning in Teacher-Student Networks}},
url = {http://arxiv.org/abs/2106.08068},
year = {2021}
}
@article{Gong2019,
abstract = {Curriculum Learning (CL) is a recently proposed learning paradigm that aims to achieve satisfactory performance by properly organizing the learning sequence from simple curriculum examples to more difficult ones. Up to now, few works have been done to explore CL for the data with graph structure. Therefore, this article proposes a novel CL algorithm that can be utilized to guide the Label Propagation (LP) over graphs, of which the target is to “learn” the labels of unlabeled examples on the graphs. Specifically, we assume that different unlabeled examples have different levels of difficulty for propagation, and their label learning should follow a simple-to-difficult sequence with the updated curricula. Furthermore, considering that the practical data are often characterized by multiple modalities, every modality in our method is associated with a “teacher” that not only evaluates the difficulties of examples from its own viewpoint, but also cooperates with other teachers to generate the overall simplest curriculum examples for propagation. By taking the curriculums suggested by the teachers as a whole, the common preference (i.e., commonality) of teachers on selecting the simplest examples can be discovered by a row-sparse matrix, and their distinct opinions (i.e., individuality) are captured by a sparse noise matrix. As a result, an accurate curriculum sequence can be established and the propagation quality can thus be improved. Theoretically, we prove that the propagation risk bound is closely related to the examples' difficulty information, and empirically, we show that our method can generate higher accuracy than the state-of-the-art CL approach and LP algorithms on various multi-modal tasks.},
annote = {Meh},
author = {Gong, Chen and Yang, Jian and Tao, Dacheng},
doi = {10.1145/3322122},
file = {:C$\backslash$:/Users/liori/Downloads/3322122.pdf:pdf},
issn = {2157-6904},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {Curriculum learning,label propagation,multi-modal learning,semi-supervised learning},
month = {aug},
number = {4},
pages = {1--25},
publisher = {Association for Computing Machinery (ACM)},
title = {{Multi-Modal Curriculum Learning over Graphs}},
url = {https://dl.acm.org/doi/abs/10.1145/3322122},
volume = {10},
year = {2019}
}
@inproceedings{Weinshall2018,
abstract = {We provide theoretical investigation of curriculum learning in the context of stochastic gradient descent when optimizing the convex linear regression loss. We prove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examples. Moreover, among all equally difficult points, convergence is faster when using points which incur higher loss with respect to the current hypothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, pre-trained on a different task. While this approach can only approximate the ideal curriculum, we observe empirically similar behavior to the one predicted by the theory, namely, a significant boost in convergence speed at the beginning of training. When the task is made more difficult, improvement in generalization performance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.},
archivePrefix = {arXiv},
arxivId = {1802.03796},
author = {Weinshall, Daphna and Cohen, Gad and Amir, Dan},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.03796},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinshall, Cohen, Amir - 2018 - Curriculum learning by transfer learning Theory and experiments with deep networks.pdf:pdf},
isbn = {9781510867963},
pages = {8331--8339},
title = {{Curriculum learning by transfer learning: Theory and experiments with deep networks}},
volume = {12},
year = {2018}
}
@techreport{Raparthy2020,
abstract = {Goal-directed Reinforcement Learning (RL) traditionally considers an agent interacting with an environment, prescribing a real-valued reward to an agent proportional to the completion of some goal. Goal-directed RL has seen large gains in sample efficiency, due to the ease of reusing or generating new experience by proposing goals. In this work, we build on the framework of self-play, allowing an agent to interact with itself in order to make progress on some unknown task. We use Active Domain Randomization and self-play to create a novel, coupled environment-goal curriculum, where agents learn through progressively more difficult tasks and environment variations. Our method, Self-Supervised Active Domain Randomization (SS-ADR), generates a growing curriculum, encouraging the agent to try tasks that are just outside of its current capabilities, while building a domain-randomization curriculum that enables state-of-the-art results on various sim2real transfer tasks. Our results show that a curriculum of co-evolving the environment difficulty along with the difficulty of goals set in each environment, provides practical benefits in the goal-directed tasks tested.},
annote = {Teacher tries to choose a task that's just a bit too hard - easy for teacher to solve (ref env) but hard for agent (on random env)
Teacher is the old agent, does a bunch of actions in reference env until stopping, gives that goal to the agent. Agent has random env (torque/weight/{\ldots}) so it's hopefully harder},
archivePrefix = {arXiv},
arxivId = {2002.07911},
author = {Raparthy, Sharath Chandra and Mehta, Bhairav and Golemo, Florian and Paull, Liam},
booktitle = {arXiv},
eprint = {2002.07911},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raparthy et al. - 2020 - Generating automatic curricula via self-supervised active domain randomization.pdf:pdf},
keywords = {diversity:discriminator,env-type:parametrized,env:custom,env:sim2real},
mendeley-tags = {diversity:discriminator,env-type:parametrized,env:custom,env:sim2real},
title = {{Generating automatic curricula via self-supervised active domain randomization}},
year = {2020}
}
@inproceedings{Bateni2020,
abstract = {Few-shot learning is a fundamental task in computer vision that carries the promise of alleviating the need for exhaustively labeled data. Most few-shot learning approaches to date have focused on progressively more complex neural feature extractors and classifier adaptation strategies, and the refinement of the task definition itself. In this paper, we explore the hypothesis that a simple class-covariance-based distance metric, namely the Mahalanobis distance, adopted into a state of the art few-shot learning approach (CNAPS [30]) can, in and of itself, lead to a significant performance improvement. We also discover that it is possible to learn adaptive feature extractors that allow useful estimation of the high dimensional feature covariances required by this metric from surprisingly few samples. The result of our work is a new “Simple CNAPS” architecture which has up to 9.2{\%} fewer trainable parameters than CNAPS and performs up to 6.1{\%} better than state of the art on the standard few-shot image classification benchmark dataset.},
archivePrefix = {arXiv},
arxivId = {1912.03432},
author = {Bateni, Peyman and Goyal, Raghav and Masrani, Vaden and Wood, Frank and Sigal, Leonid},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR42600.2020.01450},
eprint = {1912.03432},
file = {:C$\backslash$:/Users/liori/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bateni et al. - 2020 - Improved few-shot visual classification.pdf:pdf},
issn = {10636919},
pages = {14481--14490},
title = {{Improved few-shot visual classification}},
year = {2020}
}
