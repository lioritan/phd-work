\documentclass[letterpaper]{article}

% AAAI-style 2-per-page format, without the annoying bits
\setlength\topmargin{-0.25in} \setlength\oddsidemargin{-0.25in}
\setlength\textheight{9.0in} \setlength\textwidth{7.0in}
\setlength\columnsep{0.375in} \newlength\titlebox \setlength\titlebox{2.25in}
\setlength\headheight{0pt}  \setlength\headsep{0pt}
\flushbottom \sloppy

\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{natbib}

\usepackage{times} 
\usepackage{helvet}  
\usepackage{courier}  
\usepackage{url}  
\usepackage{graphicx} 

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}



\usepackage{multirow}
\usepackage{ctable}
\usepackage{color}
\usepackage{natbib}
\usepackage[normalem]{ulem}


\usepackage{romannum}

%\usepackage[style=authoryear]{biblatex}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black, % color for table of contents
	citecolor=black, % color for citations
	urlcolor=blue, % color for hyperlinks
	bookmarks=true,
}
\urlstyle{same}




\raggedbottom %nicer enumerate
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{hypothesis}{Hypothesis}[section]
\newtheorem{assumption}{Assumption}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Summary of continual and lifelong learning frontiers}
\begin{document}
	
	\pagenumbering{arabic}
	\maketitle
	
	
\section{Introduction}
	

\section{Literature review}

Relevant theory papers:

\begin{itemize}
	\item classical transfer learning and bounds
	\item PAC-Bayes and domain adaptation \citep{germain2020pac}
	\item PAC-Bayesian Domain Adaptation Bounds for Multiclass Learners \citep{sicilia2022pac}
	\item Beyond $\mathcal{H}$-Divergence: Domain Adaptation Theory With Jensen-Shannon Divergence \citep{shui2020beyond}
	\item Gap Minimization for Knowledge Sharing and Transfer \citep{wang2022gap}
	\item A Theory for Knowledge Transfer in Continual Learning \citep{benavides2022theory}
	\item Online PAC-Bayes Learning \citep{haddouche2022online}
\end{itemize}

Papers specific to linear regression/NTK and transfer/meta/continual:
\begin{enumerate}
	\item How catastrophic can catastrophic forgetting be in linear regression? \citep{evron2022catastrophic}
	\item Generalisation guarantees for continual learning with orthogonal gradient descent \citep{bennani2020generalisation}
	\item  Learning Curves for Continual Learning in Neural Networks: Self-Knowledge Transfer and Forgetting \citep{karakida2021learning}
	\item Curriculum learning by transfer learning: Theory and experiments with deep networks \citep{weinshall2018curriculum}
\end{enumerate}


\subsection{Problem variations}

Task/data order:
\begin{itemize}
	\item $i.i.d.$
	\item Predictable
	\item Curriculum
	\item Adversarial
\end{itemize}

Settings: $P_{x,y}$ identical, $P_x$ identical and $P_{y|x}$ differs, complete difference.

Task boundaries: discrete or continuous, known or unknown.

Usual metrics:
Regret $$REGRET_T = \sum_{t=1}^{T}\mathcal{L}_t(\theta_t)-\min_\theta \sum_{t=1}^{T}\mathcal{L}_t(\theta)$$
Cannot be calculated in practice, want sub-linear regret (in $t$).

Backwards transfer: new task improves old tasks compared to learning from scratch, Forward transfer: old tasks improve compared to learning from scratch.

\subsection{Common known approaches}
\begin{itemize}
	\item Follow the leader - store all data seen and train on it, fine tune to task
	\item Gradient step on new data (no memory, experiences forgetting)
	\item Store little data per task, make sure we don't unlearn (minimize loss subject to no empirical decrease on all previous tasks)
	\item Follow the meta-leader - store all data seen and meta-train on it
\end{itemize}

\section{Specific problem formulation: PAC-Bayes forgetting}
	
\subsection{Notation and preliminaries}

\begin{defn}
	A task environment $\tau$ is a distribution over tasks $\mathcal{D}\sim \tau$. A task $\mathcal{D}$ is a probability distribution over $\mathcal{X}\times \mathcal{Y}$.
\end{defn}

\begin{defn}
	A set of $n$ tasks $\{\mathcal{D}_1,\ldots,\mathcal{D}_n\}$ is called $\mathcal{F}$-related if there exists a set of transformations $\mathcal{F}\triangleq\{f|f:\mathcal{X}\rightarrow\mathcal{X}\}$ such that $\forall\mathcal{D}_i,\mathcal{D}_j \exists f\in\mathcal{F}: f(\mathcal{D}_i)=\mathcal{D}_j$.
	A set of samples $\{S_1~\mathcal{D}_1,\ldots,S_n~\mathcal{D}_n\}$ is called $\mathcal{F}$-related if the underlaying tasks $\{\mathcal{D}_1,\ldots,\mathcal{D}_n\}$ are $\mathcal{F}$-related.
\end{defn}

\begin{defn}
	A hypothesis $h$ is a function $h:\mathcal{X}\rightarrow\mathcal{Y}$. A family of hypotheses is marked $\mathbb{H}$. We assume that $\mathbb{H}$ is closed under $\mathcal{F}$. Let $\mathcal{H}$ be a family of hypothesis spaces that consist of sets of hypotheses $[h]$ that are equivalent up to transformations in $\mathcal{F}$. 
	
	If $\mathcal{F}$ is a group over $\mathbb{H}$, then we consider $\mathcal{H}=\{[h]:[h]\in\mathbb{H}\}$ be the family of all equivalence classes of $\mathbb{H}$ under $\sim_\mathcal{F}$ (there exists $f\in\mathcal{F}$ such that we can transform one hypothesis to another using it).
\end{defn}

\begin{defn}
	The expected loss of a given hypothesis $h\in \mathcal{H}$ is defined as $\mathcal{L}(h, D) \triangleq E_{z\in \mathcal{D}} \ell(h, z)$. The empirical loss of a hypothesis w.\!r.\!t.\! a sample $S\in \mathcal{D}$ is defined as $\hat{\mathcal{L}}(h, S) \triangleq \frac{1}{m}\sum_{j=1}^{m}\ell(h, z_j)$.
\end{defn}

\subsection{Problem definition - without meta-learning} \label{sec:forgetting-formulation}

Let us first consider only two tasks $\mathcal{D}_s, \mathcal{D}_t$. Let us mark $Q_s$ be a distribution over the set of hypotheses learned by some process $J_s$ over $S_s\sim \mathcal{D}_s$ and a data-free prior hypothesis distribution $P$ such that $$Q_s=J_s(S_s, P).$$ We then proceed to utilize another process $J_t$ such that $$Q_{s:t}=J_t(S_t, Q_s).$$ 

\begin{defn}
	The backwards transfer of $Q_{s:t}$ on task $\mathcal{D}_s$ is defined as $$\mathrm{BWT}(Q_{s:t}, \mathcal{D}_s) \triangleq \mathbb{E}_{h\sim Q_{s:t}}\left [\mathcal{L}(h, \mathcal{D}_s)\right ].$$
	
	The negative transfer of $Q_{s:t}$ on task $\mathcal{D}_s$ is defined as $$F(Q_{s:t}, \mathcal{D}_s) \triangleq \mathrm{BWT}(Q_{s:t}, \mathcal{D}_s) - \mathbb{E}_{h\sim Q_{s}}\left [\mathcal{L}(h, \mathcal{D}_s)\right ].$$
	
	If the negative transfer $F(Q_{s:t}, \mathcal{D}_s)>0$, we will say that $Q_{s:t}$ has forgotten task $\mathcal{D}_s$.
\end{defn}

We note that this definition of forgetting is somewhat restrictive, as it requires new tasks to be learned without any loss in performance compared to previous tasks. As such, we will also consider the following weaker condition:

\begin{defn}
	A hypothesis distribution $Q_{s:t}$ is said to completely-forget task $\mathcal{D}_s$ if
	$$\mathrm{BWT}(Q_{s:t}, \mathcal{D}_s) - \mathbb{E}_{h\sim P}\left [\mathcal{L}(h, \mathcal{D}_s)\right ]>0$$
\end{defn}

This condition is weaker in the sense that for a class to completely-forget a previous task, it must perform worse on that task than a data-free prior that has not been given any sample from the task distribution. Depending on $J_s,J_t$, this event should be less likely compared to the standard forgetting setting.

\subsection{Concentration inequalities for forgetting - without meta-learning}

One method to upper bound the backward transfer term is via concentration inequalities, among which is the following specialization of the generic change-of-measure inequality attributed to \citet{donsker1975large}:
% Shui proof actually prove this without the variational lemma, but derivation from the lemma is easy

\begin{lemma} \label{lemma:concentration} \cite{shui2020beyond} \\
	Let $\pi$ and $\rho$ be two distributions on a common space $\mathcal{Z}$ such that $\rho$ is absolutely continuous w.\!r.\!t.\! $\pi$. For any $\lambda\in \mathbb{R}$ and any measurable function $f:\mathcal{Z}\rightarrow \mathbb{R}$ such that $\mathbb{E}_{z\sim \pi}\left [e^{\lambda(f(z)-\mathbb{E}_\pi f(z))} \right ]<\infty$, we have
	
	\begin{equation}
	\lambda\left (\mathbb{E}_{z\sim \rho}\left [f(z) \right ]-\mathbb{E}_{z\sim \pi}\left [f(z) \right ]\right )\leq D_{\mathrm{KL}}(\rho||\pi)+ \log\mathbb{E}_{z\sim \pi}\left [e^{\lambda(f(z)-\mathbb{E}_\pi f(z))} \right ],
	\end{equation}
	
	where $D_{\mathrm{KL}}$ is the KL-divergence and equality is achieved for $f(z)=\mathbb{E}_\pi f(z)+\frac{1}{\lambda}\log(\frac{d\rho}{d\pi})$
\end{lemma}

Using this inequality, we can choose $\rho=Q_{s:t}, \pi=Q_s, f(z)=\mathcal{L}(z,\mathcal{D}_s)$ and provide a uniform upper bound on the negative transfer if $$\mathbb{E}_{h\sim Q_s}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathbb{E}_{Q_s} \mathcal{L}(h,\mathcal{D}_s))} \right ]<\infty.$$

We have 
\begin{equation*}
F(Q_{s:t},\mathcal{D}_s)\leq \frac{1}{\lambda}D_{\mathrm{KL}}(Q_{s:t}||Q_s)+\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathbb{E}_{Q_s} \mathcal{L}(h,\mathcal{D}_s))} \right ]
\end{equation*}

This is elegant in that it guarantees that minimizing KL-divergence would also guarantee low forgetting, and ties forgetting to the variability of the previous learner $Q_s$, but it does not offer significant insight to finding hypotheses that have low forgetting in practice. We note here that a good bound on the moment term (the expectation) is critical in turning this inequality to a useful bound. For the complete-forgetting setting with bounded loss function $\ell\in [0,K]$, trivial application of this bound would yield the vacuous $$\mathrm{BWT}(Q_{s:t},\mathcal{D}_s)-\mathbb{E}_{h\sim P}\left [\mathcal{L}(h,\mathcal{D}_s)\right ]\leq  \frac{1}{\lambda}D_{\mathrm{KL}}(Q_{s:t}||P)+K.$$

Assuming we could keep task samples between tasks, we could derive a potentially more useful version of this bound by choosing $f(z)=\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_s)$ %TODO: this seems non-useful

Alternatively, we can choose $f(z)=\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_t)$ to make use of currently available data, resulting in the bound 


\begin{align*}
\begin{split}
\lambda\mathbb{E}_{h\sim Q_{s:t}}\left [\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t) \right ] - \lambda\mathbb{E}_{h\sim Q_{s}}\left [\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t) \right ] \\
\leq D_{\mathrm{KL}}(Q_{s:t}||Q_{s})+\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))}e^{-\lambda(\mathcal{L}(Q_s,\mathcal{D}_s)-\hat{\mathcal{L}}(Q_s,S_t))} \right ]
\end{split}
\end{align*}

This means that we have

\begin{align} \label{eq:forget-base}
\begin{split}
F(Q_{s:t},\mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})\\
&+\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]
\end{split}
\end{align}

This can be expended by providing an upper bound on the exponential moment term.
One method to do so is via a given distribution, $$\hat{Q}_{s:t}^{\lambda}(h)\triangleq\frac{Q_s(h)e^{-\lambda\hat{\mathcal{L}}(h,S_t)}}{\mathbb{E}_{h\sim Q_s}\left [e^{-\lambda\hat{\mathcal{L}}(h,S_t)} \right ]}.$$

Using a simple change of measures, 
$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]=\log \mathbb{E}_{h\sim Q_s}\left [e^{-\lambda\hat{\mathcal{L}}(h,S_t)} \right ] \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda}}\left [e^{\lambda\mathcal{L}(h,\mathcal{D}_s)} \right ]$$
$$=\log \mathbb{E}_{h\sim Q_s}\left [e^{-\lambda\hat{\mathcal{L}}(h,S_t)} \right ]+\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda}}\left [e^{\lambda\mathcal{L}(h,\mathcal{D}_s)} \right ]$$

This provides us with the following relative bound:
\begin{align*}
\begin{split}
F(Q_{s:t},\mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{1}{\lambda}\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda}}\left [e^{\lambda\mathcal{L}(h,\mathcal{D}_s)}\right ]+\frac{1}{\lambda}\log \mathbb{E}_{h\sim Q_s}\left [e^{-\lambda\hat{\mathcal{L}}(h,S_t)} \right ]
\end{split}
\end{align*}

If the loss is non-negative, we have the simpler

\begin{align}
\begin{split}
F(Q_{s:t},\mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{1}{\lambda}\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda}}\left [e^{\lambda\mathcal{L}(h,\mathcal{D}_s)}\right ]
\end{split}
\end{align}

Since according to Jensen's inequality, $$\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda}}\left [e^{\lambda\mathcal{L}(h,\mathcal{D}_s)}\right ]\geq \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda}}\left [\lambda\mathcal{L}(h,\mathcal{D}_s)\right ],$$
the last term can be seen as an upper bound to the backward transfer of the Gibbs learner using $Q_s$ as a prior. Note that this result holds for any non-negative loss function, even unbounded ones.

An alternative method to provide a useful bound on \eqref{eq:forget-base} is to note that
$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ] = \log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)+\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ].$$

This gives us 
$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ] = \log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}e^{\lambda(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]\triangleq \log\mathbb{E}_{Q_{s}}\left [e^{\lambda\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}e^{\lambda\Delta\hat{\mathcal{L}}(h,\mathcal{D}_t, S_t)} \right ].$$

Using the Cauchy-Shwartz inequality $$\mathbb{E}_{X}\left [f_1(X)f_2(X)\right ]^2\leq \mathbb{E}_{X}\left [f_1(X)^2\right ]\mathbb{E}_{X}\left [f_2(X)^2\right ],$$
as well as the fact that both exponent terms are non-negative, we have

$$\log\mathbb{E}_{Q_{s}}\left [e^{\lambda\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}e^{\lambda\Delta\hat{\mathcal{L}}(h,\mathcal{D}_t, S_t)} \right ]\leq \frac{1}{2}\log\mathbb{E}_{Q_{s}}\left [e^{2\lambda\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}\right ]\mathbb{E}_{Q_{s}}\left [e^{2\lambda\Delta\hat{\mathcal{L}}(h,\mathcal{D}_t, S_t)} \right ].$$

\begin{lemma}
	Let $l:Z\times H\rightarrow[0,K]$ be a measurable function. Let $\pi\in\mathcal{M}(H)$ be a distribution over $H$ that is independent w.r.t. $Z$. Let $S\in Z^m$ be an i.\! i.\! d.\! sample. With probability at least $1-\delta$ over the choice of $S$,
	
	$$\log \mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]\leq \frac{t^2K^2}{8m}+\log{1/ \delta}$$
\end{lemma}

\begin{proof} \label{lemma:hoeffding-concentration}
	Using Markov's inequality, we know that 
	$$Pr\left (\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\frac{1}{\delta}\mathbb{E}_{S\sim Z^m}\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ] \right ) \geq 1-\delta$$
	
	Applying Fubini's theorem (both distributions are independent), we can re-order the expectations
	
	$$Pr\left (\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\frac{1}{\delta}\mathbb{E}_{h\sim \pi}\mathbb{E}_{S\sim Z^m}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ] \right ) \geq 1-\delta$$
	
	Since $S$ is drawn i.\! i.\! d.\! and $l$ is bounded, we can apply Hoeffding's lemma to each example, giving us
	
	$$Pr\left (\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\frac{1}{\delta}\mathbb{E}_{h\sim \pi}\left [e^{\frac{t^2K^2}{8m}}\right ] \right ) \geq 1-\delta$$
	
	$$Pr\left (\log\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\log\frac{1}{\delta}e^{\frac{t^2K^2}{8m}} \right ) \geq 1-\delta$$
	
	and so we have 
	
	$$Pr\left (\log\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\log\frac{1}{\delta}+\frac{t^2K^2}{8m} \right ) \geq 1-\delta$$
	
\end{proof}

If $\ell\in [0,K]$, we can use Lemma \ref{lemma:hoeffding-concentration} and get with probability at least $1-\delta/2$ over the choice of $S_t$


$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]\leq \frac{1}{2}\log\mathbb{E}_{Q_{s}}\left [e^{2\lambda\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}\right ]+\frac{\lambda^2K^2}{4m_t}+\log(2/\delta)$$

Finally, we get

\begin{align}
\begin{split}
F(Q_{s:t},\mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{1}{2\lambda}\log \mathbb{E}_{h\sim Q_{s}}\left [e^{2\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]+\frac{\lambda K^2}{4m_t}+\frac{1}{2\lambda}\log(2/\delta)
\end{split}
\end{align}

The term $\frac{1}{2\lambda}\log \mathbb{E}_{h\sim Q_{s}}\left [e^{2\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)}\right ]$ measures domain disagreement over $Q_s$, and can be hard to understand in general. A simple setting where $Q_s(h)=\frac{P(h)e^{-2\lambda\hat{\mathcal{L}}(h,S_s)}}{\mathbb{E}_{h\sim P}\left [e^{-2\lambda\hat{\mathcal{L}}(h,S_s)} \right ]}$ yields 

$$\frac{1}{2\lambda}\log \mathbb{E}_{h\sim Q_{s}}\left [e^{2\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]=\frac{1}{2\lambda}\log \int \frac{1}{\mathbb{E}_{h\sim P}\left [e^{-2\lambda\hat{\mathcal{L}}(h,S_s)} \right ]}P(h)e^{2\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_s)-\mathcal{L}(h,\mathcal{D}_t))}dh$$

$$=\frac{1}{2\lambda}\log \int \frac{\mathbb{E}_{h\sim P} e^{-2\lambda\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda\hat{\mathcal{L}}(h,S_s)}  }\frac{P(h)e^{-2\lambda\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda\mathcal{L}(h,\mathcal{D}_t)}}e^{2\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_s))}dh$$

$$=\frac{1}{2\lambda}\log\frac{\mathbb{E}_{h\sim P} e^{-2\lambda\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda\hat{\mathcal{L}}(h,S_s)}  }+\frac{1}{2\lambda}\log\mathbb{E}_{h\sim Q^{*}_t}\left [e^{2\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_s))}\right ]$$

Using Lemma \ref{lemma:hoeffding-concentration} again gives us with probability at least $1-\delta/2$ over the choice of $S_s$

$$\frac{1}{2\lambda}\log \mathbb{E}_{h\sim Q^{*}_t}\left [e^{2\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ] \leq \frac{\lambda K^2}{4m_s}+\frac{1}{2\lambda}\log(2/\delta)$$

Putting it all together with a union bound, if $Q_s(h)=\frac{P(h)e^{-2\lambda\hat{\mathcal{L}}(h,S_s)}}{\mathbb{E}_{h\sim P}\left [e^{-2\lambda\hat{\mathcal{L}}(h,S_s)} \right ]}$ and the loss is bounded $\ell\in[0,K]$, we have with probability at least $1-\delta/2$ over the choice of $S_s,S_t$:

\begin{align}
\begin{split}
F(Q_{s:t},\mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda K^2}{4m_s}+\frac{\lambda K^2}{4m_t}+\frac{1}{\lambda}\log(2/\delta)+\frac{1}{2\lambda}\log\frac{\mathbb{E}_{h\sim P} e^{-2\lambda\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda\hat{\mathcal{L}}(h,S_s)}  }
\end{split}
\end{align}

% we can also use log-sum inequality, but it's non-useful

\begin{align*}
\begin{split}
F(Q_{s:t},\mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda K^2}{4m_s}+\frac{\lambda K^2}{4m_t}+\frac{1}{\lambda}\log(2/\delta)+\frac{1}{2\lambda}\log\mathbb{E}_{h\sim P} e^{-2\lambda\mathcal{L}(h,\mathcal{D}_t)}-\frac{1}{2\lambda}\log\mathbb{E}_{h\sim P} e^{-2\lambda\hat{\mathcal{L}}(h,S_s)}   \\
& \leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda K^2}{4m_s}+\frac{\lambda K^2}{4m_t}+\frac{1}{\lambda}\log(2/\delta)+0+\frac{1}{2\lambda}\mathbb{E}_{h\sim P} 2\lambda\hat{\mathcal{L}}(h,S_s) \\
& \leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda K^2}{4m_s}+\frac{\lambda K^2}{4m_t}+\frac{1}{\lambda}\log(2/\delta)+ \hat{\mathcal{L}}(P, S_s)
\end{split}
\end{align*}

\subsection{Similar bound for forward transfer}

We now apply Lemma \ref{lemma:concentration} to the problem of forward transfer, meaning $f(z)=\mathcal{L}(z,\mathcal{D}_t)-\hat{\mathcal{L}}(z,S_t)$.

Doing so gives us the following inequality:

\begin{align}
\begin{split}
\mathcal{L}(Q_{s:t},\mathcal{D}_t) - \mathcal{L}(Q_{s},\mathcal{D}_t) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_t) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})\\
&+\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]
\end{split}
\end{align}

Conveniently enough, $Q_s$ is not dependent on $S_t$, so if the loss is bounded we can apply Lemma \ref{lemma:hoeffding-concentration} and get with probability at least $1-\delta$ over the choice of $S_t$

\begin{align}
\begin{split}
\mathcal{L}(Q_{s:t},\mathcal{D}_t) - \mathcal{L}(Q_{s},\mathcal{D}_t) \leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_t) + \frac{1}{\lambda} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})+\frac{\lambda K^2}{8m_t}+\frac{1}{\lambda}\log(1/\delta)
\end{split}
\end{align}

This implies that given enough data, forward transfer and low forgetting can both be solved by the same measures...

%\subsection{Useful inequality to use for regret later}

%Another option that is available for us if the two task domains $\mathcal{D}_s, \mathcal{D}_t$ have the same support is to apply a joint data-hypothesis version of \ref{lemma:concentration},
%choosing $\rho=(Q_{s:t},\mathcal{D}_s), \pi=(Q_s, \mathcal{D}_s), f(h,z_i)=\ell(h,z_i)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j)$
%TODO: math badness here, you can't pick and choose different functions
% Doing f=L-\hat{L}(S_t)-\hat{L}(S_s) doesn't help since the exponetial moment cannot be bounded

%\begin{align*}
%\begin{split}
%\lambda\mathbb{E}_{(h,z)\sim (Q_{s:t},\mathcal{D}_t)}\left [\ell(h,z)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j) \right ] - \lambda\mathbb{E}_{(h,z)\sim (Q_{s},\mathcal{D}_s)}\left [\ell(h,z)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j) \right ] \\
%\leq D_{\mathrm{KL}}((Q_{s:t},\mathcal{D}_t)||(Q_{s},\mathcal{D}_s))+\log\mathbb{E}_{(h,z)\sim (Q_{s},\mathcal{D}_s)}\left [e^{\lambda(\ell(h,z)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j))} \right ]
%\end{split}
%\end{align*}

%TODO: this is more generalization / regret

\subsection{Oracle inequalities for forgetting and transfer}

Starting from Lemma \ref{lemma:concentration}, we know that 

$$\mathbb{E}_{z\sim \rho}\left [f(z) \right ]\leq \mathbb{E}_{z\sim \pi}\left [f(z) \right ]+ \frac{1}{\lambda}D_{\mathrm{KL}}(\rho||\pi)+ \frac{1}{\lambda}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda(f(z)-\mathbb{E}_\pi f(z))} \right ]$$

In particular, for $\hat{\rho}_\lambda(z)\propto \pi(z) e^{-\lambda f(z) }$, this is an equality (from \citeauthor{donsker1975large}'s [\citeyear{donsker1975large}] variational lemma).
From this, we know that

\begin{equation}
\mathbb{E}_{z\sim \hat{\rho}_\lambda}\left [f(z) \right ]= \mathbb{E}_{z\sim \pi}\left [f(z) \right ]+ \frac{1}{\lambda}D_{\mathrm{KL}}(\hat{\rho}_\lambda||\pi)+ \frac{1}{\lambda}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda(f(z)-\mathbb{E}_\pi f(z))} \right ]
\end{equation}

If we pick $f(z)=\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_t)$ as before, we get

$$\mathbb{E}_{z\sim \hat{\rho}_\lambda}\left [\mathcal{L}(z,\mathcal{D}_s) \right ]= \mathbb{E}_{z\sim \pi}\left [f(z) \right ]+\mathbb{E}_{z\sim \hat{\rho}_\lambda}\left [\hat{\mathcal{L}}(z,S_t) \right ]+ \frac{1}{\lambda}D_{\mathrm{KL}}(\hat{\rho}_\lambda||\pi)+ \frac{1}{\lambda}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda(f(z)-\mathbb{E}_\pi f(z))} \right ]$$

And as such,
$$F( \hat{\rho}_\lambda,\mathcal{D}_s)\leq \inf_{\rho}\left \{ \hat{\mathcal{L}}(\rho,S_t) + \frac{1}{\lambda}D_{\mathrm{KL}}(\rho||\pi)  \right \}-\mathcal{L}(\pi,D_s)+\frac{1}{\lambda}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda(\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_t))} \right ],$$

or using our previous terminology with $\hat{Q}^{\lambda}_{s:t}(h)\propto Q_s(h)e^{-\lambda\hat{\mathcal{L}}(h,S_t)}$, 

$$F( \hat{Q}^{\lambda}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \hat{\mathcal{L}}(Q_{s:t},S_t) + \frac{1}{\lambda}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}-\mathcal{L}(Q_s,D_s)+\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

If we take an expectation on $S_t$, we get

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}F( \hat{Q}^{\lambda}_{s:t},\mathcal{D}_s)\leq \mathbb{E}_{S_t\sim \mathcal{D}_t}\inf_{Q_{s:t}}\left \{ \hat{\mathcal{L}}(Q_{s:t},S_t) + \frac{1}{\lambda}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}-\mathcal{L}(Q_s,D_s)+\frac{1}{\lambda}\mathbb{E}_{S_t\sim \mathcal{D}_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

This gives us the following oracle inequality (in expectation):

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}F( \hat{Q}^{\lambda}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}-\mathcal{L}(Q_s,D_s)+\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

$$\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]=\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}e^{\lambda(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

$$=\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\mathbb{E}_{S_t\sim \mathcal{D}_t}e^{\lambda(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

Using Hoeffding's lemma, we get

$$\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ] \leq \frac{\lambda K^2}{8m_t}+\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))} \right ]$$

Giving us

\begin{equation}
\mathbb{E}_{S_t\sim \mathcal{D}_t}F( \hat{Q}^{\lambda}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}-\mathcal{L}(Q_s,D_s)+\frac{\lambda K^2}{8m_t}+\frac{1}{\lambda}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))} \right ]
\end{equation}

Similarly, for forward transfer we would have

\begin{equation}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}(\hat{Q}^{\lambda}_{s:t},\mathcal{D}_t)-\mathcal{L}(Q_{s},\mathcal{D}_t)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}-\mathcal{L}(Q_s,D_t)+\frac{\lambda K^2}{8m_t}
\end{equation}

%TODO: moment stuff, we alreadt have the expectation on S_t

\subsection{Meta-learning formulation}

Let $\mathcal{P}\in\mathcal{M}(\mathcal{M}(\mathcal{H}))$ be a data-free distribution over hypothesis distributions, which we will refer to as the hyper-prior. We define 
$$\mathcal{Q}_s=\mathcal{J}_s(S_s,\mathcal{P})$$ and similarly for $\mathcal{Q}_{s:t}$, where the only difference is that the inputs are meta-distributions instead of normal distributions over hypotheses. For example, we could have $\mathcal{J}_s$ sample from the hyper-prior $\mathcal{P}$ and activate $J_s$ on the sampled distribution, thus giving us a ne meta-distribution that depends on $S_s$. While bounds from the previous section can be applied to this new setting, sampling from the hyper-posterior is impractical, and thus measuring loss terms is not reasonable for this general setting, so we must make a stronger assumption on the construction process.

Assume 


\section{Specific problem formulation: connecting regret and transfer with domain adaptation and PAC-Bayes}

\subsection{Problem definition} \label{sec:regret-formulation}

Considering the same setting as Section \ref{sec:forgetting-formulation}, we consider the metric of regret:

\begin{defn}
	The regret on $n$ tasks is defined as 
	$$REGRET_n = \sum_{t=1}^{n}\mathcal{L}(Q_{1:t}, \mathcal{D}_t)-\min_{Q} \sum_{t=1}^{T}\mathcal{L}(Q, \mathcal{D}_t),$$
	where $Q_{1:t}$ is the distribution obtained by applying the series of processes $J_1, \ldots, J_t$ on $P, \{S_1,\ldots,S_t\}$, such that $Q_1=J_1(S_1,P)$ and $Q_1:i=J_i(S_i, Q_{i-1})$ for $i>1$.
\end{defn}

Again, we begin with the two-task setting ($n=2$), and notice that

\begin{equation}
\mathcal{L}(Q_1, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2)=
\mathcal{L}(Q_1, \mathcal{D}_1)-\mathcal{L}(Q_1, \mathcal{D}_2)+\mathcal{L}(Q_1, \mathcal{D}_2)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2)-\mathcal{L}(Q_{1:2}, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_1)
\end{equation}

Marking $\mathcal{L}(Q_1, \mathcal{D}_2)\triangleq FWT(Q_1, \mathcal{D}_2)$ as the forward transfer from task $1$ to task $2$, we have
\begin{equation}
\mathcal{L}(Q_1, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2)=
\underbrace{\mathcal{L}(Q_1, \mathcal{D}_1)-\mathcal{L}(Q_1, \mathcal{D}_2)}_{\text{transfer gap from source}} + FWT(Q_1, \mathcal{D}_2) + \underbrace{\mathcal{L}(Q_{1:2}, \mathcal{D}_2)-\mathcal{L}(Q_{1:2}, \mathcal{D}_1)}_{\text{transfer gap from target}}+BWT(Q_{1:2}, \mathcal{D}_1)
\end{equation}

We can use known results from domain adaptation methods to provide an upper bound to the transfer gap terms. As a simple (and loose) example, we can consider the following theorem from \citet{shui2020beyond}:

\begin{lemma}
	Assume that $\ell$ is bounded in $[0, K]$. For all $h\in \mathcal{H}$ we have
	$$\mathcal{L}(h, \mathcal{D}_1)-\mathcal{L}(h, \mathcal{D}_2)\leq \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_1||\mathcal{D}_2)},$$ 
	where $D_{JS}$ is the Jensen-Shannon divergence between the joint data distributions.
\end{lemma}

Due to the symmetry of the Jensen-Shannon divergence, this bound applies for $\mathcal{L}(h, \mathcal{D}_2)-\mathcal{L}(h, \mathcal{D}_1)$ as well, and so we have

\begin{equation}
\mathcal{L}(Q_1, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2) \leq 
K\sqrt{2 D_{JS}(\mathcal{D}_1||\mathcal{D}_2)}+FWT(Q_1, \mathcal{D}_2)+ BWT(Q_{1:2}, \mathcal{D}_1).
\end{equation}

We note that while this bound is somewhat naive in that it does not take the posterior distributions into account at all, it does give us some insight into the relationship between transfer and regret - minimizing forgetting (and lowering backwards transfer) gives us a guaranteed upper bound on the overall regret. 

We can easily extend this result for $n$ tasks: (TODO later - prove this) %TODO
\begin{equation}
\begin{split}
 \sum_{t=1}^{n}\mathcal{L}(Q_{1:t}, \mathcal{D}_t) \leq &  FWT(Q_1, \mathcal{D}_2) + \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_{1}||\mathcal{D}_{2})} \\ &+ \sum_{t=2}^{n}\left ( BWT(Q_{1:t}, \mathcal{D}_{t-1})+ \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_{t}||\mathcal{D}_{t-1})}\right ) \\&
 \end{split}
\end{equation}

A minor potential improvement over this would be to use a Pac-Bayes bound for the first term $Q_1$, giving us with probability at least $1-\delta$ over the choice of $S_1$, uniformly for all $Q_1$,
\begin{equation}
\begin{split}
\sum_{t=1}^{n}\mathcal{L}(Q_{1:t}, \mathcal{D}_t) \leq &  \hat{\mathcal{L}}(Q_1, S_1)+\frac{1}{\lambda}D_{KL}(Q_1||P)+C(\lambda,\delta,P,S_1, K) \\ &+ \sum_{t=2}^{n}\left ( BWT(Q_{1:t}, \mathcal{D}_{t-1})+ \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_{t}||\mathcal{D}_{t-1})}\right ) \\&
\end{split}
\end{equation}

This implies that if the Jensen-Shannon divergence between subsequent tasks is smaller on average than $\sqrt{2}/K$, minimizing backwards transfer can potentially lead to a sub-linear regret bound.

\clearpage
\bibliographystyle{plainnat}
\bibliography{library}

\end{document}