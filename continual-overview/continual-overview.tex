\documentclass[letterpaper]{article}

% AAAI-style 2-per-page format, without the annoying bits
\setlength\topmargin{-0.25in} \setlength\oddsidemargin{-0.25in}
\setlength\textheight{9.0in} \setlength\textwidth{7.0in}
\setlength\columnsep{0.375in} \newlength\titlebox \setlength\titlebox{2.25in}
\setlength\headheight{0pt}  \setlength\headsep{0pt}
\flushbottom \sloppy

\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{natbib}

\usepackage{times} 
\usepackage{helvet}  
\usepackage{courier}  
\usepackage{url}  
\usepackage{graphicx} 

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}



\usepackage{multirow}
\usepackage{ctable}
\usepackage{color}
\usepackage{natbib}
\usepackage[normalem]{ulem}


\usepackage{romannum}

%\usepackage[style=authoryear]{biblatex}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black, % color for table of contents
	citecolor=black, % color for citations
	urlcolor=blue, % color for hyperlinks
	bookmarks=true,
}
\urlstyle{same}




\raggedbottom %nicer enumerate
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{hypothesis}{Hypothesis}[section]
\newtheorem{assumption}{Assumption}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Summary of continual and lifelong learning frontiers}
\begin{document}
	
	\pagenumbering{arabic}
	\maketitle
	
	
\section{Introduction}
	

\section{Literature review}

Relevant theory papers:

\begin{itemize}
	\item classical transfer learning and bounds
	\item PAC-Bayes and domain adaptation \citep{germain2020pac}
	\item PAC-Bayesian Domain Adaptation Bounds for Multiclass Learners \citep{sicilia2022pac}
	\item Beyond $\mathcal{H}$-Divergence: Domain Adaptation Theory With Jensen-Shannon Divergence \citep{shui2020beyond}
	\item Gap Minimization for Knowledge Sharing and Transfer \citep{wang2022gap}
	\item A Theory for Knowledge Transfer in Continual Learning \citep{benavides2022theory}
	\item Online PAC-Bayes Learning \citep{haddouche2022online}
\end{itemize}

Papers specific to linear regression/NTK and transfer/meta/continual:
\begin{enumerate}
	\item How catastrophic can catastrophic forgetting be in linear regression? \citep{evron2022catastrophic}
	\item Generalisation guarantees for continual learning with orthogonal gradient descent \citep{bennani2020generalisation}
	\item  Learning Curves for Continual Learning in Neural Networks: Self-Knowledge Transfer and Forgetting \citep{karakida2021learning}
	\item Curriculum learning by transfer learning: Theory and experiments with deep networks \citep{weinshall2018curriculum}
\end{enumerate}


\subsection{Problem variations}

Task/data order:
\begin{itemize}
	\item $i.i.d.$
	\item Predictable
	\item Curriculum
	\item Adversarial
\end{itemize}

Settings: $P_{x,y}$ identical, $P_x$ identical and $P_{y|x}$ differs, complete difference.

Task boundaries: discrete or continuous, known or unknown.

Usual metrics:
Regret $$REGRET_T = \sum_{t=1}^{T}\mathcal{L}_t(\theta_t)-\min_\theta \sum_{t=1}^{T}\mathcal{L}_t(\theta)$$
Cannot be calculated in practice, want sub-linear regret (in $t$).

Backwards transfer: new task improves old tasks compared to learning from scratch, Forward transfer: old tasks improve compared to learning from scratch.

\subsection{Common known approaches}
\begin{itemize}
	\item Follow the leader - store all data seen and train on it, fine tune to task
	\item Gradient step on new data (no memory, experiences forgetting)
	\item Store little data per task, make sure we don't unlearn (minimize loss subject to no empirical decrease on all previous tasks)
	\item Follow the meta-leader - store all data seen and meta-train on it
\end{itemize}

\section{Specific problem formulation: PAC-Bayes forgetting}
	
\subsection{Notation and preliminaries}

\begin{defn}
	A task environment $\tau$ is a distribution over tasks $\mathcal{D}\sim \tau$. A task $\mathcal{D}$ is a probability distribution over $\mathcal{X}\times \mathcal{Y}$.
\end{defn}

\begin{defn}
	A set of $n$ tasks $\{\mathcal{D}_1,\ldots,\mathcal{D}_n\}$ is called $\mathcal{F}$-related if there exists a set of transformations $\mathcal{F}\triangleq\{f|f:\mathcal{X}\rightarrow\mathcal{X}\}$ such that $\forall\mathcal{D}_i,\mathcal{D}_j \exists f\in\mathcal{F}: f(\mathcal{D}_i)=\mathcal{D}_j$.
	A set of samples $\{S_1~\mathcal{D}_1,\ldots,S_n~\mathcal{D}_n\}$ is called $\mathcal{F}$-related if the underlaying tasks $\{\mathcal{D}_1,\ldots,\mathcal{D}_n\}$ are $\mathcal{F}$-related.
\end{defn}

\begin{defn}
	A hypothesis $h$ is a function $h:\mathcal{X}\rightarrow\mathcal{Y}$. A family of hypotheses is marked $\mathbb{H}$. We assume that $\mathbb{H}$ is closed under $\mathcal{F}$. Let $\mathcal{H}$ be a family of hypothesis spaces that consist of sets of hypotheses $[h]$ that are equivalent up to transformations in $\mathcal{F}$. 
	
	If $\mathcal{F}$ is a group over $\mathbb{H}$, then we consider $\mathcal{H}=\{[h]:[h]\in\mathbb{H}\}$ be the family of all equivalence classes of $\mathbb{H}$ under $\sim_\mathcal{F}$ (there exists $f\in\mathcal{F}$ such that we can transform one hypothesis to another using it).
\end{defn}

\begin{defn}
	The expected loss of a given hypothesis $h\in \mathcal{H}$ is defined as $\mathcal{L}(h, D) \triangleq E_{z\in \mathcal{D}} \ell(h, z)$. The empirical loss of a hypothesis w.\!r.\!t.\! a sample $S\in \mathcal{D}$ is defined as $\hat{\mathcal{L}}(h, S) \triangleq \frac{1}{m}\sum_{j=1}^{m}\ell(h, z_j)$.
\end{defn}

\subsection{Problem definition - without meta-learning} \label{sec:forgetting-formulation}

Let us first consider only two tasks $\mathcal{D}_s, \mathcal{D}_t$. Let us mark $Q_s$ be a distribution over the set of hypotheses learned by some process $J_s$ over $S_s\sim \mathcal{D}_s$ and a data-free prior hypothesis distribution $P$ such that $$Q_s=J_s(S_s, P).$$ We then proceed to utilize another process $J_t$ such that $$Q_{s:t}=J_t(S_t, Q_s).$$ 

\begin{defn}
	The backwards transfer of $Q_{s:t}$ on task $\mathcal{D}_s$ is defined as $$\mathrm{BWT}(Q_{s:t}, \mathcal{D}_s) \triangleq \mathbb{E}_{h\sim Q_{s:t}}\left [\mathcal{L}(h, \mathcal{D}_s)\right ].$$
	
	The negative transfer of $Q_{s:t}$ on task $\mathcal{D}_s$ is defined as $$F(Q_{s:t}, \mathcal{D}_s) \triangleq \mathrm{BWT}(Q_{s:t}, \mathcal{D}_s) - \mathbb{E}_{h\sim Q_{s}}\left [\mathcal{L}(h, \mathcal{D}_s)\right ].$$
	
	If the negative transfer $F(Q_{s:t}, \mathcal{D}_s)>0$, we will say that $Q_{s:t}$ has forgotten task $\mathcal{D}_s$.
\end{defn}

We note that this definition of forgetting is somewhat restrictive, as it requires new tasks to be learned without any loss in performance compared to previous tasks. As such, we will also consider the following weaker condition:

\begin{defn}
	A hypothesis distribution $Q_{s:t}$ is said to completely-forget task $\mathcal{D}_s$ if
	$$\mathrm{BWT}(Q_{s:t}, \mathcal{D}_s) - \mathbb{E}_{h\sim P}\left [\mathcal{L}(h, \mathcal{D}_s)\right ]>0$$
\end{defn}

This condition is weaker in the sense that for a class to completely-forget a previous task, it must perform worse on that task than a data-free prior that has not been given any sample from the task distribution. Depending on $J_s,J_t$, this event should be less likely compared to the standard forgetting setting.

\subsection{Concentration inequalities for forgetting - without meta-learning}

One method to upper bound the backward transfer term is via concentration inequalities, among which is the following specialization of the generic change-of-measure inequality attributed to \citet{donsker1975large}:
% Shui proof actually prove this without the variational lemma, but derivation from the lemma is easy

\begin{lemma} \label{lemma:concentration} \cite{shui2020beyond} \\
	Let $\pi$ and $\rho$ be two distributions on a common space $\mathcal{Z}$ such that $\rho$ is absolutely continuous w.\!r.\!t.\! $\pi$. For any $\lambda_t\in \mathbb{R}$ and any measurable function $f:\mathcal{Z}\rightarrow \mathbb{R}$ such that $\mathbb{E}_{z\sim \pi}\left [e^{\lambda_t(f(z)-\mathbb{E}_\pi f(z))} \right ]<\infty$, we have
	
	\begin{equation}
	\lambda_t\left (\mathbb{E}_{z\sim \rho}\left [f(z) \right ]-\mathbb{E}_{z\sim \pi}\left [f(z) \right ]\right )\leq D_{\mathrm{KL}}(\rho||\pi)+ \log\mathbb{E}_{z\sim \pi}\left [e^{\lambda_t(f(z)-\mathbb{E}_\pi f(z))} \right ],
	\end{equation}
	
	where $D_{\mathrm{KL}}$ is the KL-divergence and equality is achieved for $f(z)=\mathbb{E}_\pi f(z)+\frac{1}{\lambda_t}\log(\frac{d\rho}{d\pi})$
\end{lemma}

Using this inequality, we can choose $\rho=Q_{s:t}, \pi=Q_s, f(z)=\mathcal{L}(z,\mathcal{D}_s)$ and provide a uniform upper bound on the negative transfer if $$\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathbb{E}_{Q_s} \mathcal{L}(h,\mathcal{D}_s))} \right ]<\infty.$$

We have 
\begin{equation*}
F(Q_{s:t},\mathcal{D}_s)\leq \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q_s)+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathbb{E}_{Q_s} \mathcal{L}(h,\mathcal{D}_s))} \right ]
\end{equation*}

This is elegant in that it guarantees that minimizing KL-divergence would also guarantee low forgetting, and ties forgetting to the variability of the previous learner $Q_s$, but it does not offer significant insight to finding hypotheses that have low forgetting in practice. We note here that a good bound on the moment term (the expectation) is critical in turning this inequality to a useful bound. For the complete-forgetting setting with bounded loss function $\ell\in [0,K]$, trivial application of this bound would yield the vacuous $$\mathrm{BWT}(Q_{s:t},\mathcal{D}_s)-\mathbb{E}_{h\sim P}\left [\mathcal{L}(h,\mathcal{D}_s)\right ]\leq  \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||P)+K.$$

Assuming we could keep task samples between tasks, we could derive a potentially more useful version of this bound by choosing $f(z)=\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_s)$ %TODO: this seems non-useful

Alternatively, we can choose $f(z)=\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_t)$ to make use of currently available data, resulting in the bound 


\begin{align*}
\begin{split}
\lambda_t\mathbb{E}_{h\sim Q_{s:t}}\left [\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t) \right ] - \lambda_t\mathbb{E}_{h\sim Q_{s}}\left [\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t) \right ] \\
\leq D_{\mathrm{KL}}(Q_{s:t}||Q_{s})+\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))}e^{-\lambda_t(\mathcal{L}(Q_s,\mathcal{D}_s)-\hat{\mathcal{L}}(Q_s,S_t))} \right ]
\end{split}
\end{align*}

This means that we have

\begin{align} \label{eq:forget-base}
\begin{split}
F(Q_{s:t},\mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_s) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})\\
&+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]
\end{split}
\end{align}

or in simpler terms 

\begin{align} \label{eq:forget-base2}
\begin{split}
\mathcal{L}(Q_{s:t}, \mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})\\
&+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]
\end{split}
\end{align}

This can be expended by providing an upper bound on the exponential moment term.
One method to do so is via a given distribution, $$\hat{Q}_{s:t}^{\lambda_t}(h)\triangleq\frac{Q_s(h)e^{-\lambda_t\hat{\mathcal{L}}(h,S_t)}}{\mathbb{E}_{h\sim Q_s}\left [e^{-\lambda_t\hat{\mathcal{L}}(h,S_t)} \right ]}.$$

Using a simple change of measures, 
$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]=\log \mathbb{E}_{h\sim Q_s}\left [e^{-\lambda_t\hat{\mathcal{L}}(h,S_t)} \right ] \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda_t}}\left [e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)} \right ]$$
$$=\log \mathbb{E}_{h\sim Q_s}\left [e^{-\lambda_t\hat{\mathcal{L}}(h,S_t)} \right ]+\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda_t}}\left [e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)} \right ]$$

This provides us with the following relative bound:
\begin{align*}
\begin{split}
\mathcal{L}(Q_{s:t}, \mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{1}{\lambda_t}\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda_t}}\left [e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}\right ]+\frac{1}{\lambda_t}\log \mathbb{E}_{h\sim Q_s}\left [e^{-\lambda_t\hat{\mathcal{L}}(h,S_t)} \right ]
\end{split}
\end{align*}

If the loss is non-negative, we have the simpler

\begin{align}
\begin{split}
\mathcal{L}(Q_{s:t}, \mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{1}{\lambda_t}\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda_t}}\left [e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}\right ]
\end{split}
\end{align}

Since according to Jensen's inequality, $$\log \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda_t}}\left [e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}\right ]\geq \mathbb{E}_{h\sim \hat{Q}_{s:t}^{\lambda_t}}\left [\lambda_t\mathcal{L}(h,\mathcal{D}_s)\right ],$$
the last term can be seen as an upper bound to the backward transfer of the Gibbs learner using $Q_s$ as a prior. Note that this result holds for any non-negative loss function, even unbounded ones.

An alternative method to provide a useful bound on \eqref{eq:forget-base} is to note that
$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ] = \log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)+\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ].$$

This gives us 
$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ] = \log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]\triangleq \log\mathbb{E}_{Q_{s}}\left [e^{\lambda_t\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}e^{\lambda_t\Delta\hat{\mathcal{L}}(h,\mathcal{D}_t, S_t)} \right ].$$

Using the Cauchy-Shwartz inequality $$\mathbb{E}_{X}\left [f_1(X)f_2(X)\right ]^2\leq \mathbb{E}_{X}\left [f_1(X)^2\right ]\mathbb{E}_{X}\left [f_2(X)^2\right ],$$
as well as the fact that both exponent terms are non-negative, we have

$$\log\mathbb{E}_{Q_{s}}\left [e^{\lambda_t\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}e^{\lambda_t\Delta\hat{\mathcal{L}}(h,\mathcal{D}_t, S_t)} \right ]\leq \frac{1}{2}\log\mathbb{E}_{Q_{s}}\left [e^{2\lambda_t\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}\right ]\mathbb{E}_{Q_{s}}\left [e^{2\lambda_t\Delta\hat{\mathcal{L}}(h,\mathcal{D}_t, S_t)} \right ].$$

\begin{lemma}
	Let $l:Z\times H\rightarrow[0,K]$ be a measurable function. Let $\pi\in\mathcal{M}(H)$ be a distribution over $H$ that is independent w.r.t. $Z$. Let $S\in Z^m$ be an i.\! i.\! d.\! sample. With probability at least $1-\delta$ over the choice of $S$,
	
	$$\log \mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]\leq \frac{t^2K^2}{8m}+\log{1/ \delta}$$
\end{lemma}

\begin{proof} \label{lemma:hoeffding-concentration}
	Using Markov's inequality, we know that 
	$$Pr\left (\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\frac{1}{\delta}\mathbb{E}_{S\sim Z^m}\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ] \right ) \geq 1-\delta$$
	
	Applying Fubini's theorem (both distributions are independent), we can re-order the expectations
	
	$$Pr\left (\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\frac{1}{\delta}\mathbb{E}_{h\sim \pi}\mathbb{E}_{S\sim Z^m}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ] \right ) \geq 1-\delta$$
	
	Since $S$ is drawn i.\! i.\! d.\! and $l$ is bounded, we can apply Hoeffding's lemma to each example, giving us
	
	$$Pr\left (\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\frac{1}{\delta}\mathbb{E}_{h\sim \pi}\left [e^{\frac{t^2K^2}{8m}}\right ] \right ) \geq 1-\delta$$
	
	$$Pr\left (\log\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\log\frac{1}{\delta}e^{\frac{t^2K^2}{8m}} \right ) \geq 1-\delta$$
	
	and so we have 
	
	$$Pr\left (\log\mathbb{E}_{h\sim \pi}\left [e^{t(\frac{1}{m}\sum_i l(z_i,h)-\mathbb{E}_{z}l(z,h))}\right ]<\log\frac{1}{\delta}+\frac{t^2K^2}{8m} \right ) \geq 1-\delta$$
	
\end{proof}

If $\ell\in [0,K]$, we can use Lemma \ref{lemma:hoeffding-concentration} and get with probability at least $1-\delta/2$ over the choice of $S_t$


$$\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]\leq \frac{1}{2}\log\mathbb{E}_{Q_{s}}\left [e^{2\lambda_t\Delta\mathcal{L}(h,\mathcal{D}_s, \mathcal{D}_t)}\right ]+\frac{\lambda_t^2K^2}{4m_t}+\log(2/\delta)$$

Finally, we get

\begin{align}
\begin{split}
\mathcal{L}(Q_{s:t}, \mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{1}{2\lambda_t}\log \mathbb{E}_{h\sim Q_{s}}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]+\frac{\lambda_t K^2}{4m_t}+\frac{1}{2\lambda_t}\log(2/\delta)
\end{split}
\end{align}

The term $\frac{1}{2\lambda_t}\log \mathbb{E}_{h\sim Q_{s}}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)}\right ]$ measures domain disagreement over $Q_s$, and can be hard to understand in general. A simple setting where $Q_s(h)=\frac{P(h)e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)}}{\mathbb{E}_{h\sim P}\left [e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)} \right ]}$ yields 

$$\frac{1}{2\lambda_t}\log \mathbb{E}_{h\sim Q_{s}}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]=\frac{1}{2\lambda_t}\log \int \frac{1}{\mathbb{E}_{h\sim P}\left [e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)} \right ]}P(h)e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_s)-\mathcal{L}(h,\mathcal{D}_t))}dh$$

$$=\frac{1}{2\lambda_t}\log \int \frac{\mathbb{E}_{h\sim P} e^{-2\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)}  }\frac{P(h)e^{-2\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_s))}dh$$

$$=\frac{1}{2\lambda_t}\log\frac{\mathbb{E}_{h\sim P} e^{-2\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)}  }+\frac{1}{2\lambda_t}\log\mathbb{E}_{h\sim Q^{*}_t}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_s))}\right ]$$

Using Lemma \ref{lemma:hoeffding-concentration} again gives us with probability at least $1-\delta/2$ over the choice of $S_s$

$$\frac{1}{2\lambda_t}\log \mathbb{E}_{h\sim Q^{*}_t}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ] \leq \frac{\lambda_t K^2}{4m_s}+\frac{1}{2\lambda_t}\log(2/\delta)$$

Putting it all together with a union bound, if $Q_s(h)=\frac{P(h)e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)}}{\mathbb{E}_{h\sim P}\left [e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)} \right ]}$ and the loss is bounded $\ell\in[0,K]$, we have with probability at least $1-\delta/2$ over the choice of $S_s,S_t$:

\begin{align}
\begin{split}
\mathcal{L}(Q_{s:t}, \mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda_t K^2}{4m_s}+\frac{\lambda_t K^2}{4m_t}+\frac{1}{\lambda_t}\log(2/\delta)+\frac{1}{2\lambda_t}\log\frac{\mathbb{E}_{h\sim P} e^{-2\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}{\mathbb{E}_{h\sim P} e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)}  }
\end{split}
\end{align}

% we can also use log-sum inequality, but it's non-useful

\begin{align*}
\begin{split}
\mathcal{L}(Q_{s:t}, \mathcal{D}_s) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t)+ \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda_t K^2}{4m_s}+\frac{\lambda_t K^2}{4m_t}+\frac{1}{\lambda_t}\log(2/\delta)+\frac{1}{2\lambda_t}\log\mathbb{E}_{h\sim P} e^{-2\lambda_t\mathcal{L}(h,\mathcal{D}_t)}-\frac{1}{2\lambda_t}\log\mathbb{E}_{h\sim P} e^{-2\lambda_t\hat{\mathcal{L}}(h,S_s)}   \\
& \leq \hat{\mathcal{L}}(Q_{s:t}, S_t)+ \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda_t K^2}{4m_s}+\frac{\lambda_t K^2}{4m_t}+\frac{1}{\lambda_t}\log(2/\delta)+0+\frac{1}{2\lambda_t}\mathbb{E}_{h\sim P} 2\lambda_t\hat{\mathcal{L}}(h,S_s) \\
& \leq \hat{\mathcal{L}}(Q_{s:t}, S_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})
+\frac{\lambda_t K^2}{4m_s}+\frac{\lambda_t K^2}{4m_t}+\frac{1}{\lambda_t}\log(2/\delta)+ \hat{\mathcal{L}}(P, S_s)
\end{split}
\end{align*}

\subsection{Similar bound for forward transfer}

We now apply Lemma \ref{lemma:concentration} to the problem of forward transfer, meaning $f(z)=\mathcal{L}(z,\mathcal{D}_t)-\hat{\mathcal{L}}(z,S_t)$.

Doing so gives us the following inequality:

\begin{align}
\begin{split}
\mathcal{L}(Q_{s:t},\mathcal{D}_t) - \mathcal{L}(Q_{s},\mathcal{D}_t) &\leq \hat{\mathcal{L}}(Q_{s:t}, S_t) - \mathcal{L}(Q_{s}, D_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})\\
&+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_{s}}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]
\end{split}
\end{align}

Conveniently enough, $Q_s$ is not dependent on $S_t$, so if the loss is bounded we can apply Lemma \ref{lemma:hoeffding-concentration} and get with probability at least $1-\delta$ over the choice of $S_t$

\begin{align}
\begin{split}
\mathcal{L}(Q_{s:t},\mathcal{D}_t) \leq \hat{\mathcal{L}}(Q_{s:t}, S_t) + \frac{1}{\lambda_t} D_{\mathrm{KL}}(Q_{s:t}||Q_{s})+\frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\log(1/\delta)
\end{split}
\end{align}

This implies that given enough data, forward transfer and low forgetting can both be solved by the same measures...

%\subsection{Useful inequality to use for regret later}

%Another option that is available for us if the two task domains $\mathcal{D}_s, \mathcal{D}_t$ have the same support is to apply a joint data-hypothesis version of \ref{lemma:concentration},
%choosing $\rho=(Q_{s:t},\mathcal{D}_s), \pi=(Q_s, \mathcal{D}_s), f(h,z_i)=\ell(h,z_i)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j)$
%TODO: math badness here, you can't pick and choose different functions
% Doing f=L-\hat{L}(S_t)-\hat{L}(S_s) doesn't help since the exponetial moment cannot be bounded

%\begin{align*}
%\begin{split}
%\lambda_t\mathbb{E}_{(h,z)\sim (Q_{s:t},\mathcal{D}_t)}\left [\ell(h,z)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j) \right ] - \lambda_t\mathbb{E}_{(h,z)\sim (Q_{s},\mathcal{D}_s)}\left [\ell(h,z)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j) \right ] \\
%\leq D_{\mathrm{KL}}((Q_{s:t},\mathcal{D}_t)||(Q_{s},\mathcal{D}_s))+\log\mathbb{E}_{(h,z)\sim (Q_{s},\mathcal{D}_s)}\left [e^{\lambda_t(\ell(h,z)-\frac{1}{m_t}\sum_{j=1}^{m_t}\hat{\ell}(h,z_j))} \right ]
%\end{split}
%\end{align*}

%TODO: this is more generalization / regret

\subsection{Oracle inequalities for forgetting and transfer}

Starting from Lemma \ref{lemma:concentration}, we know that 

$$\mathbb{E}_{z\sim \rho}\left [f(z) \right ]\leq \mathbb{E}_{z\sim \pi}\left [f(z) \right ]+ \frac{1}{\lambda_t}D_{\mathrm{KL}}(\rho||\pi)+ \frac{1}{\lambda_t}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda_t(f(z)-\mathbb{E}_\pi f(z))} \right ]$$

In particular, for $\hat{\rho}_\lambda(z)\propto \pi(z) e^{-\lambda_t f(z) }$, this is an equality (from \citeauthor{donsker1975large}'s [\citeyear{donsker1975large}] variational lemma).
From this, we know that

\begin{equation}
\mathbb{E}_{z\sim \hat{\rho}_\lambda}\left [f(z) \right ]= \mathbb{E}_{z\sim \pi}\left [f(z) \right ]+ \frac{1}{\lambda_t}D_{\mathrm{KL}}(\hat{\rho}_\lambda||\pi)+ \frac{1}{\lambda_t}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda_t(f(z)-\mathbb{E}_\pi f(z))} \right ]
\end{equation}

If we pick $f(z)=\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_t)$ as before, we get

$$\mathbb{E}_{z\sim \hat{\rho}_\lambda}\left [\mathcal{L}(z,\mathcal{D}_s) \right ]= \mathbb{E}_{z\sim \pi}\left [f(z) \right ]+\mathbb{E}_{z\sim \hat{\rho}_\lambda}\left [\hat{\mathcal{L}}(z,S_t) \right ]+ \frac{1}{\lambda_t}D_{\mathrm{KL}}(\hat{\rho}_\lambda||\pi)+ \frac{1}{\lambda_t}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda_t(f(z)-\mathbb{E}_\pi f(z))} \right ]$$

And as such,
$$F( \hat{\rho}_\lambda,\mathcal{D}_s)\leq \inf_{\rho}\left \{ \hat{\mathcal{L}}(\rho,S_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(\rho||\pi)  \right \}-\mathcal{L}(\pi,D_s)+\frac{1}{\lambda_t}\log\mathbb{E}_{z\sim \pi}\left [e^{\lambda_t(\mathcal{L}(z,\mathcal{D}_s)-\hat{\mathcal{L}}(z,S_t))} \right ],$$

or using our previous terminology with $\hat{Q}^{\lambda_t}_{s:t}(h)\propto Q_s(h)e^{-\lambda_t\hat{\mathcal{L}}(h,S_t)}$, 

$$\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \hat{\mathcal{L}}(Q_{s:t},S_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

If we take an expectation on $S_t$, we get

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \mathbb{E}_{S_t\sim \mathcal{D}_t}\inf_{Q_{s:t}}\left \{ \hat{\mathcal{L}}(Q_{s:t},S_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}+\frac{1}{\lambda_t}\mathbb{E}_{S_t\sim \mathcal{D}_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

This gives us the following oracle inequality (in expectation):

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

$$\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ]=\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

$$=\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\mathbb{E}_{S_t\sim \mathcal{D}_t}e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_t)-\hat{\mathcal{L}}(h,S_t))} \right ]$$

Using Hoeffding's lemma, we get

$$\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\hat{\mathcal{L}}(h,S_t))} \right ] \leq \frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))} \right ]$$

Giving us

\begin{equation} \label{eq:oracle-base}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}+\frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))} \right ]
\end{equation}

Similarly, for forward transfer we would have

\begin{equation}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}(\hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_t)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q_{s}) \right \}+\frac{\lambda_t K^2}{8m_t}
\end{equation}

Both of these equations establish similar bounds, with the backwards transfer bound \eqref{eq:oracle-base} containing an additional domain disagreement term, $$\mathrm{Dis}(Q_s,\mathcal{D}_s, \mathcal{D}_t, \lambda_t )\triangleq\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))} \right ].$$

We note that 
$$\mathbb{E}_{h\sim Q_s}\left [\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t) \right ] \leq \mathrm{Dis}(Q_s,\mathcal{D}_s, \mathcal{D}_t, \lambda_t )\leq \max_{h}\left [\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t) \right ]$$

One possible bound for this term is to consider the multivariate Laplace approximation:

%TODO: all of the conditions here

suppose $\mathcal{H}=\mathbb{R}^d, d\geq 2$, so we will mark hypotheses using $\theta\in \mathbb{R}^d$. If $\lambda_t \rightarrow \infty$, it is well known that
$$\int_{\Theta}e^{-\lambda_t(\mathcal{L}(\theta, \mathcal{D}_t)-\mathcal{L}(\theta, \mathcal{D}_s))}Q_s(\theta)d\theta$$ converges asymptotically to $$\left (\frac{2\pi}{\lambda_t}\right )^{d/2}\frac{e^{-\lambda_t(\mathcal{L}(\theta^*, \mathcal{D}_t)-\mathcal{L}(\theta^*, \mathcal{D}_s))}}{\mathrm{det}(H_{\mathcal{L}_t}(\theta^*)-H_{\mathcal{L}_s}(\theta^*))^{1/2}}Q_s(\theta^*)$$

This means that 

\begin{equation} \label{eq:dis-lim}
	\lim_{\lambda_t\rightarrow\infty}\mathrm{Dis}(Q_s,\mathcal{D}_s, \mathcal{D}_t, \lambda_t )=\frac{d}{2\lambda_t}\log\left (\frac{2\pi}{\lambda_t}\right )+\mathcal{L}(\theta^*, \mathcal{D}_s)-\mathcal{L}(\theta^*, \mathcal{D}_t)+\frac{1}{\lambda_t}\log Q_s(\theta^*)-\frac{1}{2\lambda_t}\log \mathrm{det}(H_{\mathcal{L}_t}(\theta^*)-H_{\mathcal{L}_s}(\theta^*))
\end{equation}

We note that for this approximation to apply, several conditions must be met, the most restrictive of which being 
the existence of $\delta>0,\Delta>0$ such that $\mathcal{L}(\theta, \mathcal{D}_s)-\mathcal{L}(\theta, \mathcal{D}_t)$ is convex on $B_\delta$ and for every $\theta \notin B_{\delta}$, $$\mathcal{L}(\theta, \mathcal{D}_s)\geq \mathcal{L}(\theta, \mathcal{D}_t)+\Delta.$$

Equation \eqref{eq:dis-lim} contains four main terms:
\begin{enumerate}
	\item A linear function of the dimension that can be low if we choose $\lambda_t=O(\sqrt{d})$ for large enough parameter space size $d$.
	\item The maximal difference between losses $\mathcal{L}(\theta^*, \mathcal{D}_s)-\mathcal{L}(\theta^*, \mathcal{D}_t)$.
	\item The log-density of the likelihood of $\theta^*$ under $Q_s$, essentially a measure of the likelihood of choosing parameters with large loss discrepancy under $Q_s$.
	\item The negative log-determinant of the difference between Hessians for both loss functions for the point that maximizes loss difference. This term can be high if the determinant is near zero, meaning the Hessians for both loss functions are very similar. This essentially implies that transfer is harder if the loss landscapes are similar for both domains on points with maximal loss discrepancy.
\end{enumerate}

The most dominant term here is the maximal loss discrepancy $D(\mathcal{D}_s, \mathcal{D}_t)=\max_{\theta}\left \{\mathcal{L}(\theta, \mathcal{D}_s)-\mathcal{L}(\theta, \mathcal{D}_t) \right \}$.

%TODO: consider swapping posterior and using -f(z) to get a reverse here?

Another possible setting to consider is if $Q_s$ is the expected Gibbs posterior $Q^{\lambda_t}_{s}\propto P(h)e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}$. Under this setting, $$\mathrm{Dis}(Q^{\lambda_t}_{s},\mathcal{D}_s, \mathcal{D}_t, \lambda_t )\leq -\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim P}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_s)} \right ]-\mathcal{L}(P,\mathcal{D}_t).$$

Plugging this into \eqref{eq:oracle-base} gives us

$$
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q^{\lambda_t}_{s}) \right \}+\frac{\lambda_t K^2}{8m_t}-\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim P}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_s)} \right ]-\mathcal{L}(P,\mathcal{D}_t)
$$

And using Jensen's inequality, would give us

\begin{equation}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||Q^{\lambda_t}_{s}) \right \}+\frac{\lambda_t K^2}{8m_t}+\mathcal{L}(P,\mathcal{D}_s)-\mathcal{L}(P,\mathcal{D}_t)
\end{equation}

Since we know that the optimal $Q_{s:t}$ for the right-hand-side is the respective Gibbs distribution, we can use it and get the infimum value:

\begin{equation*}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq -\frac{1}{\lambda_t}\log \mathbb{E}_{Q^\lambda_s}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}\right ] +\frac{\lambda_t K^2}{8m_t}+\mathcal{L}(P,\mathcal{D}_s)-\mathcal{L}(P,\mathcal{D}_t)
\end{equation*}

And using Jensen's inequality we get 

\begin{equation*}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \mathcal{L}(Q^\lambda_s,\mathcal{D}_t) +\frac{\lambda_t K^2}{8m_t}+\mathcal{L}(P,\mathcal{D}_s)-\mathcal{L}(P,\mathcal{D}_t)
\end{equation*}


This bound is more limited due to the specific choice of $Q_s$ that cannot be found using optimization, but turns the discrepancy term to the average loss discrepancy under a data-free prior $P$.

Similarly for $Q_s=\hat{Q}^{\lambda_t}_{s}$ we would get 
$$\mathbb{E}_{S\sim \mathcal{D}_s}\mathrm{Dis}(\hat{Q}^{\lambda_t}_{s},\mathcal{D}_s, \mathcal{D}_t, \lambda_t )\leq \frac{\lambda_t K^2}{8m_s} -\frac{1}{\lambda_t}\mathbb{E}_{S\sim \mathcal{D}_s}\log\mathbb{E}_{h\sim P}\left [e^{-\lambda_t\hat{\mathcal{L}}(h,S)} \right ]-\mathcal{L}(P,\mathcal{D}_t).$$

Again plugging this into \eqref{eq:oracle-base} and using Jensen's inequality,
$$
\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \mathbb{E}_{S_s\sim \mathcal{D}_s}\inf_{Q_{s:t}}\left \{ \mathcal{L}(Q_{s:t},\mathcal{D}_t) + \frac{1}{\lambda_t}D_{\mathrm{KL}}(Q_{s:t}||\hat{Q}^{\lambda_t}_{s}) \right \}+\frac{\lambda_t K^2}{8m_t}+\frac{\lambda_t K^2}{8m_s}+\mathcal{L}(P,\mathcal{D}_s)-\mathcal{L}(P,\mathcal{D}_t)
$$

Since we know the Gibbs measure that minimizes the right-hand-side, we can do as before and get 

\begin{equation}
\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \mathbb{E}_{S_s\sim \mathcal{D}_s}\mathcal{L}(\hat{Q}^\lambda_s,\mathcal{D}_t)+\frac{\lambda_t K^2}{8m_t}+\frac{\lambda_t K^2}{8m_s}+\mathcal{L}(P,\mathcal{D}_s)-\mathcal{L}(P,\mathcal{D}_t)
\end{equation}

This provides us with a more practical prior $Q_s$ at the cost of additional approximation error.
If $m_s,m_t\rightarrow \infty$, \footnote{In this case, $\hat{Q}^\lambda_s=Q^\lambda_s, \hat{Q}^{\lambda_t}_{s:t}=Q^{\lambda_t}_{s:t}$} we get an interesting bound involving generalization and forgetting:

\begin{equation}
\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)-\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathcal{L}(\hat{Q}^\lambda_s,\mathcal{D}_t)\leq \mathcal{L}(P,\mathcal{D}_s)-\mathcal{L}(P,\mathcal{D}_t)
\end{equation}

We note that the right-hand-side is data-free and describes the general loss landscapes of both tasks. If both tasks come from the same distribution, for example, this implies that forgetting is upper bounded by the forward transfer (for empirical Gibbs measures).
Another way to look at this inequality is 

\begin{equation*}
\mathcal{L}(P,\mathcal{D}_t)-\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathcal{L}(\hat{Q}^\lambda_s,\mathcal{D}_t)\leq \mathcal{L}(P,\mathcal{D}_s)-\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)
\end{equation*}

we can expect both sides of this equation to be non-negative. This suggests that any problem with high forgetting for the Gibbs measure would also have poor forward transfer and a problem with good forward transfer will also have low forgetting.

We can derive an alternative bound from \eqref{eq:oracle-base} by setting the optimal posterior:

\begin{equation} 
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq -\frac{1}{\lambda_t}\log \mathbb{E}_{h\sim Q_s}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}\right ]+\frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))} \right ]
\end{equation}

$$
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\log\frac{\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))} \right ]}{\mathbb{E}_{h\sim Q_s}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}\right ]}
$$

Since $e^k\geq 0$ for all $k\in \mathbb{R}$, we can apply the log-sum inequality:

$$
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\frac{\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\log\frac{e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}}{e^{\lambda_t(-\mathcal{L}(h,\mathcal{D}_t))}} \right ]}{\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}
$$

\begin{equation} \label{eq:oracle-logsum}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\frac{\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\lambda_t\mathcal{L}(h,\mathcal{D}_s) \right ]}{\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}
\end{equation}

While this equation is not easy to interpret, we note that we can re-write the second term as $\mathbb{E}_{h\sim Q'_{s:t}}\left [\mathcal{L}(h,\mathcal{D}_s) \right ]$, where $$Q'_{s:t}(h)=\frac{Q_s(h)e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}}{\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}$$.
If, for example, $Q_s(h)\propto P(h)e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_s)}$, we have $Q'_{s:t}(h)\propto  P(h)e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}$, giving us a bound on backward transfer in terms of a learner that did not use any data from the source task.

In order to better understand \eqref{eq:oracle-logsum}, we can apply the Cauchy-Schwartz theorem on the expectation and get

$$
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{\sqrt{\mathbb{E}_{Q_s}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]\mathbb{E}_{Q_s}\left [\mathcal{L}(h,\mathcal{D}_s)^2 \right ]}}{\mathbb{E}_{Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}
$$


\begin{equation} \label{eq:oracle-CS-opt}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{\sqrt{\mathbb{E}_{Q_s}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}}{\mathbb{E}_{Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}\sqrt{\mathbb{E}_{Q_s}\left [\mathcal{L}(h,\mathcal{D}_s)^2 \right ]}
\end{equation}

And using the definition of variance and the triangle inequality (since both terms are non-negative), we get

\begin{equation} \label{eq:oracle-final}
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{\sqrt{\mathbb{E}_{Q_s}\left [e^{2\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}}{\mathbb{E}_{Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}(\sqrt{\mathrm{Var}_{Q_s}(\mathcal{L}(h,\mathcal{D}_s))}+\mathcal{L}(Q_s,\mathcal{D}_s))
\end{equation}

This equation gives us three terms that quantify forgetting in terms of $Q_s$
\begin{enumerate}
	\item The first multiplicative term connects the exponential moments of the discrepancies. This term is lower bounded by $1$, and is upper bounded (using the Bhatia-Davis inequality) by $$ e^{\lambda_t/2\max_h\left \{\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)\right \}}e^{\lambda_t/2(\mathcal{L}(Q_s,\mathcal{D}_t)-\mathcal{L}(Q_s,\mathcal{D}_s))}$$ 
	We have a domain discrepancy term that refers to the averaged loss discrepancy under $Q_s$ as well as the maximal discrepancy. Since we can expect $Q_s$ to give high probability to hypotheses with low loss on $\mathcal{D}_s$, this term is likely to be dominated by the forward transfer term in $e^{\lambda_t\mathcal{L}(Q_s,\mathcal{D}_t)}$.
	We can also use a lemma from \citet{cover1996universal} that states $$\frac{\mathbb{E}[X]}{\mathbb{E}[Y]}\leq max_i \frac{x_i}{y_i}$$ if $X,Y$ are non-negative RVs, in order to give an upper bound of the form $$\max\left \{e^{\lambda_t\max_h\left \{\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)\right \}},\quad  e^{\lambda_t(\mathcal{L}(Q_s,\mathcal{D}_t)-\mathcal{L}(Q_s,\mathcal{D}_s))}\right \}$$ where the first term is if the numerator is at least $1$, and the second is if it is in $[0,1)$.
	\item The first additive term is the standard deviation of the expected loss over $Q_s$, and will be low if $Q_s$ is stable in terms of the expected error.
	\item The second additive term is the expected loss of $Q_s$, and will be low if $Q_s$ generalizes well.
\end{enumerate}

Unsurprisingly, if $\lambda_t$ is high we arrive at a trivial bound on forgetting, as the Gibbs learner focuses on minimizing the empirical error for $S_t$.

Another potential bound can be derived from \eqref{eq:oracle-logsum} by applying the Bhatia-Davis inequality on numerator:

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{\max_h e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\mathbb{E}_{h\sim Q_s}\left [\mathcal{L}(h,\mathcal{D}_s) \right ]}{\mathbb{E}_{h\sim Q_s}\left [e^{\lambda_t(\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t))}\right ]}$$

By taking an upper bound on the denominator, we get 

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\max_h \left \{ e^{2\lambda_t|\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)|}\right \}\mathcal{L}(Q_s,\mathcal{D}_s)$$

Taking an expectation over $S_s$, we get

\begin{equation}
\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\max_h \left \{ e^{2\lambda_t|\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)|}\right \}\mathbb{E}_{S_s\sim \mathcal{D}_s}\left [\mathcal{L}(Q_s,\mathcal{D}_s)\right ]
\end{equation}

And we can apply standard expectation upper bounds or oracle bounds on $\mathbb{E}_{S_s\sim \mathcal{D}_s}\left [\mathcal{L}(Q_s,\mathcal{D}_s)\right ]$.
Sadly, this  bound implies that in order to avoid forgetting, we would like $\lambda_t=O(1)$, whereas standard bounds on forward transfer usually have $\lambda_t=O(\sqrt{m_t})$. This appears to be a irreconcilable issue with the Gibbs predictor. Some potential solutions to this issue are:

\begin{enumerate}
	\item Keep data from previous tasks ($S_s$) and use it for learning $Q_{s:t}$, such as by using a follow-the-leader algorithm.
	\item Have a sufficiently rich shared representation such that parameters can remember all previous tasks. It is not entirely clear how to represent such a thing in the context of standard PB bounds (with or without domain transfer).
\end{enumerate} 

Another alternative bound would be to take \eqref{eq:oracle-CS-opt} and apply the lemma from \citet{cover1996universal} to get 

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\max_h\left \{e^{\lambda_t|\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)|}\right \}\sqrt{\mathbb{E}_{Q_s}\left [\mathcal{L}(h,\mathcal{D}_s)^2 \right ]}$$

If $Q_s(h)\propto P(h)e^{-\lambda_s \hat{\mathcal{L}}(h,S_s)}$, we can apply a Laplace method approximation for the expectation assuming $\lambda_s\rightarrow \infty$, and that $P(\theta^*)\mathcal{L}(\theta^*,\mathcal{D}_s)^2>0$ for the optimal hypothesis $\theta^*\triangleq \arg\min_\theta \hat{\mathcal{L}}(\theta, S_s)$.

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\max_h\left \{e^{\lambda_t|\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)|}\right \}\sqrt{\left (\frac{2\pi}{\lambda_s}\right )^{d/2}\frac{P(\theta^*)\mathcal{L}(\theta^*,\mathcal{D}_s)^2e^{-\lambda_s\hat{\mathcal{L}}(\theta^*,S_s)}}{|H(\theta^*)|^{1/2}\mathbb{E}_{\theta\in P}\left [e^{-\lambda_s \hat{\mathcal{L}}(\theta,S_s)}\right ]}}$$

$$\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\left (\frac{2\pi}{\lambda_s}\right )^{d/4}\max_h\left \{e^{\lambda_t|\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)|}\right \}\frac{\sqrt{P(\theta^*)}\mathcal{L}(\theta^*,\mathcal{D}_s)e^{\lambda_s/2(\hat{\mathcal{L}}(P,S_s)-\hat{\mathcal{L}}(\theta^*,S_s))}}{|H(\theta^*)|^{1/4}}$$

Taking an expectation over $S_s$ yields

\begin{equation}
\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\left (\frac{2\pi}{\lambda_s}\right )^{d/4}\max_h\left \{e^{\lambda_t|\mathcal{L}(h,\mathcal{D}_s)-\mathcal{L}(h,\mathcal{D}_t)|}\right \}\mathbb{E}_{S_s\sim \mathcal{D}_s}\left [\frac{\sqrt{P(\theta^*)}\mathcal{L}(\theta^*,\mathcal{D}_s)e^{\lambda_s/2(\hat{\mathcal{L}}(P,S_s)-\hat{\mathcal{L}}(\theta^*,S_s))}}{|H(\theta^*)|^{1/4}}\right ]
\end{equation}

Considering \eqref{eq:oracle-base}, we can try to consider special settings for it:

If $Q_s(h)\propto P(h)e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_s)}$, we have 

\begin{equation*} 
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq -\frac{1}{\lambda_t}\log \mathbb{E}_{h\sim P}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)-\lambda_t\mathcal{L}(h,\mathcal{D}_s)}\right ]+\frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim P}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)} \right ]
\end{equation*}

Using Jensen's inequality on the first term, we get

\begin{equation} 
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\mathcal{L}(P,\mathcal{D}_s)+\mathcal{L}(P,\mathcal{D}_t) +\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim P}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)} \right ]
\end{equation}

We note that $-\mathcal{L}(P,\mathcal{D}_t)\leq\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)} \right ]\leq 0$ so the lest two terms somewhat offset one another, leaving us with a bound that implies that exponential weights are not much worse than not seeing the task at all.

We can also apply Cauchy-Shwartz on \eqref{eq:oracle-base} and arrive at the somewhat similar 

\begin{equation} 
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\mathcal{L}(Q_s,\mathcal{D}_t) +\frac{1}{2\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{-2\lambda_t\mathcal{L}(h,\mathcal{D}_t)} \right ]+\frac{1}{2\lambda_t}\log\mathbb{E}_{h\sim Q_s}\left [e^{2\lambda_t\mathcal{L}(h,\mathcal{D}_s)} \right ]
\end{equation}

And choosing the same posterior for $Q_s$ (upto scale) removes the final term. This is almost identical to some of our previous results.

An alternative result using Hoeffding's inequality to bound the discrepancy yields the non-useful (but interesting) upper bound
\begin{equation} 
\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\mathcal{L}(Q_s,\mathcal{D}_s)+\frac{\lambda_t K^2}{2}
\end{equation}

\textcolor{red}{If $\mathcal{H}$ is finite, we have 
$$\frac{\sum_h Q_s(h)e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}{\sum_h Q_s(h)e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}\leq 
\sum_h \frac{Q_s(h)}{Q_s(h)e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}}e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)}=\sum_h{e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}} $$}

By averaging this we get


\begin{equation} 
\textcolor{red}{\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim U}\left [e^{\lambda_t\mathcal{L}(h,\mathcal{D}_s)}\right ] + \frac{\log |\mathcal{H}|}{\lambda_t}}
\end{equation}

Where $U$ is a uniform distribution over hypotheses. This is somewhat surprising, as it implies that Gibbs learning does not perform much worse than the average case forgetting, even if the domains are highly dissimilar.

If $Q_s(h)\propto P(h)e^{-\lambda_t\hat{\mathcal{L}}(h,S_s)}$, we get 

\begin{equation} 
\mathbb{E}_{S_s\sim \mathcal{D}_s}\mathbb{E}_{S_t\sim \mathcal{D}_t}\mathcal{L}( \hat{Q}^{\lambda_t}_{s:t},\mathcal{D}_s)\leq \frac{\lambda_t K^2}{8m_t}+\frac{\lambda_t K^2}{8m_s}+\mathcal{L}(P,\mathcal{D}_s)+\mathcal{L}(P,\mathcal{D}_t) +\frac{1}{\lambda_t}\log\mathbb{E}_{h\sim P}\left [e^{-\lambda_t\mathcal{L}(h,\mathcal{D}_t)} \right ]
\end{equation}

\subsection{Meta-learning formulation}

Let $\mathcal{P}\in\mathcal{M}(\mathcal{M}(\mathcal{H}))$ be a data-free distribution over hypothesis distributions, which we will refer to as the hyper-prior. We define 
$$\mathcal{Q}_s=\mathcal{J}_s(S_s,\mathcal{P})$$ and similarly for $\mathcal{Q}_{s:t}$, where the only difference is that the inputs are meta-distributions instead of normal distributions over hypotheses. For example, we could have $\mathcal{J}_s$ sample from the hyper-prior $\mathcal{P}$ and activate $J_s$ on the sampled distribution, thus giving us a new meta-distribution that depends on $S_s$. While bounds from the previous section can be applied to this new setting, sampling from the hyper-posterior is impractical, and thus measuring loss terms is not reasonable for this general setting, so we must make a stronger assumption on the construction process.

Assume 

%\subsection{Extending generalization bounds to forgetting for meta-learning}
%WRONG! can't use Markov like this, not independent choice
%Suppose we have an upper bound for expected error of a hyper-posterior $\mathcal{Q}$ of the form $\mathbb{E}_{\mathcal{D}\sim \tau}\mathcal{L}(\mathcal{Q}, \mathcal{D})\leq B(\mathcal{Q},\mathcal{P}, S_1,\ldots, S_N, Q)$, and assuming the training tasks are i.\!i.\!d. we can use Markov's inequality to arrive at the following with probability at least $1-\delta$ over the choice of the training domains:

%\begin{equation}
%\frac{1}{N}\sum_{i=1}^{N}\mathcal{L}(\mathcal{Q}, \mathcal{D}_i) \leq B(\mathcal{Q},\mathcal{P}, S_1,\ldots, S_N, Q) + K\sqrt{\frac{\log N/\delta}{2N}}
%\end{equation}

%We note that this bound makes no explicit use of the specific tasks - this bound applies for any set of $N$ tasks. This does imply that so long as we have access to data from all training tasks throughout meta-training, we can mitigate forgetting.

\section{Specific problem formulation: connecting regret and transfer with domain adaptation and PAC-Bayes}

\subsection{Problem definition} \label{sec:regret-formulation}

Considering the same setting as Section \ref{sec:forgetting-formulation}, we consider the metric of regret:

\begin{defn}
	The regret on $n$ tasks is defined as 
	$$REGRET_n = \sum_{t=1}^{n}\mathcal{L}(Q_{1:t}, \mathcal{D}_t)-\min_{Q} \sum_{t=1}^{T}\mathcal{L}(Q, \mathcal{D}_t),$$
	where $Q_{1:t}$ is the distribution obtained by applying the series of processes $J_1, \ldots, J_t$ on $P, \{S_1,\ldots,S_t\}$, such that $Q_1=J_1(S_1,P)$ and $Q_1:i=J_i(S_i, Q_{i-1})$ for $i>1$.
\end{defn}

Again, we begin with the two-task setting ($n=2$), and notice that

\begin{equation}
\mathcal{L}(Q_1, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2)=
\mathcal{L}(Q_1, \mathcal{D}_1)-\mathcal{L}(Q_1, \mathcal{D}_2)+\mathcal{L}(Q_1, \mathcal{D}_2)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2)-\mathcal{L}(Q_{1:2}, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_1)
\end{equation}

Marking $\mathcal{L}(Q_1, \mathcal{D}_2)\triangleq FWT(Q_1, \mathcal{D}_2)$ as the forward transfer from task $1$ to task $2$, we have
\begin{equation}
\mathcal{L}(Q_1, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2)=
\underbrace{\mathcal{L}(Q_1, \mathcal{D}_1)-\mathcal{L}(Q_1, \mathcal{D}_2)}_{\text{transfer gap from source}} + FWT(Q_1, \mathcal{D}_2) + \underbrace{\mathcal{L}(Q_{1:2}, \mathcal{D}_2)-\mathcal{L}(Q_{1:2}, \mathcal{D}_1)}_{\text{transfer gap from target}}+BWT(Q_{1:2}, \mathcal{D}_1)
\end{equation}

We can use known results from domain adaptation methods to provide an upper bound to the transfer gap terms. As a simple (and loose) example, we can consider the following theorem from \citet{shui2020beyond}:

\begin{lemma}
	Assume that $\ell$ is bounded in $[0, K]$. For all $h\in \mathcal{H}$ we have
	$$\mathcal{L}(h, \mathcal{D}_1)-\mathcal{L}(h, \mathcal{D}_2)\leq \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_1||\mathcal{D}_2)},$$ 
	where $D_{JS}$ is the Jensen-Shannon divergence between the joint data distributions.
\end{lemma}

Due to the symmetry of the Jensen-Shannon divergence, this bound applies for $\mathcal{L}(h, \mathcal{D}_2)-\mathcal{L}(h, \mathcal{D}_1)$ as well, and so we have

\begin{equation}
\mathcal{L}(Q_1, \mathcal{D}_1)+\mathcal{L}(Q_{1:2}, \mathcal{D}_2) \leq 
K\sqrt{2 D_{JS}(\mathcal{D}_1||\mathcal{D}_2)}+FWT(Q_1, \mathcal{D}_2)+ BWT(Q_{1:2}, \mathcal{D}_1).
\end{equation}

We note that while this bound is somewhat naive in that it does not take the posterior distributions into account at all, it does give us some insight into the relationship between transfer and regret - minimizing forgetting (and lowering backwards transfer) gives us a guaranteed upper bound on the overall regret. 

We can easily extend this result for $n$ tasks: (TODO later - prove this) %TODO
\begin{equation}
\begin{split}
 \sum_{t=1}^{n}\mathcal{L}(Q_{1:t}, \mathcal{D}_t) \leq &  FWT(Q_1, \mathcal{D}_2) + \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_{1}||\mathcal{D}_{2})} \\ &+ \sum_{t=2}^{n}\left ( BWT(Q_{1:t}, \mathcal{D}_{t-1})+ \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_{t}||\mathcal{D}_{t-1})}\right ) \\&
 \end{split}
\end{equation}

A minor potential improvement over this would be to use a Pac-Bayes bound for the first term $Q_1$, giving us with probability at least $1-\delta$ over the choice of $S_1$, uniformly for all $Q_1$,
\begin{equation}
\begin{split}
\sum_{t=1}^{n}\mathcal{L}(Q_{1:t}, \mathcal{D}_t) \leq &  \hat{\mathcal{L}}(Q_1, S_1)+\frac{1}{\lambda_t}D_{KL}(Q_1||P)+C(\lambda_t,\delta,P,S_1, K) \\ &+ \sum_{t=2}^{n}\left ( BWT(Q_{1:t}, \mathcal{D}_{t-1})+ \frac{K}{\sqrt{2}}\sqrt{D_{JS}(\mathcal{D}_{t}||\mathcal{D}_{t-1})}\right ) \\&
\end{split}
\end{equation}

This implies that if the Jensen-Shannon divergence between subsequent tasks is smaller on average than $\sqrt{2}/K$, minimizing backwards transfer can potentially lead to a sub-linear regret bound.

\clearpage
\bibliographystyle{plainnat}
\bibliography{library}

\end{document}