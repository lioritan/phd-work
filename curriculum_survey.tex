\documentclass[letterpaper]{article}

% AAAI-style 2-per-page format, without the annoying bits
\setlength\topmargin{-0.25in} \setlength\oddsidemargin{-0.25in}
\setlength\textheight{9.0in} \setlength\textwidth{7.0in}
\setlength\columnsep{0.375in} \newlength\titlebox \setlength\titlebox{2.25in}
\setlength\headheight{0pt}  \setlength\headsep{0pt}
\flushbottom \sloppy

\pdfpagewidth=8.5in
\pdfpageheight=11in


\usepackage{times} 
\usepackage{helvet}  
\usepackage{courier}  
\usepackage{url}  
\usepackage{graphicx} 

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{multirow}
\usepackage{ctable}
\usepackage{color}
\usepackage{natbib}
\usepackage[normalem]{ulem}
%\usepackage[style=authoryear]{biblatex}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black, % color for table of contents
	citecolor=black, % color for citations
	urlcolor=blue, % color for hyperlinks
	bookmarks=true,
}
\urlstyle{same}

\raggedbottom %nicer enumerate
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Curriculum learning in RL}
\begin{document}
	\maketitle
	\begin{abstract}
		%Added most methods not focused on varied tasks
		%Added methods focused on task variety and exploration
		%Added short-term goals, blog, POMDP fomulation
		%Added parameter and code base sections, updated goals
		%Added model-based RL section
		Added experiments section
	\end{abstract}

\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem formulations - 14.1.21} \label{sec:formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}
	A \textbf{task} (sometimes called an environment) is defined as a MDP $<S,A,P,R,\gamma>$,
	Where $S$ is a set of states, $A$ is a set of actions, $P:S\times A\times S\rightarrow [0,1]$ is a transition function, 
	$R:S\times A\rightarrow \mathbb{R}$ is a reward function, $\gamma\in[0,1]$ is a discount factor.
\end{defn}

\begin{defn} \label{defn:curriculum-pomdp}
	Given a (possibly infinite) set of possible tasks $\mathcal{T}$, a \textbf{curriculum} is the policy rollout of the POMDP $<S,A,P,R,\Omega,O, \gamma>$, where:
	\begin{itemize}
		\item $S$ is the (unobserved) set of student states. In the case of neural networks, $s_t$ is the vector of network parameters.
		\item $a_{t}^{\tau, k}\in A$ is the action of training the student on task $\tau\in \mathcal{T}$ for $k$ time steps.
		\item $P$ is the (unobserved) transition function between states. This is determined by the student's learning algorithm.
		\item $R:O\times A\times H \rightarrow \mathbb{R}$ is a reward function that evaluates student performance on a given task, possibly based on observation history $H$.
		\item $o_t$ is the teacher's student observation on the task. This is usually a `black box' observation, meaning $o_t\neq s_t$. Common examples of $o_t$ are the student's task trajectory or the student's discounted return.
	\end{itemize}	
	%\[
	%\max_{D^{\mathcal{T}}} \sum_{T \sim \mathcal{T}_target} {P_T^N dT}
	%\]
\end{defn}

A \textit{teacher algorithm} is defined by specifying the observation method $o_t$ and the teacher's reward $R$. It is often assumed that tasks in $\mathcal{T}$ have the same internal state and action space, with dynamics and task-level rewards varying between tasks.
Note that changing the start state or goal state for a task is also possible, since this is part of the specification of the task.
%There are no works (that I found) that define RL curricula with a control measure smaller than a full episode on a task.

\begin{defn} \label{defn:curriculum-pomdp-shaping}
	A \textbf{reward-shaped curriculum} is a variant curriculum where the actions are different:
	Given a set of tasks $\mathcal{T}=\{\tau|\tau=<\hat{S},\hat{A},\hat{P_i},\hat{R_i},\hat{\gamma}>\}$, the action $a_{t}^{\tau', k}$ is the action of training the student on task $\tau'=<\hat{S},\hat{A},\hat{P_i},R',\gamma'>$ for $k$ time steps, where $\gamma'\leq \hat{\gamma}$ and $R'(s,a)=\hat{R_i(s,a)}+R'(s,a)$ for a given shaped reward $R'$.
\end{defn}

An alternative formulation to the curriculum problem is embedded in game theory:
\begin{defn} \label{defn:curriculum-game-theory}
	A \textbf{collaborative curriculum} is defined according to the following sequential, asymmetric two-player game:
	\begin{itemize}
	\item $H$ is the game's history, and is available to both players.
	\item A teacher player's action is to provide a task $\tau$ and a time step budget $k$,
	\item A student player's action is to train on the given task, as well as other tasks in $H$, for a total time of $k$.
	\item The action-payoff function $v:\mathcal{T}\times \mathbb{N^+} \times H \rightarrow \mathbb{R}\times \mathbb{R}$ is the paired payoff for the teacher and student. 
	\end{itemize}
\end{defn}

We note that this game may be a zero-sum game (the teacher and student have opposed payoffs) or more cooperative, depending on the payoff function.
It is also important to note that if the student is required to perform the given task for all $k$ time steps, and the only goal is to maximize the teacher's payoff, we get a formulation that is equivalent to definition \ref{defn:curriculum-pomdp}, and in fact would be reasonable to solve using reinforcement learning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Measuring task difficulty and curriculum - 23.2.21} \label{sec:difficulty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \cite{Matiisen2020}, the task probability is based on the absolute value of the reward improvement rate, measured by the difference between the trained task reward $r_t$ and the trained task reward last measured for the same task $r_{t'}$.
This method only works for a finite set of tasks. Tasks with high improvement or large performance decreases will be sampled more often.

In \cite{Feng2020}, a curriculum is designed for a specific task (Sokoban), and task difficulty is measured by a domain-specific measurement, the number of boxes to be moved to goal locations. Difficulty is increased based on the reverse of \cite{Matiisen2020} - if the success rate on a given difficulty level has not changed for several iterations, difficulty should be increased. This is because measuring task feasibility in Sokoban is hard, and therefore improvement rate can be zero even in cases where the agent does generalize well.

In \cite{Klink2020}, some task parameters are called a `context' (e.g. friction and goal location), and task probability for a given task is based on the student's value function estimate of the start state (including the context).
Contexts are assumed to come from a Gaussian distribution and the target distribution is assumed to be \textbf{known}.
The value function estimate is normalized by the context probability and by KL-divergence from the target distribution to discourage a large gap from the target environment. A KL-divergence constraint is also used to prevent large changes in the context distribution between updates.

In \cite{Jiang2020}, a curriculum is learned in order to re-try previously solved tasks in a manner that would hopefully lead to generalization. Task probability is proportional to the (discounted) TD-error for the last sampled trajectory - this is calculated as $ \frac{1}{T} | \sum_{t=0}^{T} {G_{t:T}}|$, where $G_{t:T}$ is the $t$-step discounted return. Tasks with high TD-error supposedly have higher learning potential, and are therefore more likely to be re-tried. 

In \cite{Portelas2019}, the authors define \textit{ALP-GMM}, a MAB (Multi-Armed Bandit) based teacher, that chooses the environment parameters based on absolute learning progress, the same thing as absolute improvement rate.
Unlike \cite{Matiisen2020}, task parameters are continuous, so they are separated into regions to allow comparisons to previous tasks. The major innovation of \textit{ALP-GMM} is in how to create the separation. A Gaussian Mixture Model is learned to fit task parameters to the improvement rate, giving a better connection between parameter space and improvement rate (compared to random task selection or hard-coded regions which is a method called \textit{RIAC}).
In \cite{Portelas2020}, a continuation of this approach with multiple students was proposed. In this classroom-like setting, the goal is to maximize task selection as well as student selection, meaning schedule must offer both a task and a student to give said task to, and optimizes over all students. \textit{ALP-GMM} is used for task selection, and a knowledge vector for each student is learned and used to select a task that will have good improvement rate for multiple similar students (similarity measured by knowledge vector).

In \cite{Narvekar2019}, a curriculum for grid-world environments is learned with task-specific operators used to specify task simplification (sub-goals and parameter-based simplifications). The curriculum itself measured time to convergence, i.e. the number of episodes until the policy ceases to change. A low time was considered more desirable.
In a continuation paper, \cite{Narvekar2020}, the authors improve on this by measuring convergence with actions instead of episodes, a policy is considered to converge if the number of steps required to reach a goal state is at most $\delta$ more than the optimal policy for a pre-defined $\delta$. The set of tasks is pre-defined, and both a task and a goal must be chosen.

In \cite{Gutierrez2020}, tasks are chosen from a given set based on a combination of relevance and a difference from previously chosen tasks. The relevance criterion is measured using policy entropy. It is assumed that the optimal policy for each task is known, and a set of \textit{validation tasks} similar to the test tasks is given.
A task is considered relevant if for at least one \textit{validation task}, we run the optimal policy in the task, measure mean entropy for the visited states, then train the policy $l$ steps and measure the same entropy, and a lower entropy is reached. Basically, a task is relevant if adapting it to a validation task makes it more deterministic.
Interestingly, they show that choosing all subtasks did not necessarily give better meta-learning.

In \cite{Justesen2018}, a task-specific difficulty measure is defined, and difficulty is increased on task success and decreased on task failure by a fixed amount.

In \cite{Jain2017}, tasks are chosen from a given set, and several selection criteria are defined. Among these criteria are:
\begin{itemize}
	\item Reward maximizing task - run a trajectory on each task, pick task with highest reward.
	\item Transfer maximizing task - before training, estimate task transferability (for example by calculating reward for task b after transferring policy trained on task a) for all task pairs as well as the target task. The curriculum is one that will maximize transferability from some initial task to the target task.
	\item Active reward maximization - assumes each task comes with a feature vector, and use active-learning regression to estimate the transferability between tasks
\end{itemize}
These curriculum were shown to converge more quickly and to a higher reward than learning on the target task directly.

In \cite{Reny2019}, a curriculum for experience re-play is learned. Unlike \cite{Jiang2020}, new tasks are created. This is done by choosing goal states (like HER) that will be difficult but achievable. In the paper, HER is considered a curriculum of choosing goal states estimated to be easy to achieve (via value function estimation), and this paper adds a term to try and pick goals that will have low Wasserstein (earth-mover) distance from the target goal distribution (which is assumed to be known). The choice of Wasserstein distance comes from a theorem assuming that close goals lead to close policies.

In \cite{Dennis2020}, environment parameters are chosen (tasks) to try and create hard but solvable tasks. An adversarial task generator creates a task, the student `protagonist' trains on it, then another `antagonist' agent train on it. The task generator tries to change the environment parameters to maximize `antagonist' return and minimize `protagonist' return (both were trained to convergence on the last environment). This is shown to be a nash-equilibrium between `protagonist' and `antagonist'. The intent is that as student adapts, environment must become more difficult.

In \cite{Racaniere2019}, goal states are chosen (tasks) to try and create hard but solvable tasks. 
This is done by trying to balance a trade-off between three metrics: feasibility, validity, and coverage.
Goal validity (likelihood of student reaching the goal) is measured by the Negative Log-Likelihood of generating a goal that's very close to a previously achieved goal.
Goal feasibility is measured using a binary classifier that tries to predict agent success given a goal. The teacher knows this prediction and uses it when generating environments.
If target goal distribution is known, this is also used to measure the teacher's loss.
The paper evaluated the performance of measures in complex environments, and showed that all metrics were useful.

In \cite{Al-Shedivat2017}, a curriculum is implicitly learned in a multi-player game by adversarial self-play. The paper itself assumes task distribution is given and is similar to existing meta-RL approach MAML (\cite{Finn2017a}).

In \cite{Zhang2020}, a goal state is chosen from a given parameter space. An ensemble of value function estimators (Q was chosen in the paper) is learned, and is used to estimate the standard deviation for given goal and start state. Goals with high standard deviation (=low confidence) are assumed to be good as they should be complex but not impossible.

In \cite{Florensa2017, Florensa2018}, a curriculum based on medium difficulty goal-based tasks is created, using an algorithm called \textit{GoalGAN}. This is done by trying to find a policy to maximize the probability of reaching the goal assuming some known test goal distribution (which was uniform in the paper). Do to so efficiently, goals are sampled from a smaller set of goals (called \textit{GOID}), with estimated success probability between some lower and upper bound. This is done using a GAN, with the discriminator separating goals in \textit{GOID} and goals not in \textit{GOID}, with policy evaluations used as labels (the return should be in the range). Does well compared to self-play and \textit{RIAC}.

A similar approach can be seen in \cite{Srinivasan2019}, where this notion of medium difficulty goals was applied to discrete state and action spaces. In their paper, a curriculum of goals is created by trying to choose start states within certain distance to the goal, but since random walk can be an issue in discrete spaces, demonstrations are used to determine both the start state and the curriculum, as each task is created by following an expert $i$ steps, with $i$ decreasing as the agent improves.

In \cite{Wohlke2020}, a curriculum is learned in order to maximize probability of reaching a known goal state given that the start state is selected uniformly. This is done by changing the distribution of start states to match the $L_2$ norm of the gradient for our performance measurement (probability of reaching the goal). This is estimated by picking a dimension and doing a zeroth-order approximation of the partial derivatives. This only works if the state space is euclidean, but that is quite common. This is quite similar to improvement rate for goal-based rewards.

In \cite{Akkaya2019}, a method to randomize environment parameters was proposed. In each iteration, environment parameters are uniformly sampled from a $d$-dimensional box. To adapt the box size, a random parameter is sampled from one of the edges of its current range, and an episode is trained to measure reward. Once enough data for a parameter is gathered, we measure the average performance and if it is above or below a set threshold, we change the box size. 

In \cite{Fang2020}, a combination of task feasibility and the closeness to the test environment are used to estimate progress. A good curriculum will give tasks closer to the test environment, but still feasible. Task generation is done with a GAN setup: A generator creates tasks (by choosing task parameters), and a discriminator tries to estimate `task progress' - the similarity of the trajectory for the generated task to a trajectory on the test task. To do this, rollouts on the test task are required. The generator loss is a combination of the discriminator error and the task's expected return.

In \cite{Milano2021}, task parameters are chosen from a finite set for use in evaluations of the gradient in an Evolutionary Strategies algorithm. After a warm-up period where tasks are chosen randomly, possible parameters are organized into subsets based on average reward, and evaluations pick randomly from a subset based on this ``difficulty. Notably, they show empirically that dividing subsets into difficulties non-uniformly is preferable, using $x^2$ and $x^3$ as the difficulty measures, thus creating larger subsets of easy tasks and smaller subsets of hard tasks. 

In \cite{Li}, if a goal-conditioned task is not achieved, random search is used to try and find a sub-goal such that the agent can easily reach the original goal from the sub goal. Once data has been collected on the new trajectory to the sub-goal, the agent is updated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ensuring task variety - 24.1.21} \label{sec:variety}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \cite{Jiang2020}, a probability bonus is given proportional to the number of tasks seen since a given task was sampled, thus forcing the task to eventually be re-played.

In \cite{Gutierrez2020}, tasks are chosen from a given set based on a combination of relevance and a difference from previously chosen tasks. Difference is measured by the mean KL-divergence of the optimal policies for a pre-defined set of states. A task is different from chosen tasks if the KL-divergence of its optimal policy is sufficiently large.

In \cite{Reny2019}, an optimization constraint for choosing a goal state is imposed, forcing tasks to come from different training trajectories.

In \cite{Racaniere2019}, goal states are chosen (tasks) to try and create hard but solvable tasks. 
This is done by trying to balance a trade-off between three metrics: feasibility, validity, and coverage.
Goal coverage is measured by the entropy of the teacher over random goals.

In \cite{Mehta2019}, environment `difficulty' is estimated by learning a discriminator that tries to separate rollouts from generated environments and rollouts in a given reference environment. The discriminator output is used as the teacher's reward for the generated environment, so environments that are very different from the reference environment are rewarded. 
The teacher agent tries to choose environment parameters to maximize this reward, and therefore encourage variety. The teacher agent uses a method called \textit{SVPG}, Stein Variational Policy Gradient, a method similar to \textit{A2C} but with multiple policy parameters (called particles), and a trade-off between parameter reward and difference between them (measured with a kernel function).
Expanding on the previous apporach, \cite{Raparthy2020} create a method with two teacher agents: one uses a discriminator to force environment parameters away from the reference environment like in \cite{Mehta2019}, and the other that chooses a task from the given distribution in an adversarial manner: the task selector uses the student policy and a stopping policy to act in the reference environment and find a goal state. The student must then reach that goal in the generated environment. To update the stopping policy, the adversary is rewarded for finding a goal in few actions that also requires many actions to reach, and is therefore hopefully hard but solvable. 
In a continuation paper, \cite{Mehta2020}, a meta-RL algorithm is used to adapt the student policy to the generated environment, and the discriminator tries to predict whether the trajectory came from the policy before or after adaptation, thus encouraging tasks with large changes during adaptation. Notably this removes the need to have reference environments.

In \cite{Sukhbaatar2017}, a goal-choosing agent acts in the environment to try and find states that will be difficult to solve. This was the direct inspiration to \cite{Raparthy2020}, and where its stopping policy notion comes from. In a later paper, \cite{OpenAI2021} expanded on this approach by: filtering goal states to goals not solved by the student, the teacher having a set number of time steps to propose goals, and the student measured on achieving one of five generated goals. Additionally, a custom reward function to the teacher, and a clipped behavior cloning loss are used to improve stability.

In \cite{Gupta2018}, the environment parameters are not given directly to the trained student, but are used as latent variables to generate an environment. A discriminator is learned to try and predict the latent task that was used to generate a given trajectory, with high confidence prediction yielding high reward. Since a high variety between tasks leads to good prediction, this helps with variety.

In \cite{Jabri2019}, a curriculum to learn an MDP without a reward is proposed. The teacher agent chooses environment parameters as latent variables, and gives state reward $r_z(s) = log q(s|z) - log q(s)$, the information gain of the latent variable. The teacher agent uses the sampled trajectories to improve this $q$, via discriminative clustering.

In \cite{Kaddour2020}, tasks are characterized by a learned low-dimensional task embedding. This is done by learning an encoder on trajectories that tries to maximize likelihood of parameters and rewards given the observations. To pick a task, a point in the latent space is picked to maximize some utility.
The utility used in the paper was picking maximal surprise (self-information) - a latent point with low likelihood according to the estimate task distribution.

In \cite{Wang2019}, \textit{POET}, a genetic algorithm for varied task generation is proposed. A set of environments and a set of agents are maintained, and at each step, environment parameters are randomly changed (with a ranking based on euclidean distance of parameters from already existing environments), each agent is trained on each environment (using a single step of ES - evolution strategies), and if an environment is challenging (mean reward at least $x$ and at most $y$), it is kept.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method summary - 23.2.21} \label{sec:summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

See table \ref{methods-table}

\begin{table*}
%\centering
\caption{Comparison of approaches}
\label{methods-table}
\begin{tabular}{|l | l | l | l  | l|} 
	\hline
	Paper & Difficulty measure & variety measure  & Controllable elements     \\ \hline	
	\cite{Matiisen2020} & improvement rate & none & task selection  \\ \hline
	\cite{Feng2020} & domain-specific & none & task parameters  \\ \hline
	\textit{SPRL} \cite{Klink2020} & $V(s_0)$ & none & task parameters \\ \hline
	\cite{Jiang2020} & improvement potential (error) & uncertainty bonus & task selection  \\ \hline
	\textit{ALP-GMM}, \cite{Portelas2019} &  improvement rate & none & task parameters \\ \hline
	\cite{Portelas2020} & improvement rate & none & task parameters, student  \\ \hline
	\cite{Narvekar2019} & fast policy convergence & none & task parameters \\ \hline
	\cite{Narvekar2020} & fast policy convergence & none & task selection, goal state \\ \hline
	\cite{Gutierrez2020} & validation tasks & Policy KL-divergence & task selection \\ \hline
	\cite{Justesen2018} & $\pm$ difficulty, on success/fail & none & task parameter \\ \hline
	\cite{Jain2017} & transfer improvement & none & task selection \\ \hline
	\cite{Reny2019} & $V(s_0)-diff(g,g*)$ & Constrained goals & start state, goal state \\ \hline
	\textit{PAIRED} \cite{Dennis2020} & adversarial reward & none & task parameters \\ \hline
	\cite{Racaniere2019} & prob of task success & teacher entropy & goal state \\ \hline
	\cite{Al-Shedivat2017} & self-play & none & opponent agent \\ \hline
	\cite{Zhang2020} & ensemble confidence & none & goal state \\ \hline
	\cite{Wohlke2020} & $\sim V'(s_0)$ & none & start state \\ \hline
	\cite{Fang2020} & improvement rate \& test similarity & none (GAN helps) & task parameters \\ \hline
	\textit{GoalGAN} \cite{Florensa2018} & return in range & none (GAN helps) & goal state \\ \hline
	\cite{Mehta2019} & none & learned trajectory difference & task parameters \\ \hline
	\cite{Raparthy2020} & adversarial reward & learned trajectory difference & task parameters \\ \hline
	\cite{Sukhbaatar2017} & adversarial reward & None & task parameters \\ \hline
	\cite{OpenAI2021} & adversarial reward & new goals & task parameters \\ \hline
	\cite{Mehta2020} & none & learned adaptation difference & task parameters \\ \hline
	\cite{Gupta2018} & none & learned state difference & task parameters \\ \hline
	\cite{Jabri2019} & none & trajectory clustering & task parameters \\ \hline
	\cite{Srinivasan2019} & demonstrated trajectories & none & start state \\ \hline
	\cite{Akkaya2019} & return, fixed change size & none & task parameters \\ \hline
	\cite{Kaddour2020} & none & surprise (low likelihood) & task parameters \\ \hline
	\textit{POET} \cite{Wang2019} & return in range & distance in parameter space & task parameters \\ \hline
	\cite{Milano2021} & average reward & none & task parameters \\ \hline
	\cite{Li} & solved before & none & sub-goal \\ \hline
\end{tabular}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model-Based RL - 1.3.21} \label{sec:mbrl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

papers

\cite{Curi2020} - H-UCRL

\cite{Chua2018} - PETS

\cite{Ha2018} - world models

\cite{Raileanu2020} - RIDE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Measurable problem parameters - 3.2.21} \label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
	\item Task complexity/difficulty measure - this needs to also consider the student
	\item Student knowledge on other tasks - measured by rewards for other tasks or learning progress
	\item Student knowledge on the current task - measured by pre-train mean reward
	\item Teacher knowledge on task space - whether a well-defined measurement of task difficulty is available to the teacher
	\item Teacher knowledge on student performance - number of task rewards, possibly normalized by the size of the task space and how tasks are sampled from it
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Relevant posts - 13.1.21} \label{sec:blogs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
	\item 
	\href{https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html}{Curriculum summary} - a very good summary of curriculum methods for deep RL, with references to many of the better known papers.
	
	\item \href{https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html}{Exploration in deep RL} - a blog post on recent methods in exploration.
	
	\item 
	\href{https://bair.berkeley.edu/blog/2017/12/20/reverse-curriculum/}{Reverse curriculum generation} - a post by one of the authors of \textit{GoalGAN}, this is a short explanation of the notion of `goals of medium difficulty'.
	
	\item 
	\href{https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html}{MetaRL summary} - an OK summary of metaRL, interesting mostly for the discussion of exploration and task generation.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Code bases - 3.2.21} \label{sec:code}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
	
	\item 
	\href{https://github.com/flowersteam/meta-acl}{Meta ACL} - Curriculum RL code by R. Portelas for \cite{Portelas2020a}. Has multiple teacher algorithms.
	
	\item 
	\href{https://github.com/lioritan/phd-work}{My codebase} - The code I'm working on for curriculum learning.
	
	\item 
	\href{https://github.com/ray-project/ray}{Ray} - one of the most popular frameworks for RL algorithms, originally by Berkley. Hard to extend but supports most known methods.
	
	\item \href{https://github.com/openai/baselines}{Baselines} - OpenAI's original library for stable, open-source implementations of common RL algorithms, no longer active.
	
	\item 
	\href{https://github.com/DLR-RM/stable-baselines3}{Stable baselines} - a community (German Aerospace Center) continuation and improvement of the baselines library.
	
	\item 
	\href{https://github.com/google/dopamine}{Dopamine} - Google's RL agent library, has very few algorithms.
	
	\item 
	\href{https://github.com/facebookresearch/ReAgent}{ReAgent} - Facebook research's RL agent library, has few algorithms focusing on off-policy methods.
	
	\item 
	\href{https://github.com/deepmind/acme}{Acme} - Deepmind's RL agent library, relatively new, has few algorithms focusing on off-policy methods. 
	
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Short-term goals - 1.3.21} \label{sec:short-term}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
	\item \sout{Create a POMDP problem formulation (based on Teacher-Student paper) }
	\item \sout{Add reward shaping to the problem formulation}
	\item \sout{Make a `mixture of experts' method with multiple teachers, consider how to weigh them}
	\item \sout{Create a starting code-base with environments and simple method}
	\item \sout{Consider a game theory formulation of the problem}
	\item \sout{Run and analyze experiments on curriculum effectiveness for existing environments}
	\item \sout{Estimate model impact by leaking task parameters as state}
	\item \sout{Measure impact of exploration using EBQL}
	\item \sout{Implement a model-based method (PETS / H-UCRL) and test it on existing environments}
	\item \sout{Add Mujoco env based on PEARL code (seems not to work on Windows)}
	\item Attempt to quantify problem complexity for each environment - there's probably a teacher algorithm here
	\item Add PAIRED / similar adversarial generation method (Rancarnier?)
	\item Add GoID-like implementation (using estimated task difficulty or starting V). Also check high v low v goid
	%\item Incorporate latent variable from trajectory as parameter or reward bottleneck
	%\item Try using off-policy evaluation to estimate task performance
	%\item Try to see how in-task intervention would change the formulations
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Long-term goals - 3.2.21} \label{sec:long-term}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
	\item Create a measurable parametrization of student types based on response to tasks and teaching curricula
	\item Find a way to isolate and interpret policy building blocks that were learned on simple problems and were successfully transferred to hard problem
	% saliency maps are not good enough, something like SARFA (ICLR20, add state noise and measure actions relative and absolute)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments - 16.4.21} \label{sec:experiment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}
	%\centering
	\caption{Experiments}
	\label{experiment-table}
	\begin{tabular}{|l | l | l | l|} 
		\hline
		Parameters & Metrics & Observations     \\ \hline	
		
		\begin{tabular}{@{}c@{}} Agent: PPO \\ Task: Pendulum angle \\ Episodes: 1000 \\ Teacher: Random tasks \\ Extra: random start state \end{tabular}  
		&   
		&  \begin{tabular}{@{}c@{}}Poor results \\ Similar issues as MB \end{tabular}  
		\\ \hline
		
		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 200 \\ Teacher: Random tasks \\ Extra: random start state \end{tabular}  
		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
			50 eps: \{0:6, 40:10, 80:9, 120:108, 160:78, 200:14, 240:30, 280:7, 320:22\} \\
		100 eps: \{0:0, 40:10, 80:0, 120:16, 160:105, 200:3, 240:12, 280:19, 320:5\} \\ 
		150 eps: \{0:6, 40:5, 80:16, 120:257, 160:-8, 200:39, 240:4, 280:0, 320:5\} \\
		200 eps: \{0:2, 40:4, 80:0, 120:139, 160:25, 200:138, 240:5, 280:19, 320:-2\} \\\end{tabular}    
		&  \begin{tabular}{@{}c@{}}Better on tasks other than starting task \\ Worse on and near starting task \end{tabular}  
		    \\ \hline
		    
		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 200 \\ Teacher: ALP-GMM \\ Extra: random start state \end{tabular}  
		&  \begin{tabular}{@{}c@{}}Mean retur (last state)n, 3 trys, 10 angles \\ 
			50 eps: \{0:8, 40:24, 80:0, 120:115, 160:5, 200:61, 240:3, 280:27, 320:-8\} \\
			100 eps: \{0:0, 40:5, 80:-6, 120:-3, 160:23, 200:23, 240:2, 280:6, 320:8\} \\ 
			150 eps: \{0:12, 40:13, 80:40, 120:27, 160:-3, 200:9, 240:90, 280:8, 320:-2\} \\
			200 eps: \{0:3, 40:32, 80:0, 120:39, 160:-6, 200:40, 240:54, 280:23, 320:15\} \\\end{tabular}    
		&  \begin{tabular}{@{}c@{}}Good on more tasks \\ Lower best case \end{tabular}  
		\\ \hline
		
		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 100 \\ Teacher: custom \\
			Schedule: 25 repsx$(45^\circ,90^\circ,180^\circ,225^\circ)$ \\ Extra: random start state \end{tabular}  
		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
			25 eps: \{0:34, 40:26, 80:-3, 120:10, 160:47, 200:92, 240:27, 280:26, 320:7\} \\
			50 eps: \{0:12, 40:4, 80:21, 120:126, 160:127, 200:-4, 240:75, 280:12, 320:2\} \\ 
			75 eps: \{0:-6, 40:21, 80:8, 120:23, 160:-8, 200:-8, 240:11, 280:13, 320:12\} \\
			100 eps: \{0:3, 40:19, 80:11, 120:21, 160:-3, 200:130, 240:39, 280:18, 320:16\} \\\end{tabular}    
		&  \begin{tabular}{@{}c@{}}Some correlation between target and performance \\ Not as much as expected \end{tabular}  
		\\ \hline
   
   		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 100 \\ Teacher: constant \\
   			Schedule: 100 repsx$180^\circ$ \\ Extra: random start state \end{tabular}  
   		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
   			25 eps: \{0:8, 40:39, 80:13, 120:3, 160:27, 200:103, 240:25, 280:28, 320:-4\} \\
   			50 eps: \{0:23, 40:7, 80:36, 120:5, 160:9, 200:69, 240:6, 280:14, 320:-6\} \\ 
   			75 eps: \{0:4, 40:13, 80:7, 120:42, 160:14, 200:-8, 240:54, 280:14, 320:12\} \\
   			100 eps: \{0:15, 40:15, 80:4, 120:125, 160:-5, 200:2, 240:87, 280:10, 320:2\} \\\end{tabular}    
   		&  \begin{tabular}{@{}c@{}}No real relation between goal and learning \\ Very noisy \end{tabular}  
   		\\ \hline
		
	\end{tabular}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\bibliographystyle{plain}
\bibliography{library}

\end{document}