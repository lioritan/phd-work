\documentclass[letterpaper]{article}

% AAAI-style 2-per-page format, without the annoying bits
\setlength\topmargin{-0.25in} \setlength\oddsidemargin{-0.25in}
\setlength\textheight{9.0in} \setlength\textwidth{7.0in}
\setlength\columnsep{0.375in} \newlength\titlebox \setlength\titlebox{2.25in}
\setlength\headheight{0pt}  \setlength\headsep{0pt}
\flushbottom \sloppy

\pdfpagewidth=8.5in
\pdfpageheight=11in


\usepackage{times} 
\usepackage{helvet}  
\usepackage{courier}  
\usepackage{url}  
\usepackage{graphicx} 

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


\usepackage{multirow}
\usepackage{ctable}
\usepackage{color}
\usepackage{natbib}
\usepackage[normalem]{ulem}
\usepackage{romannum}
%\usepackage[style=authoryear]{biblatex}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black, % color for table of contents
	citecolor=black, % color for citations
	urlcolor=blue, % color for hyperlinks
	bookmarks=true,
}
\urlstyle{same}

\raggedbottom %nicer enumerate
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{hypothesis}{Hypothesis}[section]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Curriculum learning in RL}
\begin{document}
	\maketitle
	\begin{abstract}
		%Added most methods not focused on varied tasks
		%Added methods focused on task variety and exploration
		%Added short-term goals, blog, POMDP fomulation
		%Added parameter and code base sections, updated goals
		%Added model-based RL section
		%Added experiments section
		%Added ensemble agent ideas
		Added ML-CL section
	\end{abstract}

\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem formulations - 14.1.21} \label{sec:formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}
	A \textbf{task} (sometimes called an environment) is defined as a MDP $<S,A,P,R,\gamma>$,
	Where $S$ is a set of states, $A$ is a set of actions, $P:S\times A\times S\rightarrow [0,1]$ is a transition function, 
	$R:S\times A\rightarrow \mathbb{R}$ is a reward function, $\gamma\in[0,1]$ is a discount factor.
\end{defn}

\begin{defn} \label{defn:curriculum-pomdp}
	Given a (possibly infinite) set of possible tasks $\mathcal{T}$, a \textbf{curriculum} is the policy rollout of the POMDP $<S,A,P,R,\Omega,O, \gamma>$, where:
	\begin{itemize}
		\item $S$ is the (unobserved) set of student states. In the case of neural networks, $s_t$ is the vector of network parameters.
		\item $a_{t}^{\tau, k}\in A$ is the action of training the student on task $\tau\in \mathcal{T}$ for $k$ time steps.
		\item $P$ is the (unobserved) transition function between states. This is determined by the student's learning algorithm.
		\item $R:O\times A\times H \rightarrow \mathbb{R}$ is a reward function that evaluates student performance on a given task, possibly based on observation history $H$.
		\item $o_t$ is the teacher's student observation on the task. This is usually a `black box' observation, meaning $o_t\neq s_t$. Common examples of $o_t$ are the student's task trajectory or the student's discounted return.
	\end{itemize}	
	%\[
	%\max_{D^{\mathcal{T}}} \sum_{T \sim \mathcal{T}_target} {P_T^N dT}
	%\]
\end{defn}

A \textit{teacher algorithm} is defined by specifying the observation method $o_t$ and the teacher's reward $R$. It is often assumed that tasks in $\mathcal{T}$ have the same internal state and action space, with dynamics and task-level rewards varying between tasks.
Note that changing the start state or goal state for a task is also possible, since this is part of the specification of the task.
%There are no works (that I found) that define RL curricula with a control measure smaller than a full episode on a task.

\begin{defn} \label{defn:curriculum-pomdp-shaping}
	A \textbf{reward-shaped curriculum} is a variant curriculum where the actions are different:
	Given a set of tasks $\mathcal{T}=\{\tau|\tau=<\hat{S},\hat{A},\hat{P_i},\hat{R_i},\hat{\gamma}>\}$, the action $a_{t}^{\tau', k}$ is the action of training the student on task $\tau'=<\hat{S},\hat{A},\hat{P_i},R',\gamma'>$ for $k$ time steps, where $\gamma'\leq \hat{\gamma}$ and $R'(s,a)=\hat{R_i(s,a)}+R'(s,a)$ for a given shaped reward $R'$.
\end{defn}

An alternative formulation to the curriculum problem is embedded in game theory:
\begin{defn} \label{defn:curriculum-game-theory}
	A \textbf{collaborative curriculum} is defined according to the following sequential, asymmetric two-player game:
	\begin{itemize}
	\item $H$ is the game's history, and is available to both players.
	\item A teacher player's action is to provide a task $\tau$ and a time step budget $k$,
	\item A student player's action is to train on the given task, as well as other tasks in $H$, for a total time of $k$.
	\item The action-payoff function $v:\mathcal{T}\times \mathbb{N^+} \times H \rightarrow \mathbb{R}\times \mathbb{R}$ is the paired payoff for the teacher and student. 
	\end{itemize}
\end{defn}

We note that this game may be a zero-sum game (the teacher and student have opposed payoffs) or more cooperative, depending on the payoff function.
It is also important to note that if the student is required to perform the given task for all $k$ time steps, and the only goal is to maximize the teacher's payoff, we get a formulation that is equivalent to definition \ref{defn:curriculum-pomdp}, and in fact would be reasonable to solve using reinforcement learning.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Measuring task difficulty and curriculum - 23.2.21} \label{sec:difficulty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \cite{Matiisen2020}, the task probability is based on the absolute value of the reward improvement rate, measured by the difference between the trained task reward $r_t$ and the trained task reward last measured for the same task $r_{t'}$.
This method only works for a finite set of tasks. Tasks with high improvement or large performance decreases will be sampled more often.

In \cite{Feng2020}, a curriculum is designed for a specific task (Sokoban), and task difficulty is measured by a domain-specific measurement, the number of boxes to be moved to goal locations. Difficulty is increased based on the reverse of \cite{Matiisen2020} - if the success rate on a given difficulty level has not changed for several iterations, difficulty should be increased. This is because measuring task feasibility in Sokoban is hard, and therefore improvement rate can be zero even in cases where the agent does generalize well.

In \cite{Klink2020}, some task parameters are called a `context' (e.g. friction and goal location), and task probability for a given task is based on the student's value function estimate of the start state (including the context).
Contexts are assumed to come from a Gaussian distribution and the target distribution is assumed to be \textbf{known}.
The value function estimate is normalized by the context probability and by KL-divergence from the target distribution to discourage a large gap from the target environment. A KL-divergence constraint is also used to prevent large changes in the context distribution between updates.

In \cite{Jiang2020}, a curriculum is learned in order to re-try previously solved tasks in a manner that would hopefully lead to generalization. Task probability is proportional to the (discounted) TD-error for the last sampled trajectory - this is calculated as $ \frac{1}{T} | \sum_{t=0}^{T} {G_{t:T}}|$, where $G_{t:T}$ is the $t$-step discounted return. Tasks with high TD-error supposedly have higher learning potential, and are therefore more likely to be re-tried. 

In \cite{Portelas2019}, the authors define \textit{ALP-GMM}, a MAB (Multi-Armed Bandit) based teacher, that chooses the environment parameters based on absolute learning progress, the same thing as absolute improvement rate.
Unlike \cite{Matiisen2020}, task parameters are continuous, so they are separated into regions to allow comparisons to previous tasks. The major innovation of \textit{ALP-GMM} is in how to create the separation. A Gaussian Mixture Model is learned to fit task parameters to the improvement rate, giving a better connection between parameter space and improvement rate (compared to random task selection or hard-coded regions which is a method called \textit{RIAC}).
In \cite{Portelas2020}, a continuation of this approach with multiple students was proposed. In this classroom-like setting, the goal is to maximize task selection as well as student selection, meaning schedule must offer both a task and a student to give said task to, and optimizes over all students. \textit{ALP-GMM} is used for task selection, and a knowledge vector for each student is learned and used to select a task that will have good improvement rate for multiple similar students (similarity measured by knowledge vector).

In \cite{Narvekar2019}, a curriculum for grid-world environments is learned with task-specific operators used to specify task simplification (sub-goals and parameter-based simplifications). The curriculum itself measured time to convergence, i.e. the number of episodes until the policy ceases to change. A low time was considered more desirable.
In a continuation paper, \cite{Narvekar2020}, the authors improve on this by measuring convergence with actions instead of episodes, a policy is considered to converge if the number of steps required to reach a goal state is at most $\delta$ more than the optimal policy for a pre-defined $\delta$. The set of tasks is pre-defined, and both a task and a goal must be chosen.

In \cite{Gutierrez2020}, tasks are chosen from a given set based on a combination of relevance and a difference from previously chosen tasks. The relevance criterion is measured using policy entropy. It is assumed that the optimal policy for each task is known, and a set of \textit{validation tasks} similar to the test tasks is given.
A task is considered relevant if for at least one \textit{validation task}, we run the optimal policy in the task, measure mean entropy for the visited states, then train the policy $l$ steps and measure the same entropy, and a lower entropy is reached. Basically, a task is relevant if adapting it to a validation task makes it more deterministic.
Interestingly, they show that choosing all subtasks did not necessarily give better meta-learning.

In \cite{Justesen2018}, a task-specific difficulty measure is defined, and difficulty is increased on task success and decreased on task failure by a fixed amount.

In \cite{Jain2017}, tasks are chosen from a given set, and several selection criteria are defined. Among these criteria are:
\begin{itemize}
	\item Reward maximizing task - run a trajectory on each task, pick task with highest reward.
	\item Transfer maximizing task - before training, estimate task transferability (for example by calculating reward for task b after transferring policy trained on task a) for all task pairs as well as the target task. The curriculum is one that will maximize transferability from some initial task to the target task.
	\item Active reward maximization - assumes each task comes with a feature vector, and use active-learning regression to estimate the transferability between tasks
\end{itemize}
These curriculum were shown to converge more quickly and to a higher reward than learning on the target task directly.

In \cite{Reny2019}, a curriculum for experience re-play is learned. Unlike \cite{Jiang2020}, new tasks are created. This is done by choosing goal states (like HER) that will be difficult but achievable. In the paper, HER is considered a curriculum of choosing goal states estimated to be easy to achieve (via value function estimation), and this paper adds a term to try and pick goals that will have low Wasserstein (earth-mover) distance from the target goal distribution (which is assumed to be known). The choice of Wasserstein distance comes from a theorem assuming that close goals lead to close policies.

In \cite{Dennis2020}, environment parameters are chosen (tasks) to try and create hard but solvable tasks. An adversarial task generator creates a task, the student `protagonist' trains on it, then another `antagonist' agent train on it. The task generator tries to change the environment parameters to maximize `antagonist' return and minimize `protagonist' return (both were trained to convergence on the last environment). This is shown to be a nash-equilibrium between `protagonist' and `antagonist'. The intent is that as student adapts, environment must become more difficult.

In \cite{Racaniere2019}, goal states are chosen (tasks) to try and create hard but solvable tasks. 
This is done by trying to balance a trade-off between three metrics: feasibility, validity, and coverage.
Goal validity (likelihood of student reaching the goal) is measured by the Negative Log-Likelihood of generating a goal that's very close to a previously achieved goal.
Goal feasibility is measured using a binary classifier that tries to predict agent success given a goal. The teacher knows this prediction and uses it when generating environments.
If target goal distribution is known, this is also used to measure the teacher's loss.
The paper evaluated the performance of measures in complex environments, and showed that all metrics were useful.

In \cite{Al-Shedivat2017}, a curriculum is implicitly learned in a multi-player game by adversarial self-play. The paper itself assumes task distribution is given and is similar to existing meta-RL approach MAML (\cite{Finn2017a}).

In \cite{Zhang2020}, a goal state is chosen from a given parameter space. An ensemble of value function estimators (Q was chosen in the paper) is learned, and is used to estimate the standard deviation for given goal and start state. Goals with high standard deviation (=low confidence) are assumed to be good as they should be complex but not impossible.

In \cite{Florensa2017, Florensa2018}, a curriculum based on medium difficulty goal-based tasks is created, using an algorithm called \textit{GoalGAN}. This is done by trying to find a policy to maximize the probability of reaching the goal assuming some known test goal distribution (which was uniform in the paper). Do to so efficiently, goals are sampled from a smaller set of goals (called \textit{GOID}), with estimated success probability between some lower and upper bound. This is done using a GAN, with the discriminator separating goals in \textit{GOID} and goals not in \textit{GOID}, with policy evaluations used as labels (the return should be in the range). Does well compared to self-play and \textit{RIAC}.

A similar approach can be seen in \cite{Srinivasan2019}, where this notion of medium difficulty goals was applied to discrete state and action spaces. In their paper, a curriculum of goals is created by trying to choose start states within certain distance to the goal, but since random walk can be an issue in discrete spaces, demonstrations are used to determine both the start state and the curriculum, as each task is created by following an expert $i$ steps, with $i$ decreasing as the agent improves.

In \cite{Wohlke2020}, a curriculum is learned in order to maximize probability of reaching a known goal state given that the start state is selected uniformly. This is done by changing the distribution of start states to match the $L_2$ norm of the gradient for our performance measurement (probability of reaching the goal). This is estimated by picking a dimension and doing a zeroth-order approximation of the partial derivatives. This only works if the state space is euclidean, but that is quite common. This is quite similar to improvement rate for goal-based rewards.

In \cite{Akkaya2019}, a method to randomize environment parameters was proposed. In each iteration, environment parameters are uniformly sampled from a $d$-dimensional box. To adapt the box size, a random parameter is sampled from one of the edges of its current range, and an episode is trained to measure reward. Once enough data for a parameter is gathered, we measure the average performance and if it is above or below a set threshold, we change the box size. 

In \cite{Fang2020}, a combination of task feasibility and the closeness to the test environment are used to estimate progress. A good curriculum will give tasks closer to the test environment, but still feasible. Task generation is done with a GAN setup: A generator creates tasks (by choosing task parameters), and a discriminator tries to estimate `task progress' - the similarity of the trajectory for the generated task to a trajectory on the test task. To do this, rollouts on the test task are required. The generator loss is a combination of the discriminator error and the task's expected return.

In \cite{Milano2021}, task parameters are chosen from a finite set for use in evaluations of the gradient in an Evolutionary Strategies algorithm. After a warm-up period where tasks are chosen randomly, possible parameters are organized into subsets based on average reward, and evaluations pick randomly from a subset based on this ``difficulty. Notably, they show empirically that dividing subsets into difficulties non-uniformly is preferable, using $x^2$ and $x^3$ as the difficulty measures, thus creating larger subsets of easy tasks and smaller subsets of hard tasks. 

In \cite{Li}, if a goal-conditioned task is not achieved, random search is used to try and find a sub-goal such that the agent can easily reach the original goal from the sub goal. Once data has been collected on the new trajectory to the sub-goal, the agent is updated.

In \cite{Won2019}, a PPO-based agent learns a policy that is robust (to a degree) to changes in body type parameters by giving a curriculum where tasks with low expected rewards are sampled more often. 
The expected reward for a task is estimated from the value function estimates for that task. The resulting trained policy can also adapt to online changes to the body parameters, since the parameters are included in the state representation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ensuring task variety - 24.1.21} \label{sec:variety}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \cite{Jiang2020}, a probability bonus is given proportional to the number of tasks seen since a given task was sampled, thus forcing the task to eventually be re-played.

In \cite{Gutierrez2020}, tasks are chosen from a given set based on a combination of relevance and a difference from previously chosen tasks. Difference is measured by the mean KL-divergence of the optimal policies for a pre-defined set of states. A task is different from chosen tasks if the KL-divergence of its optimal policy is sufficiently large.

In \cite{Reny2019}, an optimization constraint for choosing a goal state is imposed, forcing tasks to come from different training trajectories.

In \cite{Racaniere2019}, goal states are chosen (tasks) to try and create hard but solvable tasks. 
This is done by trying to balance a trade-off between three metrics: feasibility, validity, and coverage.
Goal coverage is measured by the entropy of the teacher over random goals.

In \cite{Mehta2019}, environment `difficulty' is estimated by learning a discriminator that tries to separate rollouts from generated environments and rollouts in a given reference environment. The discriminator output is used as the teacher's reward for the generated environment, so environments that are very different from the reference environment are rewarded. 
The teacher agent tries to choose environment parameters to maximize this reward, and therefore encourage variety. The teacher agent uses a method called \textit{SVPG}, Stein Variational Policy Gradient, a method similar to \textit{A2C} but with multiple policy parameters (called particles), and a trade-off between parameter reward and difference between them (measured with a kernel function).
Expanding on the previous apporach, \cite{Raparthy2020} create a method with two teacher agents: one uses a discriminator to force environment parameters away from the reference environment like in \cite{Mehta2019}, and the other that chooses a task from the given distribution in an adversarial manner: the task selector uses the student policy and a stopping policy to act in the reference environment and find a goal state. The student must then reach that goal in the generated environment. To update the stopping policy, the adversary is rewarded for finding a goal in few actions that also requires many actions to reach, and is therefore hopefully hard but solvable. 
In a continuation paper, \cite{Mehta2020}, a meta-RL algorithm is used to adapt the student policy to the generated environment, and the discriminator tries to predict whether the trajectory came from the policy before or after adaptation, thus encouraging tasks with large changes during adaptation. Notably this removes the need to have reference environments.

In \cite{Sukhbaatar2017}, a goal-choosing agent acts in the environment to try and find states that will be difficult to solve. This was the direct inspiration to \cite{Raparthy2020}, and where its stopping policy notion comes from. In a later paper, \cite{OpenAI2021} expanded on this approach by: filtering goal states to goals not solved by the student, the teacher having a set number of time steps to propose goals, and the student measured on achieving one of five generated goals. Additionally, a custom reward function to the teacher, and a clipped behavior cloning loss are used to improve stability.

In \cite{Gupta2018}, the environment parameters are not given directly to the trained student, but are used as latent variables to generate an environment. A discriminator is learned to try and predict the latent task that was used to generate a given trajectory, with high confidence prediction yielding high reward. Since a high variety between tasks leads to good prediction, this helps with variety.

In \cite{Jabri2019}, a curriculum to learn an MDP without a reward is proposed. The teacher agent chooses environment parameters as latent variables, and gives state reward $r_z(s) = log q(s|z) - log q(s)$, the information gain of the latent variable. The teacher agent uses the sampled trajectories to improve this $q$, via discriminative clustering.

In \cite{Kaddour2020}, tasks are characterized by a learned low-dimensional task embedding. This is done by learning an encoder on trajectories that tries to maximize likelihood of parameters and rewards given the observations. To pick a task, a point in the latent space is picked to maximize some utility.
The utility used in the paper was picking maximal surprise (self-information) - a latent point with low likelihood according to the estimate task distribution.

In \cite{Wang2019}, \textit{POET}, a genetic algorithm for varied task generation is proposed. A set of environments and a set of agents are maintained, and at each step, environment parameters are randomly changed (with a ranking based on euclidean distance of parameters from already existing environments), each agent is trained on each environment (using a single step of ES - evolution strategies), and if an environment is challenging (mean reward at least $x$ and at most $y$), it is kept.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method summary - 1.6.21} \label{sec:summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

See table \ref{methods-table}

\begin{table*}
%\centering
\caption{Comparison of approaches}
\label{methods-table}
\begin{tabular}{|l | l | l | l  | l|l|} 
	\hline
	Paper & good paper & Difficulty measure & variety measure  & Controllable elements     \\ \hline	
	\cite{Matiisen2020}& seminal & improvement rate & none & task selection  \\ \hline
	\cite{Feng2020}& & domain-specific & none & task parameters  \\ \hline
	\textit{SPRL} \cite{Klink2020}& multi-env & $V(s_0)$ & none & task parameters \\ \hline
	\cite{Jiang2020} & multi-env & improvement potential (error) & uncertainty bonus & task selection  \\ \hline
	\textit{ALP-GMM} \cite{Portelas2019} & &  improvement rate & none & task parameters \\ \hline
	\cite{Portelas2020} & & improvement rate & none & task parameters, student  \\ \hline
	\cite{Narvekar2019} & & fast policy convergence & none & task parameters \\ \hline
	\cite{Narvekar2020} & curriculum works & fast policy convergence & none & task selection, goal state \\ \hline
	\cite{Gutierrez2020} & good theory & validation tasks & Policy KL-divergence & task selection \\ \hline
	\cite{Justesen2018} & & $\pm$ difficulty, on success/fail & none & task parameter \\ \hline
	\cite{Jain2017} & & transfer improvement & none & task selection \\ \hline
	\cite{Reny2019} & & $V(s_0)-diff(g,g*)$ & Constrained goals & start state, goal state \\ \hline
	\textit{PAIRED} \cite{Dennis2020} & Unsupervised SoTA & adversarial reward & none & task parameters \\ \hline
	\cite{Racaniere2019} & multi-env, deepmind & prob of task success & teacher entropy & goal state \\ \hline
	\cite{Al-Shedivat2017} & & self-play & none & opponent agent \\ \hline
	\cite{Zhang2020} & multi-env & ensemble confidence & none & goal state \\ \hline
	\cite{Wohlke2020} & & $\sim V'(s_0)$ & none & start state \\ \hline
	\cite{Fang2020} & multi-env, ok theory & improvement rate \& test diff & none (GAN helps) & task parameters \\ \hline
	\textit{GoalGAN} \cite{Florensa2018} & seminal & return in range & none (GAN helps) & goal state \\ \hline
	\cite{Mehta2019} &  & none & learned trajectory difference & task parameters \\ \hline
	\cite{Raparthy2020} & & adversarial reward & learned trajectory difference & task parameters \\ \hline
	\cite{Sukhbaatar2017} & multi-env, ok theory & adversarial reward & None & task parameters \\ \hline
	\cite{OpenAI2021} & openAI, multi-env & adversarial reward & new goals & task parameters \\ \hline
	\cite{Mehta2020} & multi-env, metaRL & none & learned adaptation difference & task parameters \\ \hline
	\cite{Gupta2018} & & none & learned state difference & task parameters \\ \hline
	\cite{Jabri2019} & & none & trajectory clustering & task parameters \\ \hline
	\cite{Srinivasan2019} & & demonstrated trajectories & none & start state \\ \hline
	\cite{Akkaya2019} & & return, fixed change size & none & task parameters \\ \hline
	\cite{Kaddour2020} & multi-env, ok theory & none & surprise (low likelihood) & task parameters \\ \hline
	\textit{POET} \cite{Wang2019} & & return in range & distance in parameter space & task parameters \\ \hline
	\cite{Milano2021} & & average reward & none & task parameters \\ \hline
	\cite{Li} & & solved before & none & sub-goal \\ \hline
	\cite{Won2019} & & expected $V(s|p)$ & none (sampling) & task parameters \\ \hline
	
\end{tabular}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Curriculum learning in ML - 18.6.21} \label{sec:CLML}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To begin, let us discuss ``self-paced learning'' (SPL), a common form of easy-to-hard curriculum.
SPL refers to curricula where each example is weighted based on the model error in a specific manner.
In SPL, an EM algorithm is used to jointly optimize example weights $v_i$ and model parameters $w$.

$w$ is optimized by minimizing weighted loss for any loss function $l$.
$v$ is optimized by minimizing $v^*(l,\lambda)=argmin_{v} vl + f(v, \lambda)$, where $\lambda$ is a monotonically increasing ``age'' parameter, and $f$ is the self-paced regularizer function, that must satisfy:
\begin{enumerate}
	\item $f(v, \lambda)$ is convex with respect to $v, l$
	\item $v^*(l,\lambda)$ monotonically decreases as l increases with limits at 0 and infinity (easy samples have more weight)
	\item $v^*(l,\lambda)$ monotonically increases as $\lambda$ increases with limits at 0 and infinity (as training progresses, more examples should be included)
\end{enumerate}

% Optimization math
In \cite{Meng2017}, a connection between SPL and majorization minimization (MM) methods is established, and using existing theory for MM methods, \emph{they derive a proof of convergence}. They also use this theory and connections between the self-paced regularizer and non-convex regularized penalties to show that SPL methods work well in problems with outliers or noisy examples, since the SPL regularizer limits the effect of examples with high loss on the early training. This happens since the conditions mean that high-loss examples have constant loss, and thus gradient methods have zero gradient on these examples and they do not impact training.

In a later paper, \cite{Liu2018}, a connection between SPL and concave conjugates is shown ($v$ is the concave conjugate for $f$).
This fact is used to reduce the condition set on the regularizer to:
\begin{enumerate}
	\item $f(v, \lambda)$ is strictly convex with respect to $v$
	\item $f(v, \lambda)$ is lower semi-continuous with respect to $v$
	\item $f$ covers all of $[0,1]$
\end{enumerate}
These simplified conditions help designing regularizers, as any regularizer satisfying these for constant $\lambda$ can be used as $f(v, \lambda)=\lambda f(v)$. This simpler condition can be used for curriculum design such as partial order, grouping of examples with similar losses and more.

%PAC-BAYES
In a series of papers, \citet{Pentina2015} show that the similarity between tasks may be an important factor to overall performance. In both a lifelong learning setting (like curriculum learning, but the input tasks are chosen randomly, and the goal is few-shot learning on new tasks) and multi-task learning (like lifelong learning, but with a single learner), the empirical risk can be bounded by a combination of the empirical training error and a factor of task dependence - either the size of the dependency set or the mean difference between tasks (this assumes task weights correspond to task similarity, unlikely to happen for complex models or complex task dependencies)

%PAC-BAYES
Two very recent papers extend these results to meta-learning, \cite{Ding2021} and \cite{Cioba2021}.
In the first one, they relax the assumption that the number of examples in the target tasks is similar to the number of examples in training tasks, leading to a bound that corresponds to the (small) number of examples in the target task, but introduces an error term related to the gap between training and target tasks. Using a subsampling method a tighter bound on the risk can be achieved (this means fewer examples are used for training, all examples are used for risk estimation).

In the second paper, they discuss whether it is better to have many tasks with few labels or few tasks with many labels. They show that for MAML with mixed linear regression, there is an optimal trade-off between number of tasks and labels per task for uniform tasks. They also show empirical experiments suggesting that a hard task requires more data, and that it is preferable to assign slightly more data to easy tasks.

% optimization math with some PAC-bayes
Another area of research that recently made use of curricula is semi-supervised learning, where \cite{Gong2019} show that a easy-to-hard curriculum can help with label propagation. 
It is assumed that every example contains $v$ modalities (feature groups) that correspond to $v$ subgraphs of related data points.
A difficulty metric based on distances is constructed, and they prove that an algorithm with a committee of teachers based on each modality to decide how to propagate labels converges in linear time.
They also prove that choosing by the weighted voting has bounded error risk (vs random choice) proportional to the difficulty threshold - if only easy examples are allowed, the error risk is low, but performance is worse since fewer examples are considered. This means that weighted voting is good.

% geometry
In \cite{Weinshall2018, Weinshall2020}, the authors define two notations: the global difficulty of an example is the loss for that example given an optimal hypothesis, and the local difficulty of an example is the loss given the current hypothesis.
For a linear hypothesis class under a convex loss, they prove that a curriculum ranked by low global difficulty leads to faster convergence. They also prove that, given identical global difficulty, \emph{high} local difficulty is preferable. They show this empirically by using a pre-trained network as an estimator of global difficulty.
This result is empirically reinforced by a later paper (\cite{Hacohen2019}), showing that the easy-to-hard curriculum improves transfer accuracy, and a hard-to-easy curriculum harms transfer accuracy.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{The Meta-Curriculum problem - 24.6.21} \label{sec:mcl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{defn} \label{defn:mcl}
%	A \textbf{meta-curriculum problem} is characterized by:
%	\begin{itemize}
%		\item A task probability distribution $\mathcal{T}$, $(\mathcal{D}_i, m_i)\sim \mathcal{T}$. We will mark $S_{i}=\{(x_j, y_j)\sim \mathcal{D}_i\}_{j=1}^{m_i}$
%		\item A Hypothesis class $H$
%		\item A loss measure $\mathcal{L}: S\times H \rightarrow \mathbb{R}$
%		\item A training set $S=\{S_1,...,S_n\}$
%	\end{itemize}
%
%	Given these, we would like to find a hyper-prior $\mathcal{P}_S$ and learn a hyper-posterior $\mathcal{Q}$ to minimize the multi-task error $\mathcal{L}(\mathcal{Q}, \mathcal{T}) \triangleq \mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{(\mathcal{D}, m)\sim \mathcal{T}} \mathbb{E}_{S\sim \mathcal{D}} [\mathcal{L}(Q(S, P), \mathcal{D})]$ \footnote{$Q$ is a base learner mapping data and prior over $H$ to posterior over $H$}. 
%	To approximate this, we use the empirical multi-task error $\mathcal{\hat{L}}(\mathcal{Q}, S_1,...,S_n) \triangleq \mathbb{E}_{P\sim \mathcal{Q}} [\frac{1}{n} \sum_{i=1}^{n} \mathcal{\hat{L}}(Q(S_i, P), S_i)]$
%\end{defn}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notable empirical results in CL - 24.6.21} \label{sec:empirical-cl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In \cite{Anonymous2021}, the effect of several difficulty-based curricula is checked.
Task difficulty is checked by one of three methods:
\begin{enumerate}
	\item A reference loss (pre-trained model)
	\item A novel measure called learned iteration, the first iteration where the model output was correct
	\item The c-score, an approximation of generalization accuracy for an example, measured by the expected accuracy for a model trained on n random examples (that are not the measured example)
\end{enumerate}
Additionally, several pacing functions for the size of training sets were examined.
For large datasets with long training times, curricula had no effect, and pacing was marginally useful to improve convergence rate. For short training times or datasets with noisy labels, curricula showed a significant improvement in performance, especially easy-to-hard curricula.

In \cite{Hacohen2019}, A similar experiment is performed with a reference loss function. Results here show a significant improvement for short training times


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notable empirical results in Meta-ML - 24.6.21} \label{sec:empirical-meta}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For meta learning, a good reference point is MAML (\cite{Finn2017})), showing a major improvement over previous methods for few-shot transfer.
These results were incrementally improved by REPTILE (\cite{Nichol2018}), by using a Polyak update for the model parameters instead of directly applying the gradient.

Recent approaches (\cite{Saglietti2021, Khodak2019}) that use the data or other expert knowledge to estimate task hardness for meta-learning yield slightly better results for zero-shot and one-shot transfer, with comparable few-shot transfer. 
Both methods do not show a significant improvement over existing methods.
Mentor-Net (\cite{Jiang2017}) is another paper that suggests learning the difficulty of tasks and using it for a curriculum. This method does improve on existing approaches, but this may be due to having a larger network.
Interestingly, Mentor-Net shows a significant improvement over a non-curriculum based approach with similar network sizes, for a setting with noisy labels. This suggests that results from traditional CL literature also hold for the meta-learning case.

Some very recent papers (\cite{Bateni2020, Tian2020}) show that a good representation is more meaningful for effective meta learning, yielding results that are far better than MAML and REPTILE, mostly by using a deeper network and some minor tweaks such as a different loss aggregation or network distillation. 
These methods suggest that the current bottleneck for better meta learning is the representation, and that curricula that impose an effective order on complex representations may be highly useful.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Math for PAC-Bayesian meta learning - 19.7.21} \label{sec:bayes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definitions and previous results}

Let $\mathcal{T}$ be an unknown task distribution, let $\mathcal{P}$ be a \emph{hyper-prior} over distributions of hypotheses (data-free). Let $\tau_i=(\mathcal{D}_i, S_i)~\mathcal{T}$ be a set of tasks ($i=1,...,n$).

We assume for now that $\tau_i$ are drawn i.i.d. from $\mathcal{T}$.

We wish to bound the learned \emph{hyper-posterior} $\mathcal{Q}$, and its error - 
$$\mathcal{L}(\mathcal{Q}, \mathcal{T})=\mathbb{E}_{P\sim \mathcal{Q}}[\mathbb{E}_{(\mathcal{D}, m)\sim \mathcal{T}}[\mathbb{E}_{S\sim \mathcal{D}^m}[\mathcal{L}(Q(S,P), D)]]]$$

Using the result from Rothfuss (TODO: cite),by applying the change of measure inequality, for any $\lambda>0$,
$$\mathcal{L}(\mathcal{Q}, \mathcal{T})\leq \mathcal{L}(\mathcal{Q}, \mathcal{D}_1,...,\mathcal{D}_n) + \frac{1}{\lambda} KL(\mathcal{Q}||\mathcal{P})+\Upsilon^{\text{\Romannum{2}}}(\lambda)$$

Where $\Upsilon^{\text{\Romannum{2}}}(\lambda)=\frac{1}{\lambda}ln \mathbb{E}_{P\sim \mathcal{P}} [e^{\frac{\lambda}{n}\Sigma_{i=1}^{n}\mathbb{E}_{(D,S)\sim \mathcal{T}}[\mathcal{L}(Q(P,S),D)- \mathcal{L}(Q(P, S_i), \mathcal{D}_i)]}]$

\subsection{Math for appendix}

We note that for any $P\sim \mathcal{Q}$, $P$ is a data-dependent prior (and $P\sim \mathcal{P}$ is a data-free prior).
From Rivasplata et al (TODO: cite), we know that for each task $i$, given a data-dependent prior $P$,
$$\Pr_{S_i\sim D_i}\Bigl ( \mathcal{L}(\mathcal{Q}, D_i)\leq \mathcal{L}(\mathcal{Q}, S_i) + KL(Q(P,S_i)||P) + ln(\frac{\xi_i(P)}{\delta_i}) \Bigr )\geq 1-\delta_i$$

We will mark $Q_i:=Q(P,S_i)$ for comfort.
By using a union bound (with a known identity connecting union and intersection), and by choosing $\delta_i=\frac{\delta}{n}$ for some $\delta>0$, we get:

With probability at least $1-\delta$ over the choice of training data $S1,..,S_n$,
$$\mathbb{E}_{P\sim \mathcal{Q}} \bigl [ \frac{1}{n}\Sigma_i \mathcal{L}(\mathcal{Q}, D_i) \Bigr ] \leq \mathbb{E}_{P\sim \mathcal{Q}} \bigl [ \frac{1}{n}\Sigma_i \hat{\mathcal{L}}(\mathcal{Q}, S_i) + \frac{1}{n}\Sigma_i KL(Q_i||P) + \frac{1}{n}\Sigma_i ln(\frac{\xi_i(P)}{\delta/n}) \Bigr ]$$


By definition of $ \mathcal{L}(\mathcal{Q}, \mathcal{D}_1,...,\mathcal{D}_n)$,  $ \hat{\mathcal{L}}(\mathcal{Q}, S_1,...,S_n)$ we get:

$$\mathcal{L}(\mathcal{Q}, \mathcal{D}_1,...,\mathcal{D}_n) \leq \hat{\mathcal{L}}(\mathcal{Q}, S_1,...,S_n) + \frac{1}{n}\Sigma_i \mathbb{E}_{P\sim \mathcal{Q}} \bigl [KL(Q_i||P) \Bigr ] + \mathbb{E}_{P\sim \mathcal{Q}} \bigl [ \frac{1}{n}\Sigma_i ln(\frac{\xi_i(P)}{\delta/n}) \Bigr ]$$

Developing this further,

$$ \mathbb{E}_{P\sim \mathcal{Q}} \bigl [ \frac{1}{n}\Sigma_i ln(\frac{\xi_i(P)}{\delta/n}) \Bigr ] = \mathbb{E}_{P\sim \mathcal{Q}} \bigl [ \frac{1}{n}ln(\frac{\Pi_i\xi_i(P)}{(\delta/n)^n}) \Bigr ] = \mathbb{E}_{P\sim \mathcal{Q}} \bigl [ ln(\frac{(\Pi_i\xi_i(P))^{\frac{1}{n}}}{(\delta/n)}) \Bigr ]$$

We note that by the definition of $xi_i$,

$$\mathbb{E}_{P\sim \mathcal{Q}} \bigl [ ln(\frac{(\Pi_i\xi_i(P))^{\frac{1}{n}}}{(\delta/n)}) \Bigr ] = 
\mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{h\sim P} \bigl [ ln(\frac{(\Pi_i e^{\mathcal{L}(h, \mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)})^{\frac{1}{n}}}{(\delta/n)}) \Bigr ] = 
\mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{h\sim P} \bigl [ ln(\frac{ e^{\frac{1}{n}\Sigma_i\mathcal{L}(h, \mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)}}{(\delta/n)}) \Bigr ]$$

Using Jensen's inequality,

$$\mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{h\sim P} \bigl [ ln(\frac{ e^{\frac{1}{n}\Sigma_i\mathcal{L}(h, \mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)}}{(\delta/n)}) \Bigr ] \leq ln \mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{h\sim P} \bigl [ \frac{ e^{\frac{1}{n}\Sigma_i\mathcal{L}(h, \mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)}}{(\delta/n)} \Bigr ] = ln(\frac{n}{\delta})+ ln \mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{h\sim P} \bigl [ e^{\frac{1}{n}\Sigma_i\mathcal{L}(h, \mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)} \Bigr ]$$

\subsection{Summary}

Putting it all together, for any $\lambda>0$ with probability at least $1-\delta$,

$$\mathcal{L}(\mathcal{Q}, \mathcal{T}) \leq \hat{\mathcal{L}}(\mathcal{Q}, S_1,...,S_n) + \frac{1}{\lambda} KL(\mathcal{Q}||\mathcal{P}) + \frac{1}{n}\Sigma_i \mathbb{E}_{P\sim \mathcal{Q}} \bigl [KL(Q_i||P) \Bigr ] + ln(\frac{n}{\delta})+ ln \mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{h\sim P} \bigl [ e^{\frac{1}{n}\Sigma_i\mathcal{L}(h, \mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)} \Bigr ] +\Upsilon^{\text{\Romannum{2}}}(\lambda)$$

Using the same method on the change of measure, we can also get:

$$\mathcal{L}(\mathcal{Q}, \mathcal{T}) \leq \hat{\mathcal{L}}(\mathcal{Q}, S_1,...,S_n) + \frac{1}{\lambda} KL(\mathcal{Q}||\mathcal{P}) + \frac{1}{n\beta}\Sigma_i \mathbb{E}_{P\sim \mathcal{Q}} \bigl [KL(Q_i||P) \Bigr ] + ln(\frac{n}{\delta})+ \frac{1}{\beta}ln \mathbb{E}_{P\sim \mathcal{Q}} \mathbb{E}_{h\sim P} \bigl [ e^{\frac{\beta}{n}\Sigma_i\mathcal{L}(h, \mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)} \Bigr ] +\Upsilon^{\text{\Romannum{2}}}(\lambda)$$

Whereas the bound in Rothfuss is (for any $\lambda,\beta>0$):

$$\mathcal{L}(\mathcal{Q}, \mathcal{T}) \leq \hat{\mathcal{L}}(\mathcal{Q}, S_1,...,S_n) + (\frac{1}{\lambda}+\frac{1}{n\beta}) KL(\mathcal{Q}||\mathcal{P}) + \frac{1}{n\beta}\Sigma_i \mathbb{E}_{P\sim \mathcal{Q}} \bigl [KL(Q_i||P) \Bigr ] + \frac{1}{\beta}ln\mathbb{E}_{P\sim \mathcal{P}}\mathbb{E}_{h\sim P}\bigl [ e^{\frac{\beta}{n}\Sigma_i\mathcal{L}(h,\mathcal{D}_i)- \hat{\mathcal{L}}(h, S_i)} \bigr ] +\Upsilon^{\text{\Romannum{2}}}(\lambda)$$

We also note that the term $ln(\frac{n}{\delta})$ is the result of a union bound, and it is possible to replace it with $\Sigma_i ln(\frac{1}{\delta_i})$. This union bound may be overly pessimistic as it assumes that the \emph{hyper-posterior} is independently bounded for each task. If tasks are not independent it may be possible to use that to derive a tighter bound. We also note that the per-task moment term is now with respect to $\mathcal{Q}$ and not $\mathcal{P}$. 

\subsection{Analysis}

If we assume that the per-task moment is not worse in expectation under $\mathcal{Q}$ compared to $\mathcal{P}$, we get: 

$$\mathcal{L}(\mathcal{Q}, \mathcal{T}) \leq \hat{\mathcal{L}}(\mathcal{Q}, S_1,...,S_n) + \frac{1}{\lambda} KL(\mathcal{Q}||\mathcal{P}) + \frac{1}{n\beta}\Sigma_i \mathbb{E}_{P\sim \mathcal{Q}} \bigl [KL(Q_i||P) \Bigr ] + ln(\frac{n}{\delta})+ 1+\frac{1}{\beta}ln K +\Upsilon^{\text{\Romannum{2}}}(\lambda)$$

compared to 

$$\mathcal{L}(\mathcal{Q}, \mathcal{T}) \leq \hat{\mathcal{L}}(\mathcal{Q}, S_1,...,S_n) + (\frac{1}{\lambda}+\frac{1}{n\beta}) KL(\mathcal{Q}||\mathcal{P}) + \frac{1}{n\beta}\Sigma_i \mathbb{E}_{P\sim \mathcal{Q}} \bigl [KL(Q_i||P) \Bigr ] + 1+\frac{1}{\beta}ln(K) +\Upsilon^{\text{\Romannum{2}}}(\lambda)$$

(Since $\frac{1}{\beta}ln(e^{\beta}K)=1+\frac{1}{\beta}ln(K)$)

Comparing the two bounds is difficult, since the latter depends on $\beta$, commonly a function of $m$ (the size of each task), but our new bound seems inferior.

\subsection{Gibbs analysis}

Let us assume that $P\sim \mathcal{Q}$ behaves as follows:
$$P_{S_i}(h)\propto e^{-\gamma \hat{L}(h,S_i)}P_0(h)$$
Where $P_0\sim \mathcal{P}$. One method to achieve this is to learn a prior per task, and given a task to use the appropriate prior (this assumes the task is known). This is a naive algorithm that essentially ignores the meta-learning setting, but still gives us an interesting bound:

From Rivasplata (cite), for each specific task, with probability at least $1-\delta_i$,
$$L(P,D_i)-L(P,S_i)\leq \frac{1}{\sqrt{m_i}}\bigl ( KL(Q_i||P) + 2(1+\frac{2\gamma}{\sqrt{m_i}})+ln(\frac{1+\sqrt{e}}{\delta_i}) \bigr )$$

Using a union bound, with probability at least $1-\delta$,
$$L(P,\{D_i\})-L(P,\{S_i\})\leq \frac{1}{\sqrt{m}}\bigl ( \Sigma_i KL(Q_i||P) + 2(1+\frac{2\gamma}{\sqrt{m}})+ln(\frac{m(1+\sqrt{e})}{\delta}) \bigr )$$

Combining with Rothfuss, with $\lambda=n$ we get:

$$L(P,\mathcal{T})-L(P,\{S_i\})\leq \frac{1}{n}KL(\mathcal{Q}||\mathcal{P}) + \frac{1}{\sqrt{m}}\Sigma_i KL(Q_i||P) + 2(\frac{1}{\sqrt{m}}+\frac{2\gamma}{m})+\frac{1}{\sqrt{m}}ln(\frac{m(1+\sqrt{e})}{\delta}) + \Upsilon^{\text{\Romannum{2}}}(n)$$
	
We note that under mild assumptions on the learner $Q$, we get that as $m\rightarrow\infty$, all terms dependant on $m$ go to zero, and as $n\rightarrow\infty$, all terms dependant on $n$ go to zero.

\subsection{Task dependencies - 22.7.21}

Suppose we know something more about tasks. Can we devise a better bound?
As an initial case, let's start with an easy case.

Let us assume that the goal task is known and was seen during training, that is, we know that we wish to bound $\mathcal{L}(\mathcal{Q}, D_i)=E_{P\sim \mathcal{Q}}E_{S\sim D_i^{m_i}}\mathcal{L}(Q(P, S), D_i)$, where we already have another dataset $S_i\sim D_i^{m_i}$. We will mark $\{D_j\}:=\{D_1,...,D_{i-1},D_{i+1},...,D_n\}, \{S_j\}:=\{S_1,...,S_{i-1},S_{i+1},...,S_n\}$ for ease of notations.

\subsubsection*{Approach 1}
Let us try to bound
$$E_{P\sim \mathcal{Q}}\bigl [E_{S\sim D_i^{m_i}}\mathcal{L}(Q(P, S), D_i)-\hat{\mathcal{L}}(Q(P, S_i), S_i)\bigr ]$$

Using the change of measure inequality, we get

$$E_{P\sim \mathcal{Q}}E_{S\sim D_i^{m_i}}\mathcal{L}(Q(P, S), D_i)\leq E_{P\sim \mathcal{Q}}\hat{\mathcal{L}}(Q(P, S_i), S_i) + KL(\mathcal{Q}||\mathcal{P})+lnE_{P\sim \mathcal{P}}\bigl [e^{E_{S\sim D_i^{m_i}}\mathcal{L}(Q(P, S), D_i)-\hat{\mathcal{L}}(Q(P, S_i), S_i)}\bigr ]$$

To minimize this, we would like to balance minimizing $E_{P\sim \mathcal{Q}}\hat{\mathcal{L}}(Q(P, S_i), S_i)$ and $KL(\mathcal{Q}||\mathcal{P})$. This is achieved by using only $S_i$.

\subsubsection*{Approach 2}
Let us try to bound
$$E_{P\sim \mathcal{Q}}\bigl [E_{S\sim D_i^{m_i}}\mathcal{L}(Q(P, S), D_i)-\frac{1}{n}\Sigma_k\hat{\mathcal{L}}(Q(P, S_k), S_i)\bigr ]$$

To minimize here, we would pick $\mathcal{Q}$ that is proportional to the loss of each trained hypothesis on $S_i$, resulting in high weight to tasks that are very different from $S_i$. While this is not useful, it is quite interesting, as it gives us a method to learn to perform well on tasks that are very dissimilar to $i$ by testing on task $i$. We also note that we can expect to have a smaller moment term here.

Intuitively it may be better to use 
$$E_{P\sim \mathcal{Q}}\bigl [E_{S\sim D_i^{m_i}}\mathcal{L}(Q(P, S), D_i)- \frac{1}{n}\Sigma_k\frac{\hat{\mathcal{L}}(Q(P, S_k), S_i)}{KL(Q(P, S_k)||Q(P, S_i))+\epsilon} \bigr ]$$

This balances the loss of a task with regards to task $i$ with how different it is from task $i$. A task with low distance will have more weight, but lower loss. A task with high distance will have a higher loss.

\subsubsection*{Approach 3 - 22.7.21}

We will try to use various identities to get a more useful bound for approach 1. To do so, we assuem that $Q$ chosen as a gibbs learner with parameter $\gamma$, so: $$KL(Q_i||Q_k)=\int_h ln\frac{Q_i(h)}{Q_k(h)}dQ_i=\int_h ln\frac{P(h)\frac{e^{-\gamma L(h,S_i)}}{Z_{\gamma}(P,S_i)}}{P(h)\frac{e^{-\gamma L(h,S_k)}}{Z_{\gamma}(P,S_k)}}dQ_i=\int_h ln\frac{e^{-\gamma L(h,S_i)} Z_{\gamma}(P,S_k)}{e^{-\gamma L(h,S_k)} Z_{\gamma}(P,S_i)}dQ_i$$

From this, we get: 
$$KL(Q_i||Q_k)=\int_h \bigl [ ln\frac{e^{-\gamma L(h,S_i)}}{e^{-\gamma L(h,S_k)} }+ln \frac{Z_{\gamma}(P,S_k)}{Z_{\gamma}(P,S_i)} \bigr ] dQ_i=\int_h \bigl [ \gamma L(h,S_k)-\gamma L(h,S_i) +C(S_i,S_k,P) \bigr ] dQ_i$$

From this last expression, we get that:
$$KL(Q_i||Q_k)=\gamma E_{h\sim Q_i}[L(h,S_k)-L(h,S_i)]+C(S_i,S_k,P)=\gamma (\hat{\mathcal{L}}(Q_i,S_k)-\hat{\mathcal{L}}(Q_i,S_i))+C(S_i,S_k,P)$$

Where $C(S_i,S_k,P)=ln\frac{Z_{\beta}(P,S_k)}{ Z_{\beta}(P,S_i)}\leq ln E_{h\sim P} \bigl [e^{\gamma (L(h,S_i)-L(h,S_k))} \bigr ]$ due to the definition of covariance.

By moving $\hat{\mathcal{L}}(Q_i,S_i)$ we get for each $k$:

$$ \hat{\mathcal{L}}(Q_i,S_i)\leq \hat{\mathcal{L}}(Q_i,S_k) - \frac{1}{\gamma} KL(Q_i||Q_k) + \frac{1}{\gamma}ln E_{h\sim P} \bigl [e^{\gamma (L(h,S_i)-L(h,S_k))} \bigr ]$$

By averaging over all tasks we get:

$$ \hat{\mathcal{L}}(Q_i,S_i)\leq \frac{1}{n}\Sigma_k\hat{\mathcal{L}}(Q_i,S_k) - \frac{1}{n\gamma} \Sigma_k KL(Q_i||Q_k) + \frac{1}{n\gamma}\Sigma_k ln E_{h\sim P} \bigl [e^{\gamma (L(h,S_i)-L(h,S_k))} \bigr ]$$

Plugging this into the equation for approach 1 we get:

$$ \mathcal{L}(\mathcal{Q}, D_i)\leq E_{P\sim \mathcal{Q}} \bigl [\frac{1}{n}\Sigma_k\hat{\mathcal{L}}(Q_i,S_k) - \frac{1}{n\gamma} \Sigma_k KL(Q_i||Q_k) + \frac{1}{n\gamma}\Sigma_k ln E_{h\sim P} \bigl [e^{\gamma (L(h,S_i)-L(h,S_k))} \bigr ] \bigr ]+ KL(\mathcal{Q}||\mathcal{P})+\Upsilon^{\text{\Romannum{2}}}(\mathcal{P}, S_i)$$

Simplifying this a bit we can write this as:


$$ \mathcal{L}(\mathcal{Q}, D_i)\leq E_{P\sim \mathcal{Q}}\bigl [\frac{1}{n}\Sigma_k \bigl ( \hat{\mathcal{L}}(Q_i,S_k) + \frac{1}{\gamma} ln E_{h\sim P}\bigl [e^{\gamma (L(h,S_i)-L(h,S_k))} \bigr ] -   \frac{1}{\gamma}  KL(Q_i||Q_k) \bigr ) \bigr ] + KL(\mathcal{Q}||\mathcal{P})+\Upsilon^{\text{\Romannum{2}}}(\mathcal{P}, S_i)$$

Let's mark:
$$g(P)=\frac{1}{n}\Sigma_k \bigl ( \hat{\mathcal{L}}(Q_i,S_k) + \frac{1}{\gamma} ln E_{h\sim P}\bigl [e^{\gamma (L(h,S_i)-L(h,S_k))} \bigr ] -   \frac{1}{\gamma}  KL(Q_i||Q_k) \bigr )$$

$$ \mathcal{L}(\mathcal{Q}, D_i)\leq E_{P\sim \mathcal{Q}}\bigl [g(P) \bigr ] + KL(\mathcal{Q}||\mathcal{P})+\Upsilon^{\text{\Romannum{2}}}(\mathcal{P}, S_i)$$

From Catoni (2007), we can minimize this by choosing:

$$Q^{*}(P)=\frac{\mathcal{P}(P) e^{-\beta g(P)}}{E_{P\sim \mathcal{P}} \bigl [ e^{-\beta g(P) } \bigr ]}$$

We note that we can also optimize $g(P)$ with respect to all other tasks, since the sum component for task $i$ is zero.

Maximizing $g$ (to minimize $Q^{*}$) means that the learned prior should encourage $Q_i$ to have poor fit for other tasks, for $Q_i$ to be as similar to other $Q_k$ as possible, and for $P$ to be a poor predictor of $S_i$ and a good predictor of $S_k$. 

These are somewhat contradictory goals - We would like to create a prior that is a good fit for all other tasks, but that fits poorly on them when $Q_i$ is applied to it. We would like $Q_i$ to be similar (predictive of) to $Q_k$ but would also like $P$ to be a poor predictor for task i despite being a good predictor of task k.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model-Based RL - 1.3.21} \label{sec:mbrl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

papers

\cite{Curi2020} - H-UCRL

\cite{Chua2018} - PETS

\cite{Ha2018} - world models

\cite{Raileanu2020} - RIDE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Measurable problem parameters - 3.2.21} \label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
	\item Task complexity/difficulty measure - this needs to also consider the student
	\item Student knowledge on other tasks - measured by rewards for other tasks or learning progress
	\item Student knowledge on the current task - measured by pre-train mean reward
	\item Teacher knowledge on task space - whether a well-defined measurement of task difficulty is available to the teacher
	\item Teacher knowledge on student performance - number of task rewards, possibly normalized by the size of the task space and how tasks are sampled from it
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Relevant posts - 19.4.21} \label{sec:blogs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
	\item 
	\href{https://lilianweng.github.io/lil-log/2020/01/29/curriculum-for-reinforcement-learning.html}{Curriculum summary} - a very good summary of curriculum methods for deep RL, with references to many of the better known papers.
	
	\item \href{https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html}{Exploration in deep RL} - a blog post on recent methods in exploration.
	
	\item 
	\href{https://bair.berkeley.edu/blog/2017/12/20/reverse-curriculum/}{Reverse curriculum generation} - a post by one of the authors of \textit{GoalGAN}, this is a short explanation of the notion of `goals of medium difficulty'.
	
	\item 
	\href{https://lilianweng.github.io/lil-log/2019/06/23/meta-reinforcement-learning.html}{MetaRL summary} - an OK summary of metaRL, interesting mostly for the discussion of exploration and task generation.
	
	\item 
	\href{https://wandb.ai/liorf/curriculum_rl} - weights and biases project for experiments.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Code bases - 3.2.21} \label{sec:code}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
	
	\item 
	\href{https://github.com/flowersteam/meta-acl}{Meta ACL} - Curriculum RL code by R. Portelas for \cite{Portelas2020a}. Has multiple teacher algorithms.
	
	\item 
	\href{https://github.com/lioritan/phd-work}{My codebase} - The code I'm working on for curriculum learning.
	
	\item 
	\href{https://github.com/ray-project/ray}{Ray} - one of the most popular frameworks for RL algorithms, originally by Berkley. Hard to extend but supports most known methods.
	
	\item \href{https://github.com/openai/baselines}{Baselines} - OpenAI's original library for stable, open-source implementations of common RL algorithms, no longer active.
	
	\item 
	\href{https://github.com/DLR-RM/stable-baselines3}{Stable baselines} - a community (German Aerospace Center) continuation and improvement of the baselines library.
	
	\item 
	\href{https://github.com/google/dopamine}{Dopamine} - Google's RL agent library, has very few algorithms.
	
	\item 
	\href{https://github.com/facebookresearch/ReAgent}{ReAgent} - Facebook research's RL agent library, has few algorithms focusing on off-policy methods.
	
	\item 
	\href{https://github.com/deepmind/acme}{Acme} - Deepmind's RL agent library, relatively new, has few algorithms focusing on off-policy methods. 
	
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Short-term goals - 1.3.21} \label{sec:short-term}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
	\item \sout{Create a POMDP problem formulation (based on Teacher-Student paper) }
	\item \sout{Add reward shaping to the problem formulation}
	\item \sout{Make a `mixture of experts' method with multiple teachers, consider how to weigh them}
	\item \sout{Create a starting code-base with environments and simple method}
	\item \sout{Consider a game theory formulation of the problem}
	\item \sout{Run and analyze experiments on curriculum effectiveness for existing environments}
	\item \sout{Estimate model impact by leaking task parameters as state}
	\item \sout{Measure impact of exploration using EBQL}
	\item \sout{Implement a model-based method (PETS / H-UCRL) and test it on existing environments}
	\item \sout{Add Mujoco env based on PEARL code (seems not to work on Windows)}
	%\item Attempt to quantify problem complexity for each environment - there's probably a teacher algorithm here
	%\item Add PAIRED / similar adversarial generation method (Rancarnier?)
	%\item Add GoID-like implementation (using estimated task difficulty or starting V). Also check high v low v goid
	%\item Incorporate latent variable from trajectory as parameter or reward bottleneck
	%\item Try using off-policy evaluation to estimate task performance
	%\item Try to see how in-task intervention would change the formulations
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Long-term goals - 3.2.21} \label{sec:long-term}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
	\item Create a measurable parametrization of student types based on response to tasks and teaching curricula
	\item Find a way to isolate and interpret policy building blocks that were learned on simple problems and were successfully transferred to hard problem
	% saliency maps are not good enough, something like SARFA (ICLR20, add state noise and measure actions relative and absolute)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments - 8.6.21} \label{sec:experiment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{table*}
%	%\centering
%	\caption{Experiments (18.5.21)}
%	\label{experiment-table}
%	\begin{tabular}{|l | l | l | l|} 
%		\hline
%		Parameters & Metrics & Observations     \\ \hline	
%		
%		\begin{tabular}{@{}c@{}} Agent: PPO \\ Task: Pendulum angle \\ Episodes: 1000 \\ Teacher: Random tasks \\ Extra: random start state \end{tabular}  
%		&   Mean return (total), 3 trys: -2300
%		&  \begin{tabular}{@{}c@{}}Poor results \\ Similar issues as MB \end{tabular}  
%		\\ \hline
%		
%		\begin{tabular}{@{}c@{}} Agent: PPO \\ Task: Pendulum angle \\ Episodes: 1000 \\ Teacher: Fixed task \\ Extra: random start state \end{tabular}  
%		&   Mean return (total), 3 trys: -2000
%		&  \begin{tabular}{@{}c@{}}Very good results in and near known task \\ Poor results otherwise \end{tabular}  
%		\\ \hline
%		
%		\begin{tabular}{@{}c@{}} Agent: PPO \\ Task: Pendulum angle \\ Episodes: 1000 \\ Teacher: Random task \\ Extra: random start state \\ Extra: add goal angle to state \end{tabular}  
%		&   Mean return (total), 3 trys: -1000
%		&  \begin{tabular}{@{}c@{}}Much better on average \end{tabular}  
%		\\ \hline
%		
%		\begin{tabular}{@{}c@{}} Agent: PPO \\ Task: Pendulum angle \\ Episodes: 1000 \\ Teacher: custom \\
%			25 tasks X$(45^\circ,90^\circ,180^\circ,225^\circ)$  \\ Extra: random start state \end{tabular}  
%		&   Mean return (total), 3 trys: -2800
%		&  \begin{tabular}{@{}c@{}} Specific tasks get better \\ No relation between goal and improvement %\end{tabular}  
%		\\ \hline
%		
%		\begin{tabular}{@{}c@{}} Agent: PPO \\ Task: Pendulum angle \\ Episodes: 1000 \\ Teacher: Random tasks \\ Extra: random start state in $[-\frac{\pi}{8},\frac{\pi}{8}]$ \end{tabular}  
%		&   Mean return (total), 3 trys: -2600
%		&  \begin{tabular}{@{}c@{}} Worse on average than true random \\ Better worst case \end{tabular}  
%		\\ \hline
%		
%		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 200 \\ Teacher: Random tasks \\ Extra: random start state \end{tabular}  
%		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
%			50 eps: \{highest:108, lowest:7\} \\
%			200 eps: \{highest:138, lowest:-2\} \\\end{tabular}    
%		&  \begin{tabular}{@{}c@{}}Better on tasks other than starting task \\ Worse on and near starting task \end{tabular}  
%		    \\ \hline
%		    
%		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 200 \\ Teacher: ALP-GMM \\ Extra: random start state \end{tabular}  
%		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
%			50 eps: \{highest:115, lowest:-8\} \\
%			200 eps: \{highest:54, lowest:-6\} \\
%			 \\\end{tabular}    
%		&  \begin{tabular}{@{}c@{}} Better on average across tasks \\ Lower best case \end{tabular}  
%		\\ \hline
%		
%		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 100 \\ Teacher: custom \\
%			25 tasks X$(45^\circ,90^\circ,180^\circ,225^\circ)$ \\ Extra: random start state \end{tabular}  
%		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
%			25 eps: \{highest:92, lowest:-3\} \\
%			100 eps: \{highest:130, lowest:-3\} \\
%			\\\end{tabular}    
%		&  \begin{tabular}{@{}c@{}}Some correlation between target and performance \\ Not as much as expected \end{tabular}  
%		\\ \hline
%   
%   		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 100 \\ Teacher: constant \\
%   			100 tasks X$180^\circ$ \\ Extra: random start state \end{tabular}  
%   		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
%   			50 eps: \{highest:103, lowest:-4, goal: 103\} \\
%   			200 eps: \{highest:125, lowest:-5, goal: 2\} \\
%   			 \\\end{tabular}    
%   		&  \begin{tabular}{@{}c@{}}No relation between goal performance and learning \\ Very noisy \end{tabular}  
%   		\\ \hline
%   		
%   		\begin{tabular}{@{}c@{}} Agent: MB-MPO \\ Task: Pendulum angle \\ Episodes: 100 \\ Teacher: custom \\
%   			25 tasks X$(45^\circ,90^\circ,180^\circ,225^\circ)$ \\ Extra: random start state in $[-\frac{\pi}{8},\frac{\pi}{8}]$ \end{tabular}  
%   		&  \begin{tabular}{@{}c@{}}Mean return (last state), 3 trys, 10 angles \\ 
%   			25 eps: \{highest: 56, lowest:-6, goal: 56\} \\
%   			100 eps: \{highest:56, lowest:-6, goal: 18\} \\
%   			\\\end{tabular}    
%   		&  \begin{tabular}{@{}c@{}} Surprisingly worse than a wider variation \end{tabular}  
%   		\\ \hline
%		
%	\end{tabular}
%\end{table*}

Setup 2 : ensemble experiments:
Possible environments:
\begin{enumerate}
	\item Parametric pendulum - pendulum environment with randomized start state $\theta \in [-\frac{\pi}{8}, \frac{\pi}{8}]$, $v\in[-1,1]$. State space is $cos\theta,sin\theta,v$. Parameter is the goal angle.
	\item Parametric HalfCheetah - Standard HalfCheetah. Parameter is the target speed going to the right. 
	\item Parametric Ant - Standard Ant. Parameters are the goal location $x,y$ coordinates.
\end{enumerate}
Rewards are of the form $\alpha d(s,g) + \beta ||a||_2$.

The possible students are either PPO or NN-ensemble.

For PPO, the major parameters are:
\begin{itemize}
	\item Network size - Number of weights, number of layers.
	\item learning rate - For loss optimizer
	\item batch size - Number of training steps to use for each SGD batch
	\item number of epochs - Number of times gradient is passed backwards on each training step
	\item $\lambda$ - The Generalized advantage estimation lambda, used to trade-off bias and variance
	\item clip range - How aggressively policy/value function clipping should be applied (an approximation of KL-divergence)
	\item entropy coefficient - Used in PPO loss
	\item value function coefficient - Used in PPO loss
\end{itemize} 

The NN-ensemble uses the log-likelihood to decide the network that is most likely to generate the trajectory, updates that network with the new sample, and uses it to pick the next action using MPC.
New networks are created by training them for several time steps, then comparing the likelihood (with a high initial likelihood) and adding a network only if it is the most likely.
A new network is removed unless it remains the most likely for several time steps.

For NN-ensemble, there are several orthogonal parameters:
\begin{itemize}
	\item MPC horizon - how many steps in the future the MPC should predict
	\item MPC optimizer - sampling method for action sequences for MPC, can be random or cross-entropy method (CEM)
	\item model pretrain period - how many environment interactions are sampled using a random policy before the model-based ensemble is used
	\item model burn-in - how many environment interactions a new model sees before evaluation begins, as well as the number of interactions a new model must be picked as most likely before it is considered a good addition to the ensemble
	\item model $\alpha$ - The initial likelihood of a new model. High values encourage many new networks.
	\item network size - number of weights, number of layers.
	\item architecture - whether to use residual input-output connections and spectral norm.
	\item prediction method - whether to use the  most likely model for prediction or to use all models with weights proportional to their predicted likelihood.
\end{itemize}

Another possible parameter is the teacher curriculum, that can be one of:
\begin{enumerate}
	\item Random tasks
	\item Constant task - repetitions of the same task
	\item Custom schedule - a hand-picked selection of tasks, repeated several times (i.e. pick the same parameters $x$ times, then move to another task $x$ times) 
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Formalizing curricula for a simple task - 6.6.21} \label{sec:pendulum-dreams}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to consider what curricula are valid, let us first define a simple task for reference.
The inverted pendulum task consists of a pendulum of predefined mass and length.
The goal of the parametric version of this task is to reach a chosen angle offset relative to the upright position, meaning a goal angle offset of 0 is the standard inverted pendulum formulation.
As a reminder, the state consists of the sine of the angle, the cosine of the angle, and the current velocity.

In order to discuss this parametric pendulum task, we must make two assumptions:
\begin{enumerate}
	\item The pendulum can be controlled to reach a steady state at any given angle - this assumption depends on the mass, length and gravity of the pendulum system. Violation of this assumption means that some tasks cannot be solved (and the optimal policy for them is either staying in the closest stable position or rotating at high velocity, depending on the cost function)
	\item The cost function penalizes angle cost significantly more than action cost. Violation of this assumption may cause the controller to prioritize inaction over a correct policy.
\end{enumerate}
Under these assumptions, we can continue to make several hypotheses on the behavior of a student and teacher.

\begin{hypothesis} (2.6.21)
	Tasks correlate with the angle offset - angles close to $\pi$ are easier, and angles close to $0$ or $2\pi$ are harder.
\end{hypothesis}
The intuition for this assumption is that since a longer trajectory is required to reach the goal state, these tasks are more difficult on average. This seems to hold in practice, as we can see most model-based agents have difficulty learning this task (mean reward increases with distance to $0$ offset). 
Model-free agents trained on random tasks with a known goal also show this behavior very clearly, and agents trained on this ``harder'' task perform better on average than those trained on an ``easier" task, suggesting that this is indeed the case. 

\begin{figure}[h]
	\includegraphics[width=0.7\textwidth]{pend-rand}
	\caption{(7/6/21) Zero-shot reward for PPO with known goal and a random curriculum. We note that $\pi$ is the easiest task, and $0$/$2\pi$ is the hardest, not including a few outliers.}
\end{figure}

\begin{hypothesis} (3.6.21)
	A schedule is relevant to task performance.
\end{hypothesis}
This seems to hold in practice - a constant schedule of the easiest task performs poorly, a constant schedule of a medium difficulty ($\frac{\pi}{2}$) converges faster (for PPO) but to a lower quality solution, and a constant hard schedule performs best (for PPO). A random schedule performed worse than both a hard schedule and a medium one.
Mixed schedules perform somewhere in the middle to no surprise.
We note that the worse performance of a medium schedule may be due to the pendulum environment: the ``hard'' task is symmetrical to both directions, whereas the medium schedule is not. 

\begin{figure}[h]
	\includegraphics[width=0.7\textwidth]{pend-sched}
	\caption{(7/6/21) Zero-shot reward for PPO with several curricula.}
\end{figure}

\begin{hypothesis} (7.6.21)
	A gradual schedule is better than a random schedule.
\end{hypothesis}
To test this, we compared several simple gradual curricula. So far, this does not seem to hold when the agent is given the goal. As we see in figure \ref{figure:mixed-sched}, This hypothesis does NOT necessarily hold. We can see that the random schedule achieves the best asymptotic performance. We also see that for this student and this environment, a gradual but fast schedule is better than a more linear one, as well as a constant one, in both asymptotic and time-to-reward.


\begin{figure}[h] \label{figure:mixed-sched}
	\includegraphics[width=0.5\textwidth]{mixed-sched-angles}
	\includegraphics[width=0.5\textwidth]{mixed-sched-rewards}
	\caption{(8/6/21) Zero-shot reward for PPO, given a known goal state, with several curricula.
	The left side shows the curriculum over the training task, and the right side shows the test reward.}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ensemble agent ideas - 26.4.21} \label{sec:ensemble}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}
	%\centering
	\caption{RL problem mapping with common solution methods. All solutions assume a dense reward function, since sparse rewards complicate all of them in similar methods.}
	\label{table-rl-problems}
	\begin{tabular}{|l | l |} 
		\hline
		\begin{tabular}{@{}c@{}}Static environment, Static dynamics \\ 
			Standard RL \\
			\end{tabular}
		 & \begin{tabular}{@{}c@{}}Static environment, Variable dynamics \\ 
		 	Meta-RL / Standard RL+Simple curricula\\
		 \end{tabular}    \\ \hline	
	 \begin{tabular}{@{}c@{}}Variable environment, Static dynamics \\ 
	 	Standard RL + curriculum  \\
	 \end{tabular}
		 & 
		 \begin{tabular}{@{}c@{}}Variable environment, Variable dynamics \\ 
		 	Unsolved \\
		 \end{tabular}  \\ \hline
	\end{tabular}
\end{table}

\begin{algorithm}[H]
	\caption{Basic curriculum-based agent}
	\label{pseudo-basic}
	\small
	\begin{algorithmic}
		\Function{Train}{$\mathcal{T}$, n, T, $\theta$}
		\State agent $\leftarrow$ \Call{Initialize}{$\theta$}
		\State teacher $\leftarrow$ \Call{Initialize}{$\theta$, $\mathcal{T}$}
		\State history $\leftarrow$ \{\}
		\For {task in n}
			\State $s_0, env$ $\leftarrow$ \Call{teacher.choose}{history, $\mathcal{T}$}
			\For {timestep in T}
				\State $a_t$ $\leftarrow$ \Call{agent.next}{$s_t$}
				\State $s_{t+1}, r_t$ $\leftarrow$ \Call{env.step}{$s_t$, $a_t$}
				\State \Call{agent.update}{$s_t$, $a_t$, $s{t+1}$, $r_t$}
			\EndFor
			\State history $\leftarrow$ history $\cup$ $\{(s_0, env, r_0, ..., r_T)\}$
		\EndFor
		\State \Return agent
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Let us begin by ignoring the design of the teacher in algorithm \ref{pseudo-basic}, and assume a random curriculum for simplicity.
Looking at table \ref{table-rl-problems},  we can see that the main interesting case is a variable environment, meaning variations in start states, goals and environment elements such as obstacles.

\subsection{Action selection}

For an ensemble of model-free experts, the action selection is done with $\pi(s_t) = \sum_{i} g_i(s_t) \pi_i(s_t)$, and then actions are chosen by sampling or choosing the optimal action.

For an ensemble of model-based experts, this is done by running MPC and calculating the best action according to a (assumed known) reward model, so $a_t = argmax_a \sum_{i} g_i(s_t) \sum_{t'=0}^{H} R(M(s_{t'}, a_{t'}))$.
This can be calculated with some shooting method, using the same action sequence for each model.
It is also possible (and may be preferable for parallelization) to calculate the optimal action for each expert by itself and using $g_i$ as a weighted average by normalizing it to be a weight measure.

For both methods, we could do as previous papers did and pick only one expert (according to $g_i(s_t)$) instead.

\subsection{Agent update}

This section is fundamentally identical for model-free and model-based experts, with the only differences being how each expert uses points to update, and how experts may be unified if an adaptive number of experts is used.

The main design choice for updates is one of credit assignment - given a data point $(s_t, a_t, s_{t+1}, r_t)$, should it be applied to expert $i$ proportionally to $g_i(s_t)$ or without regarding it.
An additional minor concern is whether to apply updates immediately or aggregate in batches.

If the number of experts is not fixed, we must choose how to learn new experts and how to combine existing ones to avoid a state of adding poor new experts.
The simple approach would be to somehow choose (perhaps randomly) when to start collecting data to train a new expert. When we have gathered a large enough number of examples to train the expert, it can be evaluated alongside the others. 
In order to unify experts, it may be wise to act in a similar way to \cite{Xu2020a}, keeping a set of data points for each expert and adding the new points to an expert when unifying. This would require re-calculating $g_i(s_t)$, but that is not a high cost. 


\subsection{Choosing ensemble weights}

There is a rather large array of potential options here. It may be wise to test all of them and see what works.
These include:
\begin{itemize}
	\item A manual design, for example splitting the episode length to $k$ parts and having $g_i$ be proportional to the section in time, such that each expert has large weight in part of the episode.
	\item $g_i(s)  = entropy(s, i)$ - give high (or low) weight to experts with high action entropy in a given state. This may be a function of timestep as well.
	\item $g_i(s)  = V_i(s)$ - learn a value function for each expert (based on the return). This only really makes sense for the model-free case. 
	\item $g_i(s) = \sum_{t=0}^{h}\gamma^t V(s_t)$ - n-step version of the previous method. Relies on the reward model. Since we follow a policy for each agent, we can train a shared value function.
	\item $g_i(s) = \frac{P(s|i,s')}{\sum_j P(s|j, s')}$ - A simple likelihood estimate. Requires saving the last state and a bootstrap solution for $s_0$. We can also go farther back in time, but that seems superfluous.
	\item Learn $g_i$ with some loss function. 
	\begin{itemize}
	 	\item MSE loss (compared to $r$) with only the expert's data. This is basically the same as $g_i(s)  = V_i(s)$ but with some data selection and normalization since the expert action is different from the chosen action. Encourages convergence to a single expert.
	 	\item Some kind of discriminative loss between experts - train global $g$ as an embedding from state space to some small space (of size $k$) to maximally discriminate expert predictions, and weigh based on the most appropriate experts for a given state. It's not really clear how to achieve this. Using the predictions of other experts may be useful as a confidence measure here. Encourages very different experts.
	 	\item Some tradeoff of the above two - minimize MSE loss but penalize low KL divergence between experts, as a simple example.
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\bibliographystyle{plain}
\bibliography{library}

\end{document}